{
  "hash": "44b75107416b412dee6d75ac945abfb0",
  "result": {
    "engine": "knitr",
    "markdown": "# Introduction to Vector Space {#sec-vectorspace-intro}\n\n\n\n\n\nThus far we have covered various forms of counting. But more advanced methods in NLP often rely on *comparing* instead. To understand these methods, we must get comfortable with the idea of vector space.\n\nThis chapter is a basic introduction to the concept of representing documents as vectors. We also introduce two basic vector-based measurement techniques: Euclidean distance and cosine similarity. A more advanced and in-depth guide to navigating vector space will be covered in @sec-navigating-vectorspace.\n\n------------------------------------------------------------------------\n\nA fictional example[^vectorspace-intro-1]: Daniel and Amos filled out a psychology questionnaire. The questionnaire measured three aspects of their personalities: extraversion, openness to experience, and neuroticism.\n\n[^vectorspace-intro-1]: This section is adapted from @alammar_2019\n\n\n::: {.cell}\n\n:::\n\n\nOn the extraversion scale, Amos scored a 2 and Daniel scored a 4:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](vectorspace-intro_files/figure-html/unnamed-chunk-2-1.png){width=768}\n:::\n:::\n\n\nOn the openness scale, Amos scored a 7 and Daniel scored a 6:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](vectorspace-intro_files/figure-html/unnamed-chunk-3-1.png){width=768}\n:::\n:::\n\n\nOn the neuroticism scale, Amos scored a 3 and Daniel scored a 5.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](vectorspace-intro_files/figure-html/unnamed-chunk-4-1.png){width=768}\n:::\n:::\n\n\nWe can now represent each person's personality as a list of three numbers, or a *three dimensional vector*. We can graph these vectors in three dimensional vector space:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](vectorspace-intro_files/figure-html/unnamed-chunk-5-1.png){width=576}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"plotly html-widget html-fill-item\" id=\"htmlwidget-74cfae68c7f2192b61f0\" style=\"width:100%;height:406px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-74cfae68c7f2192b61f0\">{\"x\":{\"visdat\":{\"a63f57458012\":[\"function () \",\"plotlyVisDat\"],\"a63f13db4a81\":[\"function () \",\"data\"]},\"cur_data\":\"a63f13db4a81\",\"attrs\":{\"a63f57458012\":{\"x\":{},\"y\":{},\"z\":{},\"split\":{},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"markers\",\"inherit\":true},\"a63f57458012.1\":{\"x\":{},\"y\":{},\"z\":{},\"split\":{},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"text\":{},\"type\":\"scatter3d\",\"mode\":\"text\",\"inherit\":true},\"a63f13db4a81\":{\"x\":{},\"y\":{},\"z\":{},\"split\":{},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"lines\",\"inherit\":true}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"xaxis\":{\"zerolinecolor\":\"black\",\"zerolinewidth\":10},\"showlegend\":false,\"scene\":{\"xaxis\":{\"title\":\"extraversion\"},\"yaxis\":{\"title\":\"openness\"},\"zaxis\":{\"title\":\"neuroticism\"}},\"hovermode\":\"closest\"},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"x\":[2],\"y\":[7],\"z\":[3],\"type\":\"scatter3d\",\"mode\":\"markers\",\"name\":\"Amos\",\"marker\":{\"color\":\"rgba(31,119,180,1)\",\"line\":{\"color\":\"rgba(31,119,180,1)\"}},\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"line\":{\"color\":\"rgba(31,119,180,1)\"},\"frame\":null},{\"x\":[4],\"y\":[6],\"z\":[5],\"type\":\"scatter3d\",\"mode\":\"markers\",\"name\":\"Daniel\",\"marker\":{\"color\":\"rgba(255,127,14,1)\",\"line\":{\"color\":\"rgba(255,127,14,1)\"}},\"error_y\":{\"color\":\"rgba(255,127,14,1)\"},\"error_x\":{\"color\":\"rgba(255,127,14,1)\"},\"line\":{\"color\":\"rgba(255,127,14,1)\"},\"frame\":null},{\"x\":[2],\"y\":[7],\"z\":[3],\"text\":\"Amos\",\"type\":\"scatter3d\",\"mode\":\"text\",\"name\":\"Amos\",\"marker\":{\"color\":\"rgba(44,160,44,1)\",\"line\":{\"color\":\"rgba(44,160,44,1)\"}},\"error_y\":{\"color\":\"rgba(44,160,44,1)\"},\"error_x\":{\"color\":\"rgba(44,160,44,1)\"},\"line\":{\"color\":\"rgba(44,160,44,1)\"},\"frame\":null},{\"x\":[4],\"y\":[6],\"z\":[5],\"text\":\"Daniel\",\"type\":\"scatter3d\",\"mode\":\"text\",\"name\":\"Daniel\",\"marker\":{\"color\":\"rgba(214,39,40,1)\",\"line\":{\"color\":\"rgba(214,39,40,1)\"}},\"error_y\":{\"color\":\"rgba(214,39,40,1)\"},\"error_x\":{\"color\":\"rgba(214,39,40,1)\"},\"line\":{\"color\":\"rgba(214,39,40,1)\"},\"frame\":null},{\"x\":[2,0],\"y\":[7,0],\"z\":[3,0],\"type\":\"scatter3d\",\"mode\":\"lines\",\"name\":\"Amos\",\"marker\":{\"color\":\"rgba(148,103,189,1)\",\"line\":{\"color\":\"rgba(148,103,189,1)\"}},\"error_y\":{\"color\":\"rgba(148,103,189,1)\"},\"error_x\":{\"color\":\"rgba(148,103,189,1)\"},\"line\":{\"color\":\"rgba(148,103,189,1)\"},\"frame\":null},{\"x\":[4,0],\"y\":[6,0],\"z\":[5,0],\"type\":\"scatter3d\",\"mode\":\"lines\",\"name\":\"Daniel\",\"marker\":{\"color\":\"rgba(140,86,75,1)\",\"line\":{\"color\":\"rgba(140,86,75,1)\"}},\"error_y\":{\"color\":\"rgba(140,86,75,1)\"},\"error_x\":{\"color\":\"rgba(140,86,75,1)\"},\"line\":{\"color\":\"rgba(140,86,75,1)\"},\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\nNow imagine that we encounter a third person, Elizabeth. We would like to know whether Elizabeth is more similar to Daniel or to Amos.\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"plotly html-widget html-fill-item\" id=\"htmlwidget-c01c30d07c8447bee182\" style=\"width:100%;height:406px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-c01c30d07c8447bee182\">{\"x\":{\"visdat\":{\"a63f3ac50286\":[\"function () \",\"plotlyVisDat\"],\"a63f5c28c98e\":[\"function () \",\"data\"]},\"cur_data\":\"a63f5c28c98e\",\"attrs\":{\"a63f3ac50286\":{\"x\":{},\"y\":{},\"z\":{},\"split\":{},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"markers\",\"inherit\":true},\"a63f3ac50286.1\":{\"x\":{},\"y\":{},\"z\":{},\"split\":{},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"text\":{},\"type\":\"scatter3d\",\"mode\":\"text\",\"inherit\":true},\"a63f5c28c98e\":{\"x\":{},\"y\":{},\"z\":{},\"split\":{},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter3d\",\"mode\":\"lines\",\"inherit\":true}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"xaxis\":{\"zerolinecolor\":\"black\",\"zerolinewidth\":10},\"showlegend\":false,\"scene\":{\"xaxis\":{\"title\":\"Dim1\"},\"yaxis\":{\"title\":\"Dim2\"},\"zaxis\":{\"title\":\"Dim3\"}},\"hovermode\":\"closest\"},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"x\":[2],\"y\":[7],\"z\":[3],\"type\":\"scatter3d\",\"mode\":\"markers\",\"name\":\"Amos\",\"marker\":{\"color\":\"rgba(31,119,180,1)\",\"line\":{\"color\":\"rgba(31,119,180,1)\"}},\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"line\":{\"color\":\"rgba(31,119,180,1)\"},\"frame\":null},{\"x\":[4],\"y\":[6],\"z\":[5],\"type\":\"scatter3d\",\"mode\":\"markers\",\"name\":\"Daniel\",\"marker\":{\"color\":\"rgba(255,127,14,1)\",\"line\":{\"color\":\"rgba(255,127,14,1)\"}},\"error_y\":{\"color\":\"rgba(255,127,14,1)\"},\"error_x\":{\"color\":\"rgba(255,127,14,1)\"},\"line\":{\"color\":\"rgba(255,127,14,1)\"},\"frame\":null},{\"x\":[8],\"y\":[4],\"z\":[6],\"type\":\"scatter3d\",\"mode\":\"markers\",\"name\":\"Elizabeth\",\"marker\":{\"color\":\"rgba(44,160,44,1)\",\"line\":{\"color\":\"rgba(44,160,44,1)\"}},\"error_y\":{\"color\":\"rgba(44,160,44,1)\"},\"error_x\":{\"color\":\"rgba(44,160,44,1)\"},\"line\":{\"color\":\"rgba(44,160,44,1)\"},\"frame\":null},{\"x\":[2],\"y\":[7],\"z\":[3],\"text\":\"Amos\",\"type\":\"scatter3d\",\"mode\":\"text\",\"name\":\"Amos\",\"marker\":{\"color\":\"rgba(214,39,40,1)\",\"line\":{\"color\":\"rgba(214,39,40,1)\"}},\"error_y\":{\"color\":\"rgba(214,39,40,1)\"},\"error_x\":{\"color\":\"rgba(214,39,40,1)\"},\"line\":{\"color\":\"rgba(214,39,40,1)\"},\"frame\":null},{\"x\":[4],\"y\":[6],\"z\":[5],\"text\":\"Daniel\",\"type\":\"scatter3d\",\"mode\":\"text\",\"name\":\"Daniel\",\"marker\":{\"color\":\"rgba(148,103,189,1)\",\"line\":{\"color\":\"rgba(148,103,189,1)\"}},\"error_y\":{\"color\":\"rgba(148,103,189,1)\"},\"error_x\":{\"color\":\"rgba(148,103,189,1)\"},\"line\":{\"color\":\"rgba(148,103,189,1)\"},\"frame\":null},{\"x\":[8],\"y\":[4],\"z\":[6],\"text\":\"Elizabeth\",\"type\":\"scatter3d\",\"mode\":\"text\",\"name\":\"Elizabeth\",\"marker\":{\"color\":\"rgba(140,86,75,1)\",\"line\":{\"color\":\"rgba(140,86,75,1)\"}},\"error_y\":{\"color\":\"rgba(140,86,75,1)\"},\"error_x\":{\"color\":\"rgba(140,86,75,1)\"},\"line\":{\"color\":\"rgba(140,86,75,1)\"},\"frame\":null},{\"x\":[2,0],\"y\":[7,0],\"z\":[3,0],\"type\":\"scatter3d\",\"mode\":\"lines\",\"name\":\"Amos\",\"marker\":{\"color\":\"rgba(227,119,194,1)\",\"line\":{\"color\":\"rgba(227,119,194,1)\"}},\"error_y\":{\"color\":\"rgba(227,119,194,1)\"},\"error_x\":{\"color\":\"rgba(227,119,194,1)\"},\"line\":{\"color\":\"rgba(227,119,194,1)\"},\"frame\":null},{\"x\":[4,0],\"y\":[6,0],\"z\":[5,0],\"type\":\"scatter3d\",\"mode\":\"lines\",\"name\":\"Daniel\",\"marker\":{\"color\":\"rgba(127,127,127,1)\",\"line\":{\"color\":\"rgba(127,127,127,1)\"}},\"error_y\":{\"color\":\"rgba(127,127,127,1)\"},\"error_x\":{\"color\":\"rgba(127,127,127,1)\"},\"line\":{\"color\":\"rgba(127,127,127,1)\"},\"frame\":null},{\"x\":[8,0],\"y\":[4,0],\"z\":[6,0],\"type\":\"scatter3d\",\"mode\":\"lines\",\"name\":\"Elizabeth\",\"marker\":{\"color\":\"rgba(188,189,34,1)\",\"line\":{\"color\":\"rgba(188,189,34,1)\"}},\"error_y\":{\"color\":\"rgba(188,189,34,1)\"},\"error_x\":{\"color\":\"rgba(188,189,34,1)\"},\"line\":{\"color\":\"rgba(188,189,34,1)\"},\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\nAfter graphing all three people in three-dimensional vector space, it becomes obvious that Elizabeth is more similar to Daniel than she is to Amos. Thinking of people (or any sort of observations) as vectors is powerful because it allows us to apply geometric reasoning to data. The beauty of this approach is that we can measure these things without knowing anything about what the dimensions represent. This will be important later.\n\nThus far we have discussed three-dimensional vector space. But what if we want to measure personality with the full Big-Five traits---openness, conscientiousness, extraversion, agreeableness, and neuroticism? Five dimensions would make it impossible to graph the data in an intuitive way as we have done above, but in a mathematical sense, it doesn't matter. We can measure distance---and many other geometric concepts---just as easily in five-dimensional vector space as in three dimensions.\n\n## Distance and Similarity\n\nWhen we added Elizabeth to the graph above, we could tell that she was more similar to Daniel than to Amos just by looking at the graph. But how do we quantify this similarity or difference?\n\n### Euclidean Distance {#sec-euclidean-distance}\n\nThe most straightforward way to measure the similarity between two points in space is to measure the distance between them. *Euclidean distance* is the simplest sort of distance---the length of the shortest straight line between the two points. The Euclidean distance between two vectors $A$ and $B$ can be calculated in any number of dimensions $n$ using the following formula:\n\n$$\nd\\left( A,B\\right)   = \\sqrt {\\sum _{i=1}^{n}  \\left( A_{i}-B_{i}\\right)^2 }\n$$\n\n**A low Euclidean distance means two vectors are very similar**. Let's calculate the Euclidean distance between Daniel and Elizabeth, and between Amos and Elizabeth:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# dataset\npersonality\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 3 × 4\n#>   person    extraversion openness neuroticism\n#>   <chr>            <dbl>    <dbl>       <dbl>\n#> 1 Daniel               4        6           5\n#> 2 Amos                 2        7           3\n#> 3 Elizabeth            8        4           6\n```\n\n\n:::\n\n```{.r .cell-code}\n# Elizabeth's vector\neliza_vec <- personality |> \n  filter(person == \"Elizabeth\") |> \n  select(extraversion:neuroticism) |> \n  as.numeric()\n\n# Euclidean distance function\neuc_dist <- function(x, y){\n  diff <- x - y\n  sqrt(sum(diff^2))\n}\n\n# distance between Elizabeth and each person\npersonality_dist <- personality |> \n  rowwise() |> \n  mutate(\n    dist_from_eliza = euc_dist(c_across(extraversion:neuroticism), eliza_vec)\n  )\n\npersonality_dist\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 3 × 5\n#> # Rowwise: \n#>   person    extraversion openness neuroticism dist_from_eliza\n#>   <chr>            <dbl>    <dbl>       <dbl>           <dbl>\n#> 1 Daniel               4        6           5            4.58\n#> 2 Amos                 2        7           3            7.35\n#> 3 Elizabeth            8        4           6            0\n```\n\n\n:::\n:::\n\nWe now see that the closest person to Elizabeth is... Elizabeth herself, with a distance of 0. After that, the closest is Daniel. So we can conclude that Daniel has a more Elizabeth-like personality than Amos does.\n\n### Cosine Similarity {#sec-cosine-similarity}\n\nBesides Euclidean distance, the most common way to measure the similarity between two vectors is with cosine similarity. This is the cosine of the angle between the two vectors. Since the cosine of 0 is 1, **a high cosine similarity (close to 1) means two vectors are very similar**.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](vectorspace-intro_files/figure-html/unnamed-chunk-9-1.png){width=768}\n:::\n:::\n\n\nA nice thing about the cosine is that it is always between -1 and 1: When the two vectors are pointing in a similar direction, the cosine is close to 1, and when they are pointing in a near-opposite direction (180°), the cosine is close to -1. \n\nLooking at the above visualization, you might wonder: Why should the angle be fixed at the zero point? What does the zero point have to do with anything? If you wondered this, good job. The reason: **Cosine similarity works best when your vector space is centered at zero (or close to it)**. In other words, it works best when zero represents a medium level of each variable. This fact is sometimes taken for granted because, in practice, many vector spaces are already centered at zero. For example, word embeddings trained with word2vec, GloVe, and related models (@sec-word-embeddings) can be assumed to center at zero given sufficiently diverse training data because their training is based on the dot products between embeddings (the dot product is a close cousin of cosine similarity). The ubiquity of zero-centered vector spaces makes cosine similarity a very useful tool. Even so, not all vector spaces are zero-centered, so take a moment to consider the nature of your vector space before deciding which similarity or distance metric to use.\n\nThe formula for calculating cosine similarity might look a bit complicated:\n\n$$\nCosine(A,B) = \\frac{A \\cdot B}{|A||B|} = \\frac{\\sum _{i=1}^{n}  A_{i}B_{i}}{\\sqrt {\\sum _{i=1}^{n} A_{i}^2} \\cdot \\sqrt {\\sum _{i=1}^{n} B_{i}^2}}\n$$\nIn R though, it's pretty simple. Let's calculate the cosine similarity between Elizabeth and each of the other people in our sample. To make sure the vector space is centered at zero, we will subtract 4 from each value (the scales all range from 1 to 7).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# cosine similarity function\ncos_sim <- function(x, y){\n  dot <- x %*% y\n  normx <- sqrt(sum(x^2))\n  normy <- sqrt(sum(y^2))\n  as.vector( dot / (normx*normy) )\n}\n\n# center at 0\neliza_vec_centered <- eliza_vec - 4\npersonality_sim <- personality |> \n  mutate(across(extraversion:neuroticism, ~.x - 4))\n\n# distance between Elizabeth and each person\npersonality_sim <- personality_sim |> \n  rowwise() |> \n  mutate(\n    similarity_to_eliza = cos_sim(c_across(extraversion:neuroticism), eliza_vec_centered)\n  )\n\npersonality_sim\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 3 × 5\n#> # Rowwise: \n#>   person    extraversion openness neuroticism similarity_to_eliza\n#>   <chr>            <dbl>    <dbl>       <dbl>               <dbl>\n#> 1 Daniel               0        2           1               0.2  \n#> 2 Amos                -2        3          -1              -0.598\n#> 3 Elizabeth            4        0           2               1\n```\n\n\n:::\n:::\n\nOnce again, we see that the most similar person to Elizabeth is Elizabeth herself, with a cosine similarity of 1. The next closest, as before, is Daniel.\n\nIf you are comfortable with cosines, you might be happy with the explanation we have given so far. Nevertheless, it might be helpful to consider the relationship between cosine similarity and a more familiar statistic that ranges between -1 and 1: the Pearson correlation coefficient (i.e. regular old correlation). Cosine similarity measures the similarity between two _vectors_, while the correlation coefficient measures the similarity between two _variables_. Now just imagine our vectors as variables, with each dimension as an observation. Since we only compare two vectors at a time with cosine similarity, let's start with Elizabeth and Amos:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](vectorspace-intro_files/figure-html/unnamed-chunk-11-1.png){width=768}\n:::\n:::\n\n\nNow imagine centering those variables at zero, like this:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](vectorspace-intro_files/figure-html/unnamed-chunk-12-1.png){width=768}\n:::\n:::\n\n\nWhen seen like this, the correlation is the same as the cosine similarity. In other words, **the correlation between two vectors is the same as the cosine similarity between them when the values of each vector are centered at zero**.^[For proof, see @oconnor_2012] Seeing cosine similarity as the non-centered version of correlation might give you extra intuition for why cosine similarity works best for vector spaces that are centered at zero.\n\n## Word Counts as Vector Space {#sec-word-count-vectors}\n\nThe advantage of thinking in vector space is that we can quantify similarities and differences even without understanding what any of the dimensions in the vector space are measuring. In the coming chapters, we will introduce methods that require this kind of relational thinking, since the dimensions of the vector space are abstract statistical contrivances. Even so, any collection of variables can be thought of as dimensions in a vector space. You might, for example, use distance or similarity metrics to analyze groups of word counts.\n\n**An example of word counts as relational vectors in research:** @ireland_pennebaker_2010 asked students to answer essay questions written in different styles. They then calculated dictionary-based word counts for both the questions and the answers using 9 linguistic word lists from LIWC (see @sec-dictionary-sources), including personal pronouns (e.g. \"I\", \"you\"), and articles (e.g, \"a\", \"the\"). They treated these 9 word counts as a 9-dimensional vector for each text, and measured the similarity between questions and responses with a metric similar to Euclidean distance. They found that students automatically matched the linguistic style of the questions (i.e. answers were more similar to the question they were answering than to other questions) and that women and students with higher grades matched their answers especially closely to the style of the questions.\n\n---\n",
    "supporting": [
      "vectorspace-intro_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/htmltools-fill-0.5.8.1/fill.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<script src=\"site_libs/plotly-binding-4.10.4/plotly.js\"></script>\n<script src=\"site_libs/typedarray-0.1/typedarray.min.js\"></script>\n<script src=\"site_libs/jquery-3.5.1/jquery.min.js\"></script>\n<link href=\"site_libs/crosstalk-1.2.1/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/crosstalk-1.2.1/js/crosstalk.min.js\"></script>\n<link href=\"site_libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/plotly-main-2.11.1/plotly-latest.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}