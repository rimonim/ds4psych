{
  "hash": "00f963757af956d5b9b818ec3d400d25",
  "result": {
    "engine": "knitr",
    "markdown": "---\ninclude-in-header:\n  - text: |\n      <style>\n      .r-output code {\n        word-break: break-wor !important;\n        white-space: pre-wrap !important;\n      }\n      </style>\n---\n\n\n# Navigating Vector Space {#sec-navigating-vectorspace}\n\n\n\n\n\n## Representing Psychological Constructs\n\nIn @sec-decontextualized-embeddings we measured the surprise in texts by comparing their embeddings to that of a single word: \"surprised\". But does the embedding of the word \"surprised\" fully capture the concept of surprise as an emotion? Faced with this question of construct validity, we have two options:\n\n1.    **Conduct a Validation Study:** We could find or construct a dataset of texts that were rated by a human (or ideally, multiple humans) on the extent to which they reflect the emotion of surprise. We could then compare our embedding-based surprise scores to the human ratings.\n2.    **Use an Already-Validated Construct Definition:** Properly validating a new measure is hard work. When possible, psychology researchers often prefer to use an existing measure that has already been carefully validated in the past. \n\nThe second option may seem difficult, since embeddings are very new to the field, so few if any validated vector representations of constructs are available. As it turns out, this is not a problem---any language-based psychological measure can be represented as a vector! Psychology has used language-based measures like dictionaries and questionnaires for over a century. To smoothly continue this existing research in the age of vector spaces, let's consider how to translate between the two.\n\n### Distributed Dictionary Representation (DDR) {#sec-ddr}\n\nLet's begin with a straightforward sort of psychological measure---the dictionary. We have already discussed dictionaries extensively in @sec-word-counting and noted that psychology researchers have been constructing, validating, and publicizing dictionaries for decades (@sec-dictionary-sources). But these dictionaries are designed for word counting---How do we apply them to a vector-based analysis? @garten_etal_2018 propose a simple solution: Get word embeddings (@sec-word-embeddings) for each word in the dictionary, and average them together to create a single Distributed Dictionary Representation (DDR). The dictionary construct can then be measured by comparing text embeddings to the DDR.\n\nDDR cannot entirely replace word counts; for linguistic concepts like pronoun use or the passive voice, dictionary-based word counts are still necessary. But DDR is ideal for studies of abstract constructs like emotions, that refer to the general gist of a text rather than particular words. The rich representation of word embeddings allows DDR to capture even the subtlest associations between words and constructs, and to precisely reflect the _extent_ to which each word is associated with each construct. It can do this even for texts that do not contain any dictionary words. Because embeddings are continuous and already calibrated to the probabilities of word use in language, DDR also avoids the difficult statistical problems that arise due to the strange distributions of word counts (@sec-word-counting-improvements).\n\n@garten_etal_2018 found that DDR works best with smaller dictionaries of only the words most directly connected to the construct being measured (around 30 words worked best in their experiments). Word embeddings work by overvaluing informative words (@sec-embedding-magnitude)---a desirable property for raw texts, in which uninformative words tend to be very frequent. But dictionaries only include one of each word. In longer dictionaries with more infrequent, tangentially connected words, averaging word embeddings will therefore _overvalue_ those infrequent words and skew the DDR. This can be fixed with Garten et al.'s method of picking out only the most informative words. Alternatively, it could be fixed by measuring the frequency of each dictionary word in a corpus and weighting the average embedding by that frequency. This method is actually more consistent with the way most dictionaries are validated, by counting the frequencies of dictionary words in text (@sec-word-counting).\n\n[^ddr-1]: For more information on this property, see our [footnote](decontextualized-embeddings.html#fn7) in @sec-word2vec. Note that this property emerges naturally from the way decontextualized models like word2vec and GloVe are trained, and therefore may not hold true for contextualized embeddings.\n\nLet's measure surprise in the Hippocorpus texts by computing a DDR of the NRC Word-Emotion Association Lexicon [@mohammad_turney_2010; @mohammad_turney_2013], which we used in @sec-word-counting. To correct for word informativeness, we will weight the dictionary word embeddings by their frequency in the corpus.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load surprise dictionary\nsurprise_dict <- quanteda.sentiment::data_dictionary_NRC[\"surprise\"]\n\n# estimate frequency of dictionary words\nsurprise_dict_freqs <- hippocorpus_dfm |> \n  dfm_keep(surprise_dict$surprise) |> \n  quanteda.textstats::textstat_frequency() |> \n  select(feature, frequency)\n\n# word2vec embeddings of dictionary words\nsurprise_ddr <- predict(word2vec_mod, surprise_dict$surprise, type = \"embedding\") |> \n  as_tibble(rownames = \"feature\") |> \n  left_join(surprise_dict_freqs) |> \n  replace_na(list(frequency = 0))\n\n# average dictionary embedding (weighted by frequency)\nsurprise_ddr <- surprise_ddr |> \n  summarise(across(V1:V300, ~weighted.mean(.x, frequency, na.rm = TRUE))) |> \n  select(V1:V300) |> \n  unlist()\n\n# document embeddings\nhippocorpus_word2vec <- hippocorpus_dfm |> \n  textstat_embedding(word2vec_mod)\n\n# score documents by surprise\nhippocorpus_surprise_ddr <- hippocorpus_word2vec |> \n  rowwise() |> \n  mutate(\n    surprise = cos_sim(c_across(V1:V300), surprise_ddr),\n    # transform cosine similarity to stay between 0 and 1\n    surprise = surprise/2 + 1/2\n  ) |> \n  ungroup() |> \n  select(-c(V1:V300))\n\n# rejoin docvars\nhippocorpus_surprise_ddr <- hippocorpus_surprise_ddr |> \n  bind_cols(docvars(hippocorpus_corp))\n```\n:::\n\n\nWith the new measure of surprise, we can retest the hypothesis that true autobiographical stories include more surprise than imagined stories. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# beta regression\nsurprise_mod_ddr <- betareg::betareg(\n  surprise ~ memType, \n  data = hippocorpus_surprise_ddr\n  )\n\nsummary(surprise_mod_ddr)\n```\n\n<pre class=\"r-output\"><code>#> \n#> Call:\n#> betareg::betareg(formula = surprise ~ memType, data = hippocorpus_surprise_ddr)\n#> \n#> Standardized weighted residuals 2:\n#>     Min      1Q  Median      3Q     Max \n#> -7.7985 -0.6101  0.0368  0.6529  3.4828 \n#> \n#> Coefficients (mean model with logit link):\n#>                  Estimate Std. Error  z value Pr(>|z|)    \n#> (Intercept)      1.812197   0.001729 1048.008  < 2e-16 ***\n#> memTyperecalled -0.020202   0.002431   -8.312  < 2e-16 ***\n#> memTyperetold   -0.022232   0.003022   -7.357 1.88e-13 ***\n#> \n#> Phi coefficients (precision model with identity link):\n#>       Estimate Std. Error z value Pr(>|z|)    \n#> (phi)  1003.36      17.14   58.55   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n#> \n#> Type of estimator: ML (maximum likelihood)\n#> Log-likelihood: 2.119e+04 on 4 Df\n#> Pseudo R-squared: 0.01286\n#> Number of iterations: 21 (BFGS) + 2 (Fisher scoring)\n</code></pre>\n:::\n\n\nWe again find significant differences in surprise between imagined and recalled stories, in the opposite of the expected direction. This is different from our results in @sec-word-counting, where we tested the same hypothesis with the same dictionary, but used word counts rather than embeddings.\n\n#### DDR for Word-by-Word Analysis\n\nAnother advantage of DDR over dictionary-based word counts is that DDR enables word-by-word analysis of text. It is not very informative to count how many surprise words are in each word (it will either be one or zero), but we can compare the embedding of each word to the surprise DDR---how close are they in the vector space? This allows us to see how a construct spreads out within a single text. As an example, let's take a single story from the Hippocorpus:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# full text as string\nstory <- word(hippocorpus_df$story[3], end = 140L)\n\ncat(story)\n```\n\n<pre class=\"r-output\"><code>#> It seems just like yesterday but today makes five months ago it happened. I had been watching my phone like an owl for the past week. I was waiting for a work related call that my team was waiting for to close a important deal. It wasnt the call I expected though. It was for  my sister was in labor with the twins. My sister is only 7 months pregnant. I got the call shortly after arriving at work. Just as fast I was back out the door and on my way to the hospital. When I arrived my sister had just delivered and I just was in awe. Even though they were a bit small they were mighty. They were the most precious things I had ever seen. I held my niece and nephew and couldnt stop crying.\n</code></pre>\n:::\n\n\nTo visualize surprise within this text, we can separate it into words and find the embedding of each word. Rather than averaging all of these embeddings together to get the embedding of the full text, we can compute a rolling average, averaging each word's embedding with those of its neighbors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# separate into vector of tokens\nstory <- word(hippocorpus_df$story[3], end = 140L) |> \n  tokens() |> as.character()\n  \n# rolling average of embeddings\nstory_surprise <- as_tibble(predict(word2vec_mod, story, type = \"embedding\")) |> \n  mutate(\n    across(\n      V1:V300, \n      ~zoo::rollapply(\n        .x, 4, mean, na.rm = TRUE, \n        align = \"center\",\n        fill = c(head(.x, 1), NA, tail(.x, 1))\n        )\n      )\n    )\n  \n# vector of computed surprise (cosine similarity)\nstory_surprise <- story_surprise |> \n  rowwise() |> \n  mutate(surprise = cos_sim(c_across(V1:V300), surprise_ddr)) |> \n  pull(surprise)\n```\n:::\n\n\nWe can now visualize the surprise in each word of the text. Since `ggplot2` makes it difficult to plot dynamically colored text in one continuous chunk, we will use ANSI color codes to print the text directly to the console.\n\n\n::: {.cell}\n\n```{.r .cell-code .code-overflow-wrap}\n# (see https://www.hackitu.de/termcolor256/ for info on ANSI colors)\n# blue-red heat scale\nansi_scale <- c(\n  063, 105, 147, 189, 188, 230, 223, \n  224, 217, 210, 203, 196, 160, 124\n  )\n\n# turn scale value into ANSI color code\nmap_to_ansi <- function(x, ansi_scale){\n  x_new <- (x - min(x, na.rm = TRUE))*(length(ansi_scale)/diff(range(x, na.rm = TRUE))) + 1\n  x_new\n  ansi_scale[round(x_new)]\n}\n\nstory_surprise <- map_to_ansi(story_surprise, ansi_scale)\n\n# print\nfor (i in 1:length(story_surprise)) {\n  if(is.na(story_surprise[i])){\n    cat(story[i], \" \")\n  }else{\n    cat(paste0(\"\\033[48;5;\", story_surprise[i], \"m\", story[i], \" \\033[0m\"))\n  }\n}\n```\n\n<pre class=\"r-output\"><code>#> <span style='background-color: #D7D7FF;'>It </span><span style='background-color: #AF0000;'>seems just </span>like  <span style='background-color: #FF5F5F;'>yesterday </span><span style='background-color: #FFD7D7;'>but </span><span style='background-color: #FF8787;'>today </span><span style='background-color: #D7D7D7;'>makes </span><span style='background-color: #D7D7FF;'>five </span><span style='background-color: #FFFFD7;'>months </span><span style='background-color: #FFAFAF;'>ago it </span><span style='background-color: #FF0000;'>happened </span><span style='background-color: #FF8787;'>. </span><span style='background-color: #FFD7AF;'>I </span><span style='background-color: #FF8787;'>had been watching </span><span style='background-color: #FF0000;'>my </span><span style='background-color: #FF8787;'>phone </span><span style='background-color: #FFD7D7;'>like </span><span style='background-color: #FFAFAF;'>an </span><span style='background-color: #D7D7D7;'>owl </span><span style='background-color: #FFFFD7;'>for the </span><span style='background-color: #D7D7D7;'>past </span><span style='background-color: #FFD7D7;'>week </span><span style='background-color: #FFAFAF;'>. </span><span style='background-color: #FF0000;'>I was </span><span style='background-color: #FFD7D7;'>waiting </span><span style='background-color: #FF8787;'>for </span><span style='background-color: #D7D7FF;'>a </span><span style='background-color: #AFAFFF;'>work </span><span style='background-color: #FFD7AF;'>related </span><span style='background-color: #FFAFAF;'>call </span><span style='background-color: #D70000;'>that </span><span style='background-color: #FF0000;'>my </span><span style='background-color: #D70000;'>team </span><span style='background-color: #FF8787;'>was </span><span style='background-color: #FFD7D7;'>waiting </span><span style='background-color: #FF8787;'>for </span><span style='background-color: #FFFFD7;'>to </span><span style='background-color: #FFD7D7;'>close </span><span style='background-color: #FF8787;'>a </span><span style='background-color: #FFD7D7;'>important </span><span style='background-color: #FF8787;'>deal </span><span style='background-color: #FFFFD7;'>. </span><span style='background-color: #FFD7D7;'>It wasnt </span><span style='background-color: #FF8787;'>the </span><span style='background-color: #FF5F5F;'>call </span><span style='background-color: #FF0000;'>I </span><span style='background-color: #FF5F5F;'>expected </span><span style='background-color: #FFAFAF;'>though . </span><span style='background-color: #FFD7AF;'>It </span><span style='background-color: #FF8787;'>was </span><span style='background-color: #FFAFAF;'>for my sister </span><span style='background-color: #FFFFD7;'>was </span><span style='background-color: #D7D7D7;'>in labor </span><span style='background-color: #FFD7AF;'>with the </span><span style='background-color: #FFD7D7;'>twins </span><span style='background-color: #D7D7D7;'>. My </span><span style='background-color: #FFD7D7;'>sister </span><span style='background-color: #FFFFD7;'>is only 7 </span><span style='background-color: #5F5FFF;'>months </span><span style='background-color: #FFD7AF;'>pregnant </span><span style='background-color: #FF0000;'>. </span><span style='background-color: #D70000;'>I got </span><span style='background-color: #FF5F5F;'>the </span><span style='background-color: #FFD7AF;'>call shortly </span><span style='background-color: #FFFFD7;'>after </span><span style='background-color: #FFAFAF;'>arriving </span><span style='background-color: #FFD7AF;'>at </span><span style='background-color: #FFD7D7;'>work . </span><span style='background-color: #FFD7AF;'>Just </span><span style='background-color: #FF8787;'>as fast </span><span style='background-color: #FF0000;'>I </span><span style='background-color: #FF5F5F;'>was </span><span style='background-color: #FF8787;'>back </span><span style='background-color: #FFD7D7;'>out </span><span style='background-color: #FFFFD7;'>the </span><span style='background-color: #D7D7FF;'>door </span><span style='background-color: #FFFFD7;'>and </span><span style='background-color: #FF8787;'>on my </span><span style='background-color: #FF5F5F;'>way </span><span style='background-color: #FF8787;'>to </span><span style='background-color: #D7D7D7;'>the </span><span style='background-color: #FFD7AF;'>hospital </span><span style='background-color: #FFAFAF;'>. </span><span style='background-color: #FF5F5F;'>When </span><span style='background-color: #FF0000;'>I </span><span style='background-color: #FF5F5F;'>arrived </span><span style='background-color: #FF8787;'>my </span><span style='background-color: #D70000;'>sister </span><span style='background-color: #FF5F5F;'>had </span><span style='background-color: #FF8787;'>just </span><span style='background-color: #FF0000;'>delivered and </span><span style='background-color: #D70000;'>I just was </span><span style='background-color: #FFAFAF;'>in awe . Even </span><span style='background-color: #FFD7D7;'>though they </span><span style='background-color: #FFAFAF;'>were </span><span style='background-color: #FFD7D7;'>a </span><span style='background-color: #FF5F5F;'>bit </span><span style='background-color: #FF8787;'>small </span><span style='background-color: #FFD7D7;'>they were </span><span style='background-color: #FFFFD7;'>mighty . </span><span style='background-color: #D7D7D7;'>They </span><span style='background-color: #FFFFD7;'>were </span><span style='background-color: #FFD7AF;'>the </span><span style='background-color: #FF5F5F;'>most </span><span style='background-color: #FF0000;'>precious </span><span style='background-color: #AF0000;'>things </span><span style='background-color: #FF0000;'>I </span><span style='background-color: #FFAFAF;'>had </span><span style='background-color: #FFFFD7;'>ever </span><span style='background-color: #FFD7D7;'>seen </span><span style='background-color: #FFD7AF;'>. </span><span style='background-color: #FFD7D7;'>I </span><span style='background-color: #FF8787;'>held </span><span style='background-color: #FFD7AF;'>my </span><span style='background-color: #FFFFD7;'>niece </span><span style='background-color: #8787FF;'>and </span><span style='background-color: #D7D7D7;'>nephew </span><span style='background-color: #FFD7AF;'>and </span><span style='background-color: #FFAFAF;'>couldnt stop </span>crying  .\n</code></pre>\n:::\n\n\n::: {.callout-tip icon=\"false\"}\n## Advantages of DDR\n\n-   **Richer, More Robust Construct Representation Than Word Counting**\n-   **Avoids Statistical Problems With Word Count Distributions**\n-   **Enables Word-by-Word Analysis**\n-   **Works Well With Short Dictionaries:** DDR only needs a dictionary that captures the essence of the construct being measured. For many constructs, this could be only a few words. You can even ask an LLM chatbot to generate a list of words that people high or low in a certain construct might use.\n:::\n\n::: {.callout-important icon=\"false\"}\n## Disadvantages of DDR\n\n-   **Can Implicitly Encode Associated Constructs:** For example, if surprised texts tend to have positive valence in the data used to train the word embedding model, the DDR for surprise may embed some positive valence as well. This can be remedied by constructing a DDR for positive valence too, and using it as a statistical control when testing hypotheses.\n-   **May Not Work With Contextualized Embeddings:** Even if we assume that contextualized embeddings (@sec-contextualized-embeddings) conform to the geometrical properties associated with word embeddings, LLMs are not designed to embed single words, which is required for DDR.\n-   **Not Appropriate for Linguistic Measures:** Word embeddings encode the general gist of a text, whereas constructs like passive voice or pronoun use refer to specific words.\n:::\n\n### Contextualized Construct Representation (CCR) {#sec-ccr}\n\nDictionaries are not the only validated psychological measures that we can apply using embeddings. With contextualized embeddings, we can extract the gist of any text and compare it to that of any other text (@sec-contextualized-embeddings). @atari_etal_2023 propose to do this with the most popular form of psychometric scale: the questionnaire. Psychologists have been using questionnaires to measure things for over a century, and tens of thousands of validated questionnaires are now available [online](https://www.apa.org/pubs/databases/psyctests). The LLM embedding of a questionnaire is referred to as a Contextualized Construct Representation (CCR).\n\nWe can use CCR to measure surprise in the Hippocorpus texts. For our questionnaire, we will use an adapted version of the surprise scale used by @choi_choi_2010 and @choi_nisbett_2000.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsurprise_items <- c(\n  \"I was extremely surprised by the outcome of the event.\",\n  \"The outcome of the event was extremely interesting.\",\n  \"The outcome of the event was extremely new.\"\n  )\n```\n:::\n\n\n::: {.callout-important}\n## Beware of Reverse Coding!\n\nMany questionnaires include reverse-coded items (e.g. \"I often feel happy\" on a depression questionnaire). The easiest way to deal with these is to manually add negations to flip their meaning (e.g. \"I _do not_ often feel happy\"). \n:::\n\nThe first step in using CCR is to compute contextualized embeddings for the texts in the Hippocorpus dataset. We already did this in @sec-contextualized-embeddings. The next step is to compute contextualized embeddings for the items in the questionnaire, and average them to produce a CCR.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# embed items (using the same model as we used before)\nlibrary(text)\n  \nsurprise_sbert <- textEmbed(\n  surprise_items,\n  model = \"sentence-transformers/all-MiniLM-L12-v2\", # model name\n  layers = -2,  # second to last layer (default)\n  tokens_select = \"[CLS]\", # use only [CLS] token\n  dim_name = FALSE,\n  keep_token_embeddings = FALSE\n  )\n\n# compute CCR by averaging item embeddings\nsurprise_ccr <- surprise_sbert$texts[[1]] |>\n  summarise(across(everything(), mean)) |> \n  unlist()\n```\n:::\n\n\nWe can now measure surprise in the Hippocorpus texts by computing the cosine similarity between their embeddings and the surprise CCR.^[Cosine similarity is appropriate here because our contextualized embeddings were generated by an SBERT model, which was designed to be used with cosine similarity. If we had used another model such as RoBERTa, Euclidean distance might be more appropriate.]\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# score documents by surprise\nhippocorpus_surprise_ccr <- hippocorpus_sbert |> \n  rowwise() |> \n  mutate(\n    surprise = cos_sim(c_across(Dim1:Dim384), surprise_ccr),\n    # transform cosine similarity to stay between 0 and 1\n    surprise = surprise/2 + 1/2\n  ) |> \n  ungroup() |> \n  select(-c(Dim1:Dim384))\n\n# beta regression\nsurprise_mod_ccr <- betareg::betareg(\n  surprise ~ memType,\n  hippocorpus_surprise_ccr\n  )\n\nsummary(surprise_mod_ccr)\n```\n\n<pre class=\"r-output\"><code>#> \n#> Call:\n#> betareg::betareg(formula = surprise ~ memType, data = hippocorpus_surprise_ccr)\n#> \n#> Standardized weighted residuals 2:\n#>     Min      1Q  Median      3Q     Max \n#> -6.1295 -0.6100  0.0520  0.6634  3.6654 \n#> \n#> Coefficients (mean model with logit link):\n#>                  Estimate Std. Error  z value Pr(>|z|)    \n#> (Intercept)      5.636437   0.002885 1953.487  < 2e-16 ***\n#> memTyperecalled -0.020755   0.004042   -5.135 2.83e-07 ***\n#> memTyperetold   -0.026428   0.005016   -5.269 1.37e-07 ***\n#> \n#> Phi coefficients (precision model with identity link):\n#>       Estimate Std. Error z value Pr(>|z|)    \n#> (phi)  12226.6      209.2   58.43   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n#> \n#> Type of estimator: ML (maximum likelihood)\n#> Log-likelihood: 4.187e+04 on 4 Df\n#> Pseudo R-squared: 0.005735\n#> Number of iterations: 59 (BFGS) + 14 (Fisher scoring)\n</code></pre>\n:::\n\n\nOnce again, a significant difference in surprise between remembered and recalled stories in the opposite of the expected direction. However, CCR has a fundamental problem that needs to be addressed.\n\nEmbeddings capture the overall \"vibes\" of a text, including its tone and dialect. With CCR, we are comparing the \"vibes\" of a questionnaire written by academics to the \"vibes\" of narratives written by Hippocorpus participants. By comparing these vectors, we are not just measuring how much surprise is in each text---we are also measuring the extent to which each text is in the style of a questionnaire written by academics. This introduces a confounding variable into our analysis---questionnaire-ness.\n\nThe questionnaire-ness problem means that CCR is most effective for analyzing texts that bear a strong similarity to the questionnaire itself. For example, if you are analyzing participant descriptions of their own values, and your questionnaire items are statements about values in the first person (as any questionnaires are), CCR is likely to work well, especially with the improvement described in @sec-dimension-projection and @sec-ccr-improvement. With this method, you can compare participant responses to the questionnaire without actually administering the questionnaire itself; participants can answer in their own words, which CCR will compare to the wording of the questionnaire. \n\n::: {.callout-tip icon=\"false\"}\n## Advantages of CCR\n\n-   **Can Apply Existing Questionnaires**\n-   **Effectively Uses Contextualized Embeddings**\n-   **Allows Free Response Items:** Compares free-written participant responses with questionnaire wording.\n:::\n\n::: {.callout-important icon=\"false\"}\n## Disadvantages of CCR\n\n-   **Limited Applicability:** Less effective on texts that do not contain similar content and wording to the questionnaires\n-   **Risks Measuring Questionnaire-ness:** This risk can be mitigated by using an anchored vector (@sec-ccr-improvement) \n:::\n\n## Reasoning in Vector Space: Beyond Cosine Similarity and Dot Products\n\n### Additive Analogies {#sec-parallelograms}\n\nNearly every introduction to word embeddings opens with their analogical property. This is for good reason: it is extremely cool. Embeddings can be added to each other in order to arrive at new concepts. Here's an example, using word2vec embeddings reduced to two dimensions with PCA:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](navigating-vectorspace_files/figure-html/unnamed-chunk-9-1.png){width=768}\n:::\n:::\n\n\nIf we subtract the embedding of \"man\" from the embedding of \"woman\", we get the vector shown in blue. This vector represents the move from male to female gender. A vector between two embeddings is called an **anchored vector**. So when we add the man-woman anchored vector to the embedding of \"aunt\", we get very close to the embedding of \"uncle\". This property was first noted in word2vec [@mikolov_etal_2013], and GloVe [@pennington_etal_2014] was specifically designed with it in mind. \n\n::: {.callout-tip}\n## Additive Analogies in Contextualized Embeddings\n\nNotice that the analogical property relies on the magnitude of the vectors---if some vectors were shorter or longer than necessary, the parallelogram would not fit. This means that analogical reasoning may not be applicable to LLM embeddings, which are often organized in nonlinear patterns [@cai_etal_2021; @ethayarajh_2019; @gao_etal_2019]. Even specialized models like SBERT are generally not designed with the additive analogical property in mind [@reimers_gurevych_2019]. Even though some geometrically motivated methods work fairly well in LLM embeddings, as we will see in @sec-ccr-improvement, there is lots of room for improvement in this area.[^analogies-1]\n:::\n\n[^analogies-1]: There are some promising methods for getting more geometrically regular embeddings out of LLMs. For example, averaging the last two layers of the model seems to help [@li_etal_2020]. Taking a different approach, @ethayarajh_2019 created static word embeddings from an LLM by running it on a large corpus and taking the set of each wordâ€™s contextualized representations from all the places it appears in the corpus. The loadings of the first principal component of this set represent the dimensions along which the meaning of the word changes across different contexts. These loadings can themselves be used as a vector embedding which can out-perform GloVe and FastText embeddings on many word vector benchmarks, including analogy solving. This approach worked best for embeddings from the early layers of the LLM.\n\nThe simplest application of the analogical property is to complete analogies like \"telescope is to astronomy as ________ is to psychology.\" You can find word2vec's answer to this puzzle by subtracting the embedding of \"telescope\" from the embedding of \"astronomy\", adding the result to the embedding of \"psychology\", and finding the embedding with the lowest Euclidean distance to that vector. \n\n### Anchored Vectors For Better Construct Representations {#sec-dimension-projection}\n\nThere is a fundamental problem with all embeddings that additive analogical reasoning can help us solve. Consider the embeddings for \"happy\" and \"sad\". These may seem like opposites, but actually they are likely to be very close to each other in vector space because they both relate to emotional valence. This means that if we try to measure the happiness of words by comparing their embeddings to the embedding for \"happy\", we will actually be measuring the extent to which the words relate to emotion in general. The word \"depression\" might seem happier than the word \"table\", since depression is more emotion-related. This problem can be solved by using **anchored vectors**. Just like we created an anchored vector between \"man\" and \"woman\" to represent masculinity (as opposed to femininity), we can create an anchored vector between \"happy\" and \"sad\" to represent happiness (as opposed to sadness). As we saw in @sec-parallelograms, anchored vectors can be applied wherever necessary in embedding space. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nhappy_vec <- predict(word2vec_mod, \"happy\", type = \"embedding\")\nsad_vec <- predict(word2vec_mod, \"sad\", type = \"embedding\")\n\nhappiness_anchor <- happy_vec - sad_vec\n```\n:::\n\n\nTo measure constructs with an anchored vector, take the dot product of your text embeddings with the anchored vector. This is the equivalent of \"projecting\" the embeddings down onto the scale between one end of the anchored vector and the other.[^anchored-vecs-1] \n\n[^anchored-vecs-1]: For an intuitive explanation of why the dot product is equivalent to a projection, see [3blue1brown's video on the subject.](https://youtu.be/LyGKycYT2v0?si=86cfrN9DP9xw5HUx). Incidentally, the dot product with the anchored vector is also equivalent to the dot product with the positive embedding (e.g. \"happy\") minus the dot product with the negative vector (e.g. \"sad\").\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](navigating-vectorspace_files/figure-html/unnamed-chunk-11-1.png){width=768}\n:::\n:::\n\n\nBy projecting each embedding down onto the anchored vector between happy and sad, we create a scale from happy to sad.[^anchored-vecs-2] This is sometimes referred to as **semantic projection** [@grand_etal_2022]. \n\n[^anchored-vecs-2]: Taking the dot product with an anchored vector yields an unstandardized version of this scale. If you want \"sad\" to be 0 and \"happy\" to be 1 on the scale, use the `anchored_sim()` function included in [our Github repo](https://github.com/rimonim/ds4psych/blob/main/vector_scripts.R). \n\n#### Improving DDR With Anchored Vectors {#sec-ddr-improvement}\n\nIn @sec-polarity, we used two dictionaries to measure surprise _as opposed to anticipation_ with word counts. By creating an anchored vector between surprise and anticipation, we can now replicate that analysis using DDR. The first step is to create a DDR for each dictionary. Since we already have one for surprise from @sec-ddr, we just need to replicate the process for anticipation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get dictionary\nanticipation_dict <- quanteda.sentiment::data_dictionary_NRC$anticipation\n\n# estimate frequency of dictionary words\nanticipation_dict_freqs <- hippocorpus_dfm |> \n  dfm_keep(anticipation_dict) |> \n  quanteda.textstats::textstat_frequency() |> \n  select(feature, frequency)\n\n# word2vec embeddings of dictionary words\nanticipation_ddr <- predict(word2vec_mod, anticipation_dict, type = \"embedding\") |> \n  as_tibble(rownames = \"feature\") |> \n  left_join(anticipation_dict_freqs) |> \n  replace_na(list(frequency = 0))\n\n# average dictionary embedding (weighted by frequency)\nanticipation_ddr <- anticipation_ddr |> \n  summarise(across(V1:V300, ~weighted.mean(.x, frequency, na.rm = TRUE))) |> \n  select(V1:V300) |> \n  unlist()\n```\n:::\n\n\nNow that we have DDRs for both surprise and anticipation, we can create an anchored vector between them:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsurprise_ddr_anchored <- surprise_ddr - anticipation_ddr\n```\n:::\n\n\nWe can now score the Hippocorpus texts by the dot product between their word2vec embeddings and the anchored vector, effectively projecting each one onto a scale between anticipation and surprise.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# score documents by surprise\nhippocorpus_surprise_ddr_anchored <- hippocorpus_word2vec |> \n  rowwise() |> \n  mutate(surprise = dot_prod(c_across(V1:V300), surprise_ddr_anchored)) |> \n  ungroup() |> \n  select(-c(V1:V300))\n\n# rejoin docvars\nhippocorpus_surprise_ddr_anchored <- hippocorpus_surprise_ddr_anchored |> \n  bind_cols(docvars(hippocorpus_corp))\n```\n:::\n\n\nSince the scale is theoretically infinite (a text could have more surprise than the average dictionary embedding for surprise), we can analyze it with a standard linear regression.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsurprise_mod_ddr_anchored <- lm(\n  surprise ~ memType,\n  data = hippocorpus_surprise_ddr_anchored\n  )\n\nsummary(surprise_mod_ddr_anchored)\n```\n\n<pre class=\"r-output\"><code>#> \n#> Call:\n#> lm(formula = surprise ~ memType, data = hippocorpus_surprise_ddr_anchored)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -2.65062 -0.36894 -0.00133  0.37938  2.64127 \n#> \n#> Coefficients:\n#>                 Estimate Std. Error  t value Pr(>|t|)    \n#> (Intercept)     -4.19667    0.01084 -387.242   <2e-16 ***\n#> memTyperecalled  0.01157    0.01529    0.757   0.4493    \n#> memTyperetold   -0.04791    0.01905   -2.515   0.0119 *  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.5689 on 6851 degrees of freedom\n#> Multiple R-squared:  0.001468,\tAdjusted R-squared:  0.001176 \n#> F-statistic: 5.035 on 2 and 6851 DF,  p-value: 0.006531\n</code></pre>\n:::\n\n\nWe found no significant difference between imagined and recalled stories, but we did find a significant difference between imagined and retold stories such that retold stories had slightly less surprise as opposed to anticipation.\n\n#### Improving CCR With Anchored Vectors {#sec-ccr-improvement}\n\nRemember the questionnaire-ness problem with CCR from @sec-ccr? Anchored vectors can help us solve this problem. This time, let's just negate each item from the surprise questionnaire, like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsurprise_items_pos <- c(\n  \"I was extremely surprised by the outcome of the event.\",\n  \"The outcome of the event was extremely interesting.\",\n  \"The outcome of the event was extremely new.\"\n  )\n\nsurprise_items_neg <- c(\n  \"I was not surprised at all by the outcome of the event.\",\n  \"The outcome of the event was not interesting at all.\",\n  \"The outcome of the event was not new at all.\"\n  )\n```\n:::\n\n\nThis approach has the advantage of maintaining most of the original wording. By creating an anchored vector between the positive and negative CCRs, we can disregard this questionnaire-y wording, focusing only on the direction between lots of surprise and no surprise at all. Even though this approach makes big assumptions about the linearity of the contextualized embedding space (@sec-parallelograms), it has been shown to work fairly well for a variety of constructs and models [@grand_etal_2022]. It is particularly applicable to the Hippocorpus data, since the texts are first-person narratives about an event, just like the questionnaire items.\n\nLet's create the new anchored CCR and use it to reanalyze the Hippocorpus data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# embed items (using the same model as we used before)\nlibrary(text)\n  \nsurprise_neg_sbert <- textEmbed(\n  surprise_items_neg,\n  model = \"sentence-transformers/all-MiniLM-L12-v2\", # model name\n  layers = -2,  # second to last layer (default)\n  tokens_select = \"[CLS]\", # use only [CLS] token\n  dim_name = FALSE,\n  keep_token_embeddings = FALSE\n  )\n\n# compute negative CCR by averaging item embeddings\nsurprise_neg_ccr <- surprise_neg_sbert$texts[[1]] |>\n  summarise(across(everything(), mean)) |> \n  unlist()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsurprise_ccr_anchored <- surprise_ccr - surprise_neg_ccr\n\n# score documents by surprise\nhippocorpus_surprise_ccr_anchored <- hippocorpus_sbert |> \n  rowwise() |> \n  mutate(surprise = dot_prod(c_across(Dim1:Dim384), surprise_ccr_anchored)) |> \n  ungroup() |> \n  select(-c(Dim1:Dim384))\n\n# linear regression\nsurprise_mod_ccr_anchored <- lm(\n  surprise ~ memType, \n  hippocorpus_surprise_ccr_anchored\n  )\n\nsummary(surprise_mod_ccr_anchored)\n```\n\n<pre class=\"r-output\"><code>#> \n#> Call:\n#> lm(formula = surprise ~ memType, data = hippocorpus_surprise_ccr_anchored)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.39157 -0.06969 -0.00115  0.06810  0.41258 \n#> \n#> Coefficients:\n#>                 Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)     1.061703   0.001922 552.448  < 2e-16 ***\n#> memTyperecalled 0.010240   0.002712   3.775 0.000161 ***\n#> memTyperetold   0.012183   0.003378   3.607 0.000312 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.1009 on 6851 degrees of freedom\n#> Multiple R-squared:  0.00283,\tAdjusted R-squared:  0.002539 \n#> F-statistic: 9.721 on 2 and 6851 DF,  p-value: 6.083e-05\n</code></pre>\n:::\n\n\nWe found a significant difference between imagined and recalled stories such that recalled stories had more surprising content (p < .001)! We also found that retold stories had more surprising content than imagined stories (p < .001). These results support Sap et al.'s hypothesis that true autobiographical stories would include more surprising events than imagined stories.\n\n**An example of using anchored vectors and CCR in research:** @simchon_etal_2023 collected 10,000 posts from the [r/depression](https://www.reddit.com/r/depression) subreddit, along with a control group of 100 posts each from 100 randomly selected subreddits. They then used a variant of SBERT, `all-MiniLM-L6-v2` (see @sec-contextualized-embeddings), to compute CCR embeddings of a psychological questionnaire measuring \"locus of control,\" the feeling that you have control over your own life. The questionnaire included items measuring an internal locus of control (\"I have control\"), and items measuring an external locus of control (\"External forces have control\"). Simchon et al. constructed an anchored vector to capture the direction between internal and external locus of control, and projected embeddings of the Reddit posts onto that vector to measure how much each post reflected an internal vs. an external locus of control. They found that posts in r/depression exhibited a more external locus of control than posts in the control group.\n\n### Correlational Anchored Vectors {#sec-correlational-anchors}\n\nIn @sec-generating-dictionaries, we used the [Crowdflower Emotion in Text dataset](https://data.world/crowdflower/sentiment-analysis-in-text) to generate a new dictionary for the emotion of surprise. We can use a similar approach to generate an anchored vector. Remember that the anchored vector for surprise is simply a direction in the embedding space. Rather than finding this direction by subtracting a negative construct embedding from a positive one (as we did in @sec-ccr-improvement and @sec-ccr-improvement), we can use machine learning to find the direction that best represents surprise in a training dataset.\n\nTo train an anchored vector on the Crowdflower dataset, we will first need to embed its 40,000 Twitter posts. We will do this just as we did for the Hippocorpus texts in @sec-word2vec.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# data from https://data.world/crowdflower/sentiment-analysis-in-text\ncrowdflower <- read_csv(\"data/text_emotion.csv\") |> \n  rename(text = content) |> \n  mutate(\n    doc_id = as.character(tweet_id),\n    surprise = if_else(sentiment == \"surprise\", \"surprise\", \"no surprise\"),\n    surprise = factor(surprise, levels = c(\"no surprise\", \"surprise\"))\n  )\n\ncrowdflower_dfm <- crowdflower |> \n  corpus() |> \n  tokens(remove_punct = TRUE, remove_url = TRUE) |> \n  dfm()\n\n# word2vec document embeddings\ncrowdflower_word2vec <- crowdflower_dfm |> \n  textstat_embedding(word2vec_mod)\n\ncrowdflower <- crowdflower |> \n  left_join(crowdflower_word2vec, by = \"doc_id\")\n```\n:::\n\n\nWith Partial Least Squares (PLS) regression [@mevik_wehrens_2007; @wold_etal_2001], which finds directions in the feature space that best correlate with the dependent variable (in this case, surprise), we can create a **correlational anchored vector**. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\n```\n\n<pre class=\"r-output\"><code>#> Loading required package: lattice\n</code></pre><pre class=\"r-output\"><code>#> \n#> Attaching package: 'caret'\n</code></pre><pre class=\"r-output\"><code>#> The following object is masked from 'package:purrr':\n#> \n#>     lift\n</code></pre>\n\n```{.r .cell-code}\nset.seed(2024)\npls_surprise <- train(\n  surprise ~ ., \n  data = select(crowdflower, surprise, V1:V300), \n  method = \"pls\",\n  scale = FALSE,  # keep original embedding dimensions\n  trControl = trainControl(\"cv\", number = 10),  # cross-validation\n  tuneLength = 1  # only 1 component (our anchored vector)\n)\n\nsurprise_anchored_pls <- pls_surprise$finalModel$projection[,1]\n```\n:::\n\n\nWith the new correlational anchored vector, we can redo our analysis from @sec-ddr.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# score documents by surprise\nhippocorpus_surprise_anchored_pls <- hippocorpus_word2vec |> \n  rowwise() |> \n  mutate(surprise = dot_prod(c_across(V1:V300), surprise_anchored_pls)) |> \n  ungroup() |> \n  select(-c(V1:V300))\n\n# rejoin docvars\nhippocorpus_surprise_anchored_pls <- hippocorpus_surprise_anchored_pls |> \n  bind_cols(docvars(hippocorpus_corp))\n\nsurprise_mod_anchored_pls <- lm(\n  surprise ~ memType,\n  data = hippocorpus_surprise_anchored_pls\n  )\n\nsummary(surprise_mod_anchored_pls)\n```\n\n<pre class=\"r-output\"><code>#> \n#> Call:\n#> lm(formula = surprise ~ memType, data = hippocorpus_surprise_anchored_pls)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -1.28741 -0.20195  0.00297  0.21233  1.03080 \n#> \n#> Coefficients:\n#>                  Estimate Std. Error  t value Pr(>|t|)    \n#> (Intercept)     -1.506064   0.005941 -253.519   <2e-16 ***\n#> memTyperecalled  0.105878   0.008384   12.629   <2e-16 ***\n#> memTyperetold    0.093503   0.010442    8.955   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.3119 on 6851 degrees of freedom\n#> Multiple R-squared:  0.02523,\tAdjusted R-squared:  0.02494 \n#> F-statistic: 88.65 on 2 and 6851 DF,  p-value: < 2.2e-16\n</code></pre>\n:::\n\n\nOnce again we find significant results in support of @sap_etal_2022!\n\n### Machine Learning Methods {#sec-machine-learning-methods}\n\nAfter @sec-correlational-anchors, you may wonder why we stopped at a single direction in embedding space. Why not go all out with the machine learning? If you wondered this, great job! Psychologists are increasingly training machine learning algorithms on text embeddings to quantify relevant constructs [@kjell_etal_2022]. Indeed, this is the approach used to generate [the cover of this book](https://github.com/rimonim/ds4psych/blob/main/cover.R).\n\nWith machine learning approaches, the nonlinearity of contextualized embedding spaces becomes less of a problem. Given enough training data, we can specify a model that can capture nonlinear patterns, such as a [support vector machine](https://rpubs.com/uky994/593668). We could also simultaneously use embeddings from multiple layers of the LLM with `aggregation_from_layers_to_tokens = \"concatenate\"` in `textEmbed()`. Some research advises using both the `[CLS]` token and an average of the other token embeddings as input to the machine learning model [@lee_etal_2023]. There is no blanket rule about which machine learning algorithms work best with embeddings, but @kjell_etal_2022 recommend ridge regression for continuous outputs, and random forest for classification. If you are not comfortable fitting machine learning algorithms in R, you can use the convenience function, `textTrain()`, provided by the `text` package. In the example code below, we train a random forest model on the Crowdflower dataset, and use it to identify surprise in the Hippocorpus texts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(text)\n\n# embed Hippocorpus texts\nhippocorpus_subset_distilroberta <- textEmbed(\n  hippocorpus_df$story,\n  model = \"distilroberta-base\",\n  layers = c(-2, -1),  # last two layers\n  # aggregate token embeddings in each layer, then concatenate layers\n  aggregation_from_tokens_to_texts = \"mean\", \n  aggregation_from_layers_to_tokens = \"concatenate\", \n  dim_name = FALSE,\n  keep_token_embeddings = FALSE\n  )\n\n# load training set\nset.seed(2024)\ncrowdflower_subset <- crowdflower |> \n  select(doc_id, text, surprise) |> \n  group_by(surprise) |> \n  slice_sample(n = 2000)\n\n# embed training set\ncrowdflower_subset_distilroberta <- textEmbed(\n  crowdflower_subset$text,\n  model = \"distilroberta-base\",\n  layers = c(-2, -1),  # last two layers\n  # aggregate token embeddings in each layer, then concatenate layers\n  aggregation_from_tokens_to_texts = \"mean\", \n  aggregation_from_layers_to_tokens = \"concatenate\", \n  dim_name = FALSE,\n  keep_token_embeddings = FALSE\n  )\n\n# fit random forest model\nsurprise_randomforest <- textTrain(\n  x = crowdflower_subset_distilroberta$texts$texts,\n  y = crowdflower_subset$surprise\n  )\n\n# predict on Hippocorpus texts\nsurprise_pred <- textPredict(\n  surprise_randomforest,\n  hippocorpus_subset_distilroberta$texts$texts\n  )\n```\n:::\n\n\n**An example of using embedding-based machine learning models trained in research:** @chersoni_etal_2021 used PLS regression to map word embeddings from various models (including word2vec, fastText, GloVe, and BERT) to human-rated semantic features derived from research in cognitive psychology. By comparing the performance of the different models, they could draw inferences about the types of information encoded in words. They found that cognition, causal reasoning, and social content were best predicted across models. General categories (e.g. vision, arousal) tended to be better predicted than specific characteristics (e.g. dark, light, happy, sad).\n",
    "supporting": [
      "navigating-vectorspace_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}