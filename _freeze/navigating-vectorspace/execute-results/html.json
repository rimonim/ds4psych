{
  "hash": "6cc1182f48ad031ebe42d764f2e06f99",
  "result": {
    "engine": "knitr",
    "markdown": "# Navigating Vector Space {#sec-navigating-vectorspace}\n\n\n\n\n\n::: {.callout-important}\n## This page is still under construction. Come back soon!\n:::\n\n@gunther_etal_2019\n\n## Representing Psychological Constructs\n\nIn @sec-decontextualized-embeddings we measured the surprise in texts by comparing their embeddings to that of a single word: \"surprised\". \n\n### Distributed Dictionary Representation (DDR)\n\nNow that we have contextualized embeddings for the Hippocorpus texts, how can we use them to test the hypothesis that true autobiographical stories include more surprise than imagined stories? \n\n@garten_etal_2018\n\n- weight word embeddings by frequency in a corpus?\n  - since word vector magnitudes reflect informativeness [@schakel_wilson_2015; @oyama_etal_2023], averaging the embeddings of a text automatically controls for word frequency, similar to TF-IDF weighting (since more frequent words tend to be less informative) [@ethayarajh_etal_2019]. This is not true in a dictionary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load surprise dictionary\nsurprise_dict <- quanteda.sentiment::data_dictionary_NRC[\"surprise\"]\n\n# estimate frequency of dictionary words\nsurprise_dict_freqs <- hippocorpus_dfm |> \n  dfm_keep(surprise_dict$surprise) |> \n  quanteda.textstats::textstat_frequency() |> \n  select(feature, frequency)\n\n# word2vec embeddings of dictionary words\nsurprise_ddr <- predict(word2vec_mod, surprise_dict$surprise, type = \"embedding\") |> \n  as_tibble(rownames = \"feature\") |> \n  left_join(surprise_dict_freqs) |> \n  replace_na(list(frequency = 0))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n#> `.name_repair` is omitted as of tibble 2.0.0.\n#> ℹ Using compatibility `.name_repair`.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Joining with `by = join_by(feature)`\n```\n\n\n:::\n\n```{.r .cell-code}\n# average dictionary embedding (weighted by frequency)\nsurprise_ddr <- surprise_ddr |> \n  summarise(across(V1:V300, ~weighted.mean(.x, frequency))) |> \n  select(V1:V300) |> \n  unlist()\n\n# document embeddings\nhippocorpus_word2vec <- hippocorpus_dfm |> \n  textstat_embedding(word2vec_mod)\n\n# score documents by surprise\nhippocorpus_surprise_ddr <- hippocorpus_word2vec |> \n  rowwise() |> \n  mutate(\n    surprise = cos_sim(c_across(V1:V300), surprise_ddr)\n  ) |> \n  ungroup() |> \n  select(-c(V1:V300))\n\n# rejoin docvars\nhippocorpus_surprise_ddr <- hippocorpus_surprise_ddr |> \n  bind_cols(docvars(hippocorpus_corp))\n\nsurprise_mod_ddr <- lm(surprise ~ memType, hippocorpus_surprise_ddr)\n\nsummary(surprise_mod_ddr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> lm(formula = surprise ~ memType, data = hippocorpus_surprise_ddr)\n#> \n#> Residuals:\n#>       Min        1Q    Median        3Q       Max \n#> -0.216732 -0.013010  0.001501  0.014793  0.069393 \n#> \n#> Coefficients:\n#>                   Estimate Std. Error  t value Pr(>|t|)    \n#> (Intercept)      0.7192923  0.0004246 1694.092  < 2e-16 ***\n#> memTyperecalled -0.0049621  0.0005992   -8.281  < 2e-16 ***\n#> memTyperetold   -0.0054746  0.0007463   -7.336 2.46e-13 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.02229 on 6851 degrees of freedom\n#> Multiple R-squared:  0.01263,\tAdjusted R-squared:  0.01234 \n#> F-statistic: 43.83 on 2 and 6851 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nThese results look similar to the ones we got using only the embedding of the word \"surprised\": Both recalled and retold stories have _less_ surprise-related language than imagined ones (p < .001).\n\n### Contextualized Construct Representation (CCR)\n\n@atari_etal_2023\n\nAdapted from the scale used in @choi_choi_2010 and @choi_nisbett_2000\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsurprise_items_pos <- c(\n  \"I was extremely surprised by the outcome of the event.\",\n  \"The outcome of the event was extremely interesting.\",\n  \"The outcome of the event was extremely new.\"\n)\n\nsurprise_items_neg <- c(\n  \"I was not surprised at all by the outcome of the event.\",\n  \"The outcome of the event was not interesting at all.\",\n  \"The outcome of the event was not new at all.\"\n)\n```\n:::\n\n\n### Correlational Methods\n\ni.e. averaging from group in training set\n\n\n::: {.cell}\n\n:::\n\n\n## Reasoning in Vector Space: Beyond Cosine Similarity\n\n### Parallelograms {#sec-parallelograms}\n\nIntroduced with word2vec by @mikolov_etal_2013\n\nGlove [@pennington_etal_2014] is designed with this property in mind. Transformer models are not.\n\n- Transformer models, including BERT, tend to generate embedding spaces that do not center at zero and which tend to form a narrow cone in the vector space [@ethayarajh_2019; @gao_etal_2019]\n  - in BERT, token embeddings in the same sentence become more dissimilar to each other in upper layers but are still more similar than randomly sampled words are on average\n  - BERT aggregated text embeddings perform worse than word2vec and GloVe when analyzed using cosine similarity [@reimers_gurevych_2019], though averaging the embeddings from the last two layers of BERT can improve this [@li_etal_2020]\n  \n  static embeddings created by taking the first principal component of a word’s contextualized representations out- perform GloVe and FastText embeddings on many word vector benchmarks [@ethayarajh_2019]\n\n\n::: {.cell}\n\n:::\n\n\n#### Improving CCR With Geometric Reasoning\n\nIn CCR, for example, there’s a difference between agreeing with statements in a questionnaire and actually writing like a questionnaire. How do I know that I’m measuring how anxious/depressed the comments are and not how questionnaire-y they are?\nPotential workaround: reverse all of the statements in the questionnaire (as for reverse-coded items), add them to the original questionnaire, and get the average embedding of the construct-neutral questionnaire. Then subtract this embedding from the original questionnaire average embedding.\n\n### Advanced Similarity Measures {#sec-advanced-similarity}\n\n#### Dot Product\n\n#### Jaccard similarity\n\n#### Mutual Information\n\n#### Jensen–Shannon divergence\n\n### Semantic Projection {#sec-dimension-projection}\n\n@grand_etal_2022\n\n#### Improving CCR With Semantic Projection\n\n**An example of using dimension projection and CCR in research:** @simchon_etal_2023 collected 10,000 messages from the [r/depression](https://www.reddit.com/r/depression) subreddit, along with a control group of 100 messages each from 100 randomly selected subreddits. They then used a variant of SBERT, `all-MiniLM-L6-v2` (see @sec-contextualized-embeddings), to compute CCR embeddings of a psychological questionnaire measuring \"locus of control,\" the feeling that you have control over your own life. The questionnaire included items measuring an internal locus of control (\"I have control\"), and items measuring an external locus of control (\"External forces have control\"). Simchon et al. constructed an anchored vector to capture the direction between internal and external locus of control, and projected embeddings of the Reddit posts onto that vector to measure how much each post reflected an internal vs. an external locus of control. They found that posts in r/depression exhibited a more external locus of control than posts in the control group.\n\n### Machine Learning Methods {#sec-machine-learning-methods}\n\n@kjell_etal_2022\n\n@chersoni_etal_2021 used partial least squares regression to map distributional embeddings to human-rated semantic features derived from research in cognitive psychology.\n\nSome research advises using both the `[CLS]` token and an aggregation of the other token embeddings [@lee_etal_2023]\n",
    "supporting": [
      "navigating-vectorspace_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}