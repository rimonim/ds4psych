{
  "hash": "ffbcb2c40317a568c19b95e52872606e",
  "result": {
    "engine": "knitr",
    "markdown": "# Web Scraping {#sec-scraping}\n\n\n\n\n\nIn some cases, data are available for free on the internet, but the host does not provide an API. In such cases, we can write code to retrieve the data directly from the web page. This is called web scraping.\n\n## Be Polite\n\nEach time you visit a website, the website's server must send the requested data to your computer. For normal web browsing by humans, this is not a problem. But often, scraping requires your code to visit many web pages. For example, say we wanted a list of poems by English poets, along with the birth dates of their authors. We might have the computer scrape the [list of English poets from the Poetry Foundation](https://www.poetryfoundation.org/poets/browse#page=1&sort_by=recently_added&region=175), which requires clicking through 17 pages of search results, and retrieve each author's birth date and the URL for each author's full list of poems. We could then have the computer visit each of those URLs and retrieve the URL for each individual poem by each author. Finally, the computer would visit each poem URL and retrieve the text of the poem. In all, this process might require visiting hundreds or even thousands of web pages.\n\nFor the same reasons that APIs generally have rate limits ([@sec-apis]), web scraping algorithms should be polite---not, for example, requesting thousands of pages in the space of a few seconds. If you are not polite, you are likely to get banned as a bot.\n\nTo read more about web scraping etiquette and easy ways to implement it, see [the relevant chapter in R for Data Science](https://r4ds.hadley.nz/webscraping), and the homepage of the  [`polite`](https://dmi3kno.github.io/polite/) package. \n\n## A Simple Example\n\nTo give you a taste for what web scraping is like, we will give a simple example of scraping a single page.\n\nThe page in this case will be [the blog reel of one of the authors of this book, Louis](https://rimonim.github.io/blog.html).\n\n![](images/scraping1.png)\n\nWe will scrape the name and date of each blog post using the [`rvest`](https://rvest.tidyverse.org) package. First, we retrieve the raw html code of the webpage. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rvest)\n\nhtml <- read_html(\"https://rimonim.github.io/blog.html\")\n```\n:::\n\n\nThen, we have to identify the name of the particular objects we are looking for. In most cases, this can be done by right clicking and pressing \"Inspect Element\" in the web browser.^[Using \"Inspect Element\" on more complicated web pages can be difficult. For finding \nelement names more easily, we recommend [SelectorGadget](https://rvest.tidyverse.org/articles/selectorgadget.html)]\n\n![](images/scraping2.png)\n\nNow that we know that blog titles are called \"h3.no-anchor.listing-title,\" we can extract those objects from the raw html using `html_elements()` and convert them to regular text using `html_text2()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npost_titles <- html |> \n  html_elements(\"h3.no-anchor.listing-title\") |> \n  html_text2()\n\nhead(post_titles)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] \"Americans Have Eight Kinds of Days\"                                               \n#> [2] \"Advanced Machine Learning Approaches for Detecting Trolls on Twitter\"             \n#> [3] \"Shockingly, Most Reddit Environmentalists are not Greta Thunberg\"                 \n#> [4] \"Comparing Four Methods of Sentiment Analysis\"                                     \n#> [5] \"Do Employees Tend to Have the Same First Name as Their Bosses?\"                   \n#> [6] \"Designing a Poster to Visualize the Timeline of Philosophers in the Islamic World\"\n```\n\n\n:::\n:::\n\n\nTo build a dataset with multiple variables, we repeat this process for each variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposts <- tibble(\n  date = html |> \n    html_elements(\"div.listing-date\") |> \n    html_text2(),\n  title = post_titles\n)\n\nhead(posts)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 6 × 2\n#>   date         title                                                            \n#>   <chr>        <chr>                                                            \n#> 1 Aug 23, 2023 Americans Have Eight Kinds of Days                               \n#> 2 Jul 20, 2023 Advanced Machine Learning Approaches for Detecting Trolls on Twi…\n#> 3 Jul 12, 2023 Shockingly, Most Reddit Environmentalists are not Greta Thunberg \n#> 4 Jul 10, 2023 Comparing Four Methods of Sentiment Analysis                     \n#> 5 Jun 28, 2023 Do Employees Tend to Have the Same First Name as Their Bosses?   \n#> 6 May 8, 2023  Designing a Poster to Visualize the Timeline of Philosophers in …\n```\n\n\n:::\n:::\n\n\nThis is a very basic example. For a more in-depth tutorial on web scraping (including more complex examples), see [the relevant chapter in _R for Data Science_](https://r4ds.hadley.nz/webscraping).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}