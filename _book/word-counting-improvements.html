<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Louis Teitelbaum and Almog Simchon">
<title>Data Science for Psychology: Natural Language - 16&nbsp; Transforming Word Counts</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./vectorspace-intro.html" rel="next">
<link href="./dla.html" rel="prev">
<link href="./images/favicon.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./quantification.html">Quantifying Psychological Properties of Text</a></li><li class="breadcrumb-item"><a href="./word-counting-improvements.html"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Transforming Word Counts</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Data Science for Psychology: Natural Language</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/rimonim/ds4psych" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction: Why Does Psychology Need Natural Language?</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ethics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Ethics of Data Science in Psychology</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./data-viz.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Visualization</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./aesthetics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Why Aesthetic Choices are Important</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./telling-a-story.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Don’t Distract From the Story</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./word-viz.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Visualizing Distributions of Words</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data-viz-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Additional Resources for Data Visualization</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./data-sources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sources of Data</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./experiments.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Experiments</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./corpora.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Corpus Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./apis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Web APIs</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./scraping.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Web Scraping</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./quantification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quantifying Psychological Properties of Text</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./look-at-your-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Look at Your Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quanteda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Introduction to Quanteda</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tokenization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Tokenization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./word-counting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Dictionary-Based Word Counts</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dla.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Open Vocabulary Word Counting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./word-counting-improvements.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Transforming Word Counts</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./vectorspace-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Introduction to Vector Space</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./decontextualized-embeddings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Bag of Words Embeddings: Decontextualized Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./contextualized-embeddings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Contextualization and Large Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./navigating-vectorspace.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Navigating Vector Space</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linguistic-complexity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Measures of Linguistic Complexity</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./audio-video-image.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Audio, Video, and Image Data</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./transcription.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Transcribing Audio</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Additional Resources</span></span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#text-normalization" id="toc-text-normalization" class="nav-link active" data-scroll-target="#text-normalization"><span class="header-section-number">16.1</span> Text Normalization</a>
  <ul class="collapse">
<li><a href="#occurrence-thresholds" id="toc-occurrence-thresholds" class="nav-link" data-scroll-target="#occurrence-thresholds"><span class="header-section-number">16.1.1</span> Occurrence Thresholds</a></li>
  <li><a href="#sec-stopwords" id="toc-sec-stopwords" class="nav-link" data-scroll-target="#sec-stopwords"><span class="header-section-number">16.1.2</span> Removing Stop Words</a></li>
  </ul>
</li>
  <li><a href="#binary-boolean-tokenization" id="toc-binary-boolean-tokenization" class="nav-link" data-scroll-target="#binary-boolean-tokenization"><span class="header-section-number">16.2</span> Binary (Boolean) Tokenization</a></li>
  <li><a href="#sec-relative-tokenization" id="toc-sec-relative-tokenization" class="nav-link" data-scroll-target="#sec-relative-tokenization"><span class="header-section-number">16.3</span> Relative Tokenization</a></li>
  <li><a href="#sec-anscombe" id="toc-sec-anscombe" class="nav-link" data-scroll-target="#sec-anscombe"><span class="header-section-number">16.4</span> The Anscombe Transform</a></li>
  <li><a href="#sec-tfidf" id="toc-sec-tfidf" class="nav-link" data-scroll-target="#sec-tfidf"><span class="header-section-number">16.5</span> TF-IDF</a></li>
  <li><a href="#sec-smoothing" id="toc-sec-smoothing" class="nav-link" data-scroll-target="#sec-smoothing"><span class="header-section-number">16.6</span> Smoothing</a></li>
  <li><a href="#sec-machine-learning-word-counts" id="toc-sec-machine-learning-word-counts" class="nav-link" data-scroll-target="#sec-machine-learning-word-counts"><span class="header-section-number">16.7</span> Machine Learning Approaches</a></li>
  </ul><div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/rimonim/ds4psych/blob/main/word-counting-improvements.qmd" class="toc-action">View source</a></p><p><a href="https://github.com/rimonim/ds4psych/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title"><span id="sec-word-counting-improvements" class="quarto-section-identifier"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Transforming Word Counts</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header><p>So far, we have used statistical methods that work with raw counts, like negative binomial regression in <a href="word-counting.html#sec-modeling-word-counts"><span>Section&nbsp;14.3.1</span></a>, and likelihood ratio testing in <a href="dla.html#sec-keyness"><span>Section&nbsp;15.2</span></a> (with the minor exception of <a href="word-counting.html#sec-word-scoring"><span>Section&nbsp;14.5</span></a>, which used averaged scores). Raw token counts are difficult to work with: They are not normally distributed, they do not usually change linearly with predictors, and they are overly sensitive to quirks of linguistic style and text length.</p>
<p>Researchers and engineers have proposed various ways to fix these problems by transforming the raw counts or by transforming the text before performing the counting. In this chapter, we introduce a few of the most common transformations. These transformations can be applied to any analysis of word counts, including dictionary-based analysis (<a href="word-counting.html"><span>Chapter&nbsp;14</span></a>) and open vocabulary methods (<a href="dla.html"><span>Chapter&nbsp;15</span></a>), to bypass certain problems or bring out certain features of the data. As always, each technique has its own advantages and disadvantages.</p>
<section id="text-normalization" class="level2" data-number="16.1"><h2 data-number="16.1" class="anchored" data-anchor-id="text-normalization">
<span class="header-section-number">16.1</span> Text Normalization</h2>
<p>The simplest way to transform word counts is by transforming the text itself. We have already seen some simple examples of this in <a href="tokenization.html#sec-custom-preprocessing"><span>Section&nbsp;13.2</span></a>: removing punctuation, symbols, or URLs before tokenization. Such transformations are often called <strong>text normalization</strong>, since they get rid of the quirks in each text and ensure that everything follows a standard format.</p>
<section id="occurrence-thresholds" class="level3" data-number="16.1.1"><h3 data-number="16.1.1" class="anchored" data-anchor-id="occurrence-thresholds">
<span class="header-section-number">16.1.1</span> Occurrence Thresholds</h3>
<p>Besides removing or standardizing certain types of tokens, like URLs, researchers commonly enforce an <strong>occurrence threshold</strong>, removing any token that occurs less than a certain number of times in the data. Occurrence thresholds can be calculated on the full dataset (term frequency) or between documents (document frequency; e.g.&nbsp;removing tokens used in fewer than 1% of documents). Using a document frequency threshold is often beneficial, since sometimes a single document will use a token a lot—either because it happens to be discussing a specific topic or because of a quirk of the author’s language style—and drive up the overall frequency in a misleading way.</p>
<p>Occurrence thresholds can be performed on DFMs in Quanteda using the <code>dfm_trim()</code> function, with either a <code>min_termfreq</code>, a <code>min_docfreq</code>, or both. Maximum frequency thresholds can also be imposed.</p>
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># remove tokens used by fewer than 1% of documents</span></span>
<span><span class="va">hippocorpus_dfm_threshold</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_dfm</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">dfm_trim</span><span class="op">(</span>min_docfreq <span class="op">=</span> <span class="fl">0.01</span>, docfreq_type <span class="op">=</span> <span class="st">"prop"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advantages of Occurrence Thresholds
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>
<strong>Cleaner Results:</strong> Occurence thresholds are an easy way to remove quirks of individuals’ writing styles, or very rare terms that complicate analysis without adding much information.</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Disadvantages of Occurrence Thresholds
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>
<strong>Arbitrary:</strong> Determining what threshold to use can be difficult, and runs the risk of excluding important information from the analysis.</li>
</ul>
</div>
</div>
</section><section id="sec-stopwords" class="level3" data-number="16.1.2"><h3 data-number="16.1.2" class="anchored" data-anchor-id="sec-stopwords">
<span class="header-section-number">16.1.2</span> Removing Stop Words</h3>
<p>In natural language processing, a common step in text normalization is to remove “stop words,” everyday words like “the” and “of” that do not contribute much to the meaning of the text. Indeed, Quanteda offers a built-in list of stop words:</p>
<div class="cell">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">stopwords</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "i"      "me"     "my"     "myself" "we"     "our"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Although removing stop words can be useful for analyzing the <em>topics</em> of texts, it is generally a bad idea when you are interested in the <em>psychology</em> of texts. This is because <strong>the forms in which people choose to write a word—including stop words—are often predictive of their personality</strong>. For example, neurotic people tend to use more first-person singulars <span class="citation" data-cites="mehl_etal_2006">(<a href="#ref-mehl_etal_2006" role="doc-biblioref">Mehl et al., 2006</a>)</span>, and articles like “the” and “a” are highly predictive of males, being older, and openness <span class="citation" data-cites="schwartz_etal_2013">(<a href="#ref-schwartz_etal_2013" role="doc-biblioref">Schwartz et al., 2013</a>)</span>.</p>
<p>The relationships between language and personality also extend to more subtle patterns. For example, extraverts tend to use longer words <span class="citation" data-cites="mehl_etal_2006">(<a href="#ref-mehl_etal_2006" role="doc-biblioref">Mehl et al., 2006</a>)</span>, those high in openness tend to use more quotations <span class="citation" data-cites="sumner_etal_2011">(<a href="#ref-sumner_etal_2011" role="doc-biblioref">Sumner et al., 2011</a>)</span>, and those high in neuroticism tend to use more acronyms <span class="citation" data-cites="holtgraves_2011">(<a href="#ref-holtgraves_2011" role="doc-biblioref">Holtgraves, 2011</a>)</span>. So if you are looking for psychological differences, be gentle with the text normalization—you never know what strange predictors you might find.</p>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advantages of Removing Stop Words
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>
<strong>Intuitive Appeal:</strong> Removing stop words focuses an analysis on content, rather than form. When people think of differences between texts, they generally think of differences in content.</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Disadvantages of Removing Stop Words
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>
<strong>Removes Important Information:</strong> While words like “the” and “a” may seem insignificant, they often carry important psychological information.</li>
</ul>
</div>
</div>
</section></section><section id="binary-boolean-tokenization" class="level2" data-number="16.2"><h2 data-number="16.2" class="anchored" data-anchor-id="binary-boolean-tokenization">
<span class="header-section-number">16.2</span> Binary (Boolean) Tokenization</h2>
<p>In some cases, it makes sense to stop counting at one—each text either uses a given token or it does not. While this might seem like needlessly throwing away information, binary tokenization fixes a core problem with the bag of words assumption (BOW). Recall from <a href="tokenization.html#sec-quanteda-dfms"><span>Section&nbsp;13.3</span></a> that BOW imagines that each author or topic has its characteristic bag of words, and speaking or writing is just a matter of pulling those words out of the bag one at a time at random. A central problem with this picture is that words are not pulled out one at a time at random—the word I am writing now is intimately tied to the words immediately before it. It may be very unlikely overall that I will write “parthenon,” but if I write it once, it is very likely that I will write it again in the same paragraph. This is because I am probably writing about the Parthenon.</p>
<p>The non-independence of words in text means that the difference between zero occurrences of “parthenon” and one occurrence is much more meaningful than the difference between one and two. If a particular token sometimes occurs lots of times in text, statistical procedures like regression may be led to focus on that variance rather than on the more interesting first occurrence. Binary tokenization is the simplest way to avoid this problem.</p>
<p>In Quanteda, a DFM can be converted to binary tokenization with <code>dfm_weight(scheme = "boolean")</code>.</p>
<div class="cell">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">hippocorpus_dfm_binary</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_dfm</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">dfm_weight</span><span class="op">(</span>scheme <span class="op">=</span> <span class="st">"boolean"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">hippocorpus_dfm_binary</span>, max_ndoc <span class="op">=</span> <span class="fl">6</span>, max_nfeat <span class="op">=</span> <span class="fl">6</span><span class="op">)</span></span>
<span><span class="co">#&gt; Document-feature matrix of: 6,854 documents, 27,667 features (99.50% sparse) and 6 docvars.</span></span>
<span><span class="co">#&gt;                                 features</span></span>
<span><span class="co">#&gt; docs                             concerts are my most favorite thing</span></span>
<span><span class="co">#&gt;   32RIADZISTQWI5XIVG5BN0VMYFRS4U        1   1  1    1        1     1</span></span>
<span><span class="co">#&gt;   3018Q3ZVOJCZJFDMPSFXATCQ4DARA2        0   0  1    0        0     0</span></span>
<span><span class="co">#&gt;   3IRIK4HM3B6UQBC0HI8Q5TBJZLEC61        0   0  1    1        0     1</span></span>
<span><span class="co">#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG04RAI        0   1  1    0        0     0</span></span>
<span><span class="co">#&gt;   3MTMREQS4W44RBU8OMP3XSK8NMJAWZ        0   0  1    0        0     0</span></span>
<span><span class="co">#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG06AR3        0   0  1    0        0     0</span></span>
<span><span class="co">#&gt; [ reached max_ndoc ... 6,848 more documents, reached max_nfeat ... 27,661 more features ]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To use binary tokenization in dictionary-based analysis, you can also perform the dictionary look-up first (<a href="word-counting.html"><span>Chapter&nbsp;14</span></a>) and then convert it to binary with <code>mutate(surprise_binary = as.integer(surprise &gt; 0))</code>.</p>
<p>Keep in mind: Once you convert your DFM to binary tokenization, you are no longer working with a count variable. This means that the negative binomial regression models we covered in <a href="word-counting.html"><span>Chapter&nbsp;14</span></a> are no longer appropriate. Instead, if you want to model the binary tokenization as a dependent variable, you will have to use a binary model like logistic regression.</p>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advantages of Binary Tokenization
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>
<strong>Removes Non-Independence of Observations:</strong> Raw word counts can be misleading (to both humans and statistical models) because the observations are not independent; the more a text uses a word, the more likely it is to use that word again. Binary tokenization avoids this problem by only counting one event per text.</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Disadvantages of Binary Tokenization
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>
<strong>Devalues Common Tokens:</strong> With binary tokenization, you might miss differences in common tokens like “the.” Since almost every text uses “the” at least once, you won’t be able to detect that one group uses “the” more often than another.</li>
<li>
<strong>Difficult to Control for Text Length:</strong> The longer a text is, the more likely any given word is to appear in it. But when we stop counting after the first word, this relationship becomes especially difficult to characterize. When working with shorter texts, beware of mistaking differences in text length for differences in the probability of a word appearing!</li>
</ul>
</div>
</div>
</section><section id="sec-relative-tokenization" class="level2" data-number="16.3"><h2 data-number="16.3" class="anchored" data-anchor-id="sec-relative-tokenization">
<span class="header-section-number">16.3</span> Relative Tokenization</h2>
<p>The most common transformation that researchers apply to word counts is dividing them by the total number of words (or other tokens) in the text. The resulting ratio is referred to as a <strong>relative frequency</strong>. This strategy has intuitive appeal (e.g.&nbsp;what percentage of the words are surprise-related), and the value of intuitive appeal should not be discounted <a href="telling-a-story.html#sec-simplify-the-story">see&nbsp;<span>4.1</span></a>. But working with ratios or percentages can cause problems with statistical analysis. This is because dividing by the total number of words is no guarantee that the total number of words will be properly controlled for—these are two separate variables. Regression analyses on relative frequencies are likely to give false positives when predictors are correlated with text length <span class="citation" data-cites="kronmal_1993">(<a href="#ref-kronmal_1993" role="doc-biblioref">Kronmal, 1993</a>)</span>. This is the same reason why we added total word count as both a log offset and a regular predictor in <a href="word-counting.html#sec-modeling-word-counts"><span>Section&nbsp;14.3.1</span></a>.</p>
<p>Before you start worrying about how to control for text length, make sure to stop and ponder: Do you want to control for text length? Usually we assume that longer documents are longer because the author is more verbose. But what if longer texts are longer because they cover multiple topics, and one of those topics is what we are interested in? In this case, controlling for text length—especially using relative tokenization—will make it look like the longer texts have less of that topic, when in fact they just have more of other topics.</p>
<p>If you decide to use relative tokenization, the process is simple. For dictionary-based word counts, divide your count by the total word count to get a relative frequency. If you want to convert a full DFM to relative tokenization, you can use <code>dfm_weight(scheme = "prop")</code>.</p>
<div class="cell">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">hippocorpus_dfm_relative</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_dfm</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">dfm_weight</span><span class="op">(</span>scheme <span class="op">=</span> <span class="st">"prop"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">hippocorpus_dfm_relative</span>, max_ndoc <span class="op">=</span> <span class="fl">6</span>, max_nfeat <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span>
<span><span class="co">#&gt; Document-feature matrix of: 6,854 documents, 27,667 features (99.50% sparse) and 6 docvars.</span></span>
<span><span class="co">#&gt;                                 features</span></span>
<span><span class="co">#&gt; docs                                concerts         are         my        most</span></span>
<span><span class="co">#&gt;   32RIADZISTQWI5XIVG5BN0VMYFRS4U 0.004926108 0.004926108 0.02463054 0.004926108</span></span>
<span><span class="co">#&gt;   3018Q3ZVOJCZJFDMPSFXATCQ4DARA2 0           0           0.02732240 0          </span></span>
<span><span class="co">#&gt;   3IRIK4HM3B6UQBC0HI8Q5TBJZLEC61 0           0           0.03759398 0.011278195</span></span>
<span><span class="co">#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG04RAI 0           0.006060606 0.02424242 0          </span></span>
<span><span class="co">#&gt;   3MTMREQS4W44RBU8OMP3XSK8NMJAWZ 0           0           0.03086420 0          </span></span>
<span><span class="co">#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG06AR3 0           0           0.01257862 0          </span></span>
<span><span class="co">#&gt; [ reached max_ndoc ... 6,848 more documents, reached max_nfeat ... 27,663 more features ]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>An example of relative tokenization in research:</strong> Golder &amp; Macy (2011) collected messages from all Twitter user accounts (~2.4 million) created between February 2008 and April 2009, and measured positive and negative affect as proportion of in-dictionary (from LIWC) words to total word count. This calculation was done by hour of the day, day of the week, and month of the year, revealing fluctuations in mood in line with circadian rhythms and seasonal changes.</p>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advantages of Relative Tokenization
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>
<strong>Intuitive Appeal:</strong> Relative tokenization makes sense if longer documents are longer because of verbosity, or if the construct of interest does not fluctuate over the course of a longer text (e.g.&nbsp;personality).</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Disadvantages of Relative Tokenization
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>
<strong>Discounts Longer Texts:</strong> If texts are long because they cover multiple topics (or multiple emotions), relative tokenization will dilute true occurrences of the construct of interest.</li>
<li>
<strong>Does Not Control for Text Length:</strong> People often assume that using a percentage will control for the denominator in statistical analyses. This is wrong, which might make ratios like relative tokenization more trouble than they’re worth.</li>
<li>
<strong>Not Normally Distributed:</strong> Dividing count variables by text length does not make them normally distributed. This can cause problems for certain statistical methods. Using the Anscombe transform can partially remedy these problems.</li>
</ul>
</div>
</div>
</section><section id="sec-anscombe" class="level2" data-number="16.4"><h2 data-number="16.4" class="anchored" data-anchor-id="sec-anscombe">
<span class="header-section-number">16.4</span> The Anscombe Transform</h2>
<p>Word counts (whether divided by text length or not) are not normally distributed. In <a href="word-counting.html"><span>Chapter&nbsp;14</span></a>, we avoided this problem by using negative binomial regression. An alternative way to deal with this problem is to try to transform the counts to a normal distribution. Remember that complicated-looking formula from <span class="citation" data-cites="schwartz_etal_2013">Schwartz et al. (<a href="#ref-schwartz_etal_2013" role="doc-biblioref">2013</a>)</span> in <a href="telling-a-story.html#sec-simplify-the-story"><span>Section&nbsp;4.1</span></a>? The upper part of that formula is relative tokenization. The lower part is called the Anscombe transform, and it transforms a Poisson distribution (i.e.&nbsp;a well-behaved count variable) into an approximately normal distribution. The transformed variable can then be used in linear regression. In R, the Anscombe transform can be written as <code>2*sqrt(count + 3/8)</code>.</p>
<p>The Anscombe transform can be useful for analyzing very large numbers of word count variables (as <span class="citation" data-cites="schwartz_etal_2013">Schwartz et al. (<a href="#ref-schwartz_etal_2013" role="doc-biblioref">2013</a>)</span> did) without having to run negative binomial regression each time. But if you are only analyzing a few variables, we recommend against it. This is because word counts do not follow a Poisson distribution and therefore will not be properly normally distributed even after the transform. The Poisson process assumes that words occur independently from each other, and with equal probability throughout a text. In other words, it makes the BOW assumption. As we’ve covered already, this assumption is problematic. In this case, the fact that words are not pulled randomly out of a bag makes word count distributions <em>overdispersed</em>, meaning that the variance of the distribution is higher than expected in a Poisson process. Negative binomial regression may be complicated and take longer to compute, but it tends to be robust to overdispersion.</p>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advantages of the Anscombe Transform
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Computational Efficiency</strong></li>
<li><strong>Addresses Non-Normality of Word Frequencies</strong></li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Disadvantages of the Anscombe Transform
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Wrongly Assumes That Word Counts Follow a Poisson Distribution</strong></li>
</ul>
</div>
</div>
</section><section id="sec-tfidf" class="level2" data-number="16.5"><h2 data-number="16.5" class="anchored" data-anchor-id="sec-tfidf">
<span class="header-section-number">16.5</span> TF-IDF</h2>
<p>Term frequency-inverse document frequency (TF-IDF) is one of the great triumphs of NLP in the last century. It was first introduced by <span class="citation" data-cites="sparckjones_1972">Sparck Jones (<a href="#ref-sparckjones_1972" role="doc-biblioref">1972</a>)</span> and is still widely used half a century later, especially in search engines.</p>
<p>The idea of TF-IDF is to measure how <em>topical</em> a token is in a document In other words, how representative is that token of the particular features of that document as opposed to other documents? Let’s start with term frequency (TF). TF is just relative frequency - a word count divided by the total number of words in the document The problem with relative frequency is that it emphasizes common words that don’t tell us much about the meaning of the document—if a document has a high frequency of “the,” should we conclude that “the” is very important to the meaning of the document? Of course not. To fix this, we multiply TF by inverse document frequency (IDF). Document frequency is the proportion of documents that have at least one instance of our token (i.e.&nbsp;the average binary tokenization across documents). IDF is the log of the inverse of the document frequency. <strong>IDF answers the question: How unusual is it for this token to appear at all in a document?</strong> So “the,” which appears in almost every document, will have a very low IDF—it is not unusual at all. Even though “the” appears a lot in our document (TF is high), its TF-IDF score will be low, reflecting the fact that “the” doesn’t tell us much about the content of this particular document.</p>
<p><span class="math display">\[
TF \cdot IDF = relative\:frequency × \log{\frac{total\:documents}{documents\:with\:token}}
\]</span></p>
<p>You might wonder: If we want to compare how common the token is in this document to how common it is in other documents, shouldn’t we use the inverse <em>token</em> frequency (i.e.&nbsp;the average relative frequency of tokens across documents in the corpus)? Why do we use the inverse document frequency? The answer: TF-IDF does not make the BOW assumption. If all documents were just bags of words, the frequency within documents would be distributed similarly to the frequency between documents (e.g.&nbsp;since the bigram “verbal reasoning” occurs in very few tweets, you would expect the probability of it occurring twice in the same document to be near-impossible). In reality though, it wouldn’t be surprising to see “verbal reasoning” even 3 times in the same tweet if that tweet were discussing verbal reasoning. Multiplying relative <em>term frequency</em> by inverse <em>document frequency</em> provides a measure of exactly how wrong BOW is in this case. In other words, how topical is the token?</p>
<p>In Quanteda, we can convert a DFM to TF-IDF with the <code>dfm_tfidf()</code> function:</p>
<div class="cell">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">hippocorpus_dfm_tfidf</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_dfm</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">dfm_tfidf</span><span class="op">(</span>scheme_tf <span class="op">=</span> <span class="st">"prop"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">hippocorpus_dfm_tfidf</span>, max_ndoc <span class="op">=</span> <span class="fl">6</span>, max_nfeat <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span>
<span><span class="co">#&gt; Document-feature matrix of: 6,854 documents, 27,667 features (99.50% sparse) and 6 docvars.</span></span>
<span><span class="co">#&gt;                                 features</span></span>
<span><span class="co">#&gt; docs                               concerts         are           my</span></span>
<span><span class="co">#&gt;   32RIADZISTQWI5XIVG5BN0VMYFRS4U 0.01296465 0.002577006 0.0005541024</span></span>
<span><span class="co">#&gt;   3018Q3ZVOJCZJFDMPSFXATCQ4DARA2 0          0           0.0006146600</span></span>
<span><span class="co">#&gt;   3IRIK4HM3B6UQBC0HI8Q5TBJZLEC61 0          0           0.0008457352</span></span>
<span><span class="co">#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG04RAI 0          0.003170499 0.0005453711</span></span>
<span><span class="co">#&gt;   3MTMREQS4W44RBU8OMP3XSK8NMJAWZ 0          0           0.0006943381</span></span>
<span><span class="co">#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG06AR3 0          0           0.0002829755</span></span>
<span><span class="co">#&gt;                                 features</span></span>
<span><span class="co">#&gt; docs                                    most</span></span>
<span><span class="co">#&gt;   32RIADZISTQWI5XIVG5BN0VMYFRS4U 0.003125847</span></span>
<span><span class="co">#&gt;   3018Q3ZVOJCZJFDMPSFXATCQ4DARA2 0          </span></span>
<span><span class="co">#&gt;   3IRIK4HM3B6UQBC0HI8Q5TBJZLEC61 0.007156545</span></span>
<span><span class="co">#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG04RAI 0          </span></span>
<span><span class="co">#&gt;   3MTMREQS4W44RBU8OMP3XSK8NMJAWZ 0          </span></span>
<span><span class="co">#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG06AR3 0          </span></span>
<span><span class="co">#&gt; [ reached max_ndoc ... 6,848 more documents, reached max_nfeat ... 27,663 more features ]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Overall, TF-IDF combines the advantages of many simpler word count transformations without many of their downsides. For example, removing stop words focuses the analysis on text content, but may remove important information. TF-IDF discounts uninformative tokens like stop words without removing them outright. Likewise, we saw that binary tokenization solves part of the problem with the BOW assumption, but throws out a lot of information in the process. Once again, TF-IDF turns this problem into a feature (quantifying how topical a token is) without throwing out any information.</p>
<p>Despite all of its benefits, TF-IDF is not widely used in psychology research. This is for two reasons. First, it is difficult to interpret intuitively, and researchers prize interpretability. Second, it focuses the analysis on the <em>topic</em> of texts rather than the subtleties of language style that are often associated with psychological differences (see <a href="#sec-stopwords"><span>Section&nbsp;16.1.2</span></a>). Nevertheless, if your construct is of a less subtle nature—more akin to the “topic” of the text—consider using TF-IDF.</p>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advantages of TF-IDF
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Computational Efficiency</strong></li>
<li>
<strong>Discounts Uninformative Tokens Without Losing Information:</strong> TF-IDF combines the advantages of stop word removal without removing tokens from the analysis.</li>
<li>
<strong>Does Not Rely on BOW:</strong> TF-IDF leverages the discrepancy between term frequency and document frequency to discover patterns of meaning.</li>
<li>
<strong>Proven to Work:</strong> TF-IDF has a long history of use in search engines and automated recommendations. It works surprisingly well for such a simple calculation.</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Disadvantages of TF-IDF
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>
<strong>May Discount Psychologically Relevant Informartion:</strong> TF-IDF operates on an assumption that more frequent tokens (across documents) are less relevant to the construct in question. For semantics this is largely true, but for latent psychological constructs it may not be.</li>
</ul>
</div>
</div>
</section><section id="sec-smoothing" class="level2" data-number="16.6"><h2 data-number="16.6" class="anchored" data-anchor-id="sec-smoothing">
<span class="header-section-number">16.6</span> Smoothing</h2>
<p>In <a href="#sec-relative-tokenization"><span>Section&nbsp;16.3</span></a>, we mentioned that dividing by text length does not adequately control for text length. This is true of ratios in general, but for word frequencies in particular there is extra cause for concern.</p>
<p>Consider this short text written by an imaginary participant, Allison:</p>
<p><em>I was sitting at the table, and <strong>suddenly</strong> I understood.</em></p>
<p>The text has 10 words in total. One of these, “suddenly,” is a surprise-related word. Given only this text, you might estimate the probability of surprise words in Allison’s language at 1/10, the relative frequency. You would be wrong. To see why, imagine that we also wanted to measure the probability of anger-related words. There are no anger-related words in this text. Is the probability of anger-related words then 0/10 = 0? Of course not. If we read more of Allison’s texts, we might very well encounter an anger-related word. Relative frequency therefore underestimates the probability of unobserved words. Conversely, it must overestimate the probability of observed words. So to leave room for the probability of unobserved words, we must admit that the true probability of anger-related words is likely to be a little less than 1/10.</p>
<p>The first solution to this kind of problem was offered by <span class="citation" data-cites="laplace_1816">Laplace (<a href="#ref-laplace_1816" role="doc-biblioref">1816</a>)</span>, who proposed to simply add one to all the frequency counts before computing the relative frequency. This is called <strong>Laplace smoothing</strong>, or sometimes simply <em>add-one smoothing</em>. To perform Laplace smoothing in Quanteda, use the <code>dfm_smooth()</code> function to add one, and then call <code>dfm_weight()</code> as before.</p>
<div class="cell">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">hippocorpus_dfm_laplace</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_dfm</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">dfm_smooth</span><span class="op">(</span>smoothing <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">dfm_weight</span><span class="op">(</span>scheme <span class="op">=</span> <span class="st">"prop"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Warning in asMethod(object): sparse-&gt;dense coercion: allocating vector of size</span></span>
<span><span class="co">#&gt; 1.4 GiB</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">hippocorpus_dfm_laplace</span>, max_ndoc <span class="op">=</span> <span class="fl">6</span>, max_nfeat <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span>
<span><span class="co">#&gt; Document-feature matrix of: 6,854 documents, 27,667 features (0.00% sparse) and 6 docvars.</span></span>
<span><span class="co">#&gt;                                 features</span></span>
<span><span class="co">#&gt; docs                                 concerts          are           my</span></span>
<span><span class="co">#&gt;   32RIADZISTQWI5XIVG5BN0VMYFRS4U 7.176175e-05 7.176175e-05 0.0002152853</span></span>
<span><span class="co">#&gt;   3018Q3ZVOJCZJFDMPSFXATCQ4DARA2 3.590664e-05 3.590664e-05 0.0002154399</span></span>
<span><span class="co">#&gt;   3IRIK4HM3B6UQBC0HI8Q5TBJZLEC61 3.579995e-05 3.579995e-05 0.0003937994</span></span>
<span><span class="co">#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG04RAI 3.592986e-05 7.185973e-05 0.0001796493</span></span>
<span><span class="co">#&gt;   3MTMREQS4W44RBU8OMP3XSK8NMJAWZ 3.593374e-05 3.593374e-05 0.0002156024</span></span>
<span><span class="co">#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG06AR3 3.573343e-05 3.573343e-05 0.0001786671</span></span>
<span><span class="co">#&gt;                                 features</span></span>
<span><span class="co">#&gt; docs                                     most</span></span>
<span><span class="co">#&gt;   32RIADZISTQWI5XIVG5BN0VMYFRS4U 7.176175e-05</span></span>
<span><span class="co">#&gt;   3018Q3ZVOJCZJFDMPSFXATCQ4DARA2 3.590664e-05</span></span>
<span><span class="co">#&gt;   3IRIK4HM3B6UQBC0HI8Q5TBJZLEC61 1.431998e-04</span></span>
<span><span class="co">#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG04RAI 3.592986e-05</span></span>
<span><span class="co">#&gt;   3MTMREQS4W44RBU8OMP3XSK8NMJAWZ 3.593374e-05</span></span>
<span><span class="co">#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG06AR3 3.573343e-05</span></span>
<span><span class="co">#&gt; [ reached max_ndoc ... 6,848 more documents, reached max_nfeat ... 27,663 more features ]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Laplace smoothing is better than nothing, but it is designed for a situation in which the number of possible token types is small and known. In the case of natural language, the number of possible tokens is extremely large and entirely unknown <span class="citation" data-cites="baayen_2001">(<a href="#ref-baayen_2001" role="doc-biblioref">Baayen, 2001</a>)</span>. You can partially make up for this by adding a very small smoothing number instead of 1 (e.g.&nbsp;1 divided by the number of token types in the corpus: <code>smoothing = 1/ncol(hippocorpus_dfm)</code>), but if you are willing to invest a bit more computation time, there are better ways.</p>
<p>During World War II, the German navy encrypted and decrypted its communications using the Enigma machine, which scrambled and unscrambled messages according to an initial input setting. This input setting was updated each day, and part of the process required the operator of the machine to select a three-letter sequence from a large book “at random” <span class="citation" data-cites="good_2000">(<a href="#ref-good_2000" role="doc-biblioref">Good, 2000</a>)</span>. Alan Turing, who was in charge of the decryption effort on the part of the British, quickly understood that the German operators’ choices from this book were not entirely random, and that patterns in their choices could be exploited to narrow down the search. The problem became how to estimate the probability of each three-letter sequence based on a relatively small sample of previously decoded input settings—a sample much smaller than the number of three-letter sequences in the book (which, like the number of possible tokens in English text, was extremely large). Turing solved this problem—essentially the same problem posed by word frequencies in text—by developing a method that used frequencies of frequencies in the sample to estimate the total probability of yet-unseen sequences and correct the observed frequencies based on this estimate. The algorithm was later refined and published by Turing’s assistant I. J. Good <span class="citation" data-cites="good_1953">(<a href="#ref-good_1953" role="doc-biblioref">1953</a>)</span>, and has since seen many variations. Today, the most popular of these variations is the <strong>Simple Good-Turing</strong> algorithm <span class="citation" data-cites="gale_sampson_1995">(<a href="#ref-gale_sampson_1995" role="doc-biblioref">Gale &amp; Sampson, 1995</a>)</span>. Unfortunately, Simple Good-Turing smoothing is not yet implemented in Quanteda, but that is expected to change in the coming months. As soon as it does, we will add it to the textbook.</p>
<p>While smoothing may seem unnecessarily complex, it can be an important safeguard against false positives when analyzing word counts or frequencies. This is because the bias that smoothing accounts for (due to unobserved tokens) is stronger for shorter texts, and grows in a non-linear pattern <span class="citation" data-cites="baayen_2001">(<a href="#ref-baayen_2001" role="doc-biblioref">Baayen, 2001</a>)</span>. This bias can become a confounding factor in regression models, especially when a predictor variable is strongly correlated with text length.</p>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advantages of Smoothing
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Better Estimates of Token Probabilities</strong></li>
<li>
<strong>Corrects Text-Length-Dependent Bias: </strong> This bias can cause false positive results in regression analyses.</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Disadvantages of Smoothing
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>
<strong>Computationally Inefficient: </strong> Laplace smoothing is much more efficient than Simple Good-Turing, but it does not perform as well.</li>
</ul>
</div>
</div>
</section><section id="sec-machine-learning-word-counts" class="level2" data-number="16.7"><h2 data-number="16.7" class="anchored" data-anchor-id="sec-machine-learning-word-counts">
<span class="header-section-number">16.7</span> Machine Learning Approaches</h2>
<p>This chapter is dedicated to methods that can be carried out after computing word counts (<a href="tokenization.html"><span>Chapter&nbsp;13</span></a>) and before analysis (<a href="word-counting.html"><span>Chapter&nbsp;14</span></a>; <a href="dla.html"><span>Chapter&nbsp;15</span></a>). These methods can be thought of as transformations that we apply to word counts to make them better at measuring what we want them to measure. The last of these transformations is the most elaborate one: supervised machine learning.</p>
<p>For supervised machine learning, you need a training dataset of text that is already labeled with your construct of interest. We already saw a dataset like this for the emotion of surprise in <a href="dla.html#sec-generating-dictionaries"><span>Section&nbsp;15.4</span></a>: <a href="https://data.world/crowdflower/sentiment-analysis-in-text">Crowdflower Emotion in Text dataset</a>. Many other good examples can be found in <a href="corpora.html"><span>Chapter&nbsp;8</span></a>. The labels could be generated by participants who read the texts, or by the authors of the texts themselves (e.g.&nbsp;in the form of questionnaires that measure their personality).</p>
<p>Once you have your training dataset, compute word counts for its texts (<a href="tokenization.html"><span>Chapter&nbsp;13</span></a>). These could be dictionary-based counts from a variety of different dictionaries, or they could be individual token counts in the form of a DFM. You can even transform these counts in some of the ways described in this chapter (or even more than one). These counts will become the predictor variables for your machine learning model.</p>
<p>This book is not the place an in-depth tutorial on machine learning<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, but we will give a brief example. Recall that in <a href="dla.html#sec-generating-dictionaries"><span>Section&nbsp;15.4</span></a> we used the <a href="https://data.world/crowdflower/sentiment-analysis-in-text">Crowdflower Emotion in Text dataset</a> to find the words most indicative of surprise in tweets (based on PMI). In <a href="dla.html#sec-generating-dictionaries"><span>Section&nbsp;15.4</span></a> we used these words as a dictionary, but we could get more robust results by using them as predictor variables to train a regularized machine learning model, in this case ridge regression, although <a href="https://topepo.github.io/caret/available-models.html">there are many other options</a>.</p>
<div class="cell">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># New Hippocorpus DFM </span></span>
<span><span class="va">hippocorpus_dfm_ngrams</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_corp</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">tokens</span><span class="op">(</span>remove_punct <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">tokens_ngrams</span><span class="op">(</span>n <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1L</span>, <span class="fl">2L</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span> <span class="co"># 1-grams and 2-grams</span></span>
<span>  <span class="fu">dfm</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">dfm_weight</span><span class="op">(</span>scheme <span class="op">=</span> <span class="st">"prop"</span><span class="op">)</span> <span class="co"># relative tokenization</span></span>
<span></span>
<span><span class="co"># Select only high PMI tokens from Crowdflower</span></span>
<span><span class="va">hippocorpus_dfm_surprise</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_dfm_ngrams</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">dfm_select</span><span class="op">(</span><span class="va">tweet_surprise_dict</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Crowdflower DFM with only tokens that appear in Hippocorpus DFM</span></span>
<span><span class="va">crowdflower_dfm</span> <span class="op">&lt;-</span> <span class="va">crowdflower_dfm</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">dfm_weight</span><span class="op">(</span>scheme <span class="op">=</span> <span class="st">"prop"</span><span class="op">)</span> <span class="op">|&gt;</span>  <span class="co"># relative tokenization</span></span>
<span>  <span class="fu">dfm_select</span><span class="op">(</span><span class="fu">featnames</span><span class="op">(</span><span class="va">hippocorpus_dfm_surprise</span><span class="op">)</span><span class="op">)</span> </span>
<span></span>
<span><span class="co"># Rejoin to Crowdflower DFM labeled data</span></span>
<span><span class="va">crowdflower_train</span> <span class="op">&lt;-</span> <span class="va">crowdflower</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span></span>
<span>    doc_id <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/character.html">as.character</a></span><span class="op">(</span><span class="va">tweet_id</span><span class="op">)</span>,</span>
<span>    label_surprise <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/integer.html">as.integer</a></span><span class="op">(</span><span class="va">sentiment</span> <span class="op">==</span> <span class="st">"surprise"</span><span class="op">)</span></span>
<span>    <span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="va">doc_id</span>, <span class="va">label_surprise</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">left_join</span><span class="op">(</span></span>
<span>    <span class="fu">convert</span><span class="op">(</span><span class="va">crowdflower_dfm</span>, <span class="st">"data.frame"</span><span class="op">)</span>,</span>
<span>    by <span class="op">=</span> <span class="st">"doc_id"</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="op">-</span><span class="va">doc_id</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Balanced training dataset</span></span>
<span><span class="co"># set.seed(2)</span></span>
<span><span class="co"># crowdflower_train &lt;- crowdflower_train |&gt; </span></span>
<span><span class="co">#   group_by(label_surprise) |&gt; </span></span>
<span><span class="co">#   slice_sample(n = sum(crowdflower_train$label_surprise==1))</span></span>
<span></span>
<span><span class="co"># Ridge Regression (10-fold cross-validation)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/topepo/caret/">caret</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">tg</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/expand.grid.html">expand.grid</a></span><span class="op">(</span></span>
<span>  alpha <span class="op">=</span> <span class="fl">0</span>, </span>
<span>  lambda <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span> <span class="op">^</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>, <span class="op">-</span><span class="fl">6</span>, length <span class="op">=</span> <span class="fl">20</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">surprise_ridge</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/train.html">train</a></span><span class="op">(</span></span>
<span>  <span class="va">label_surprise</span> <span class="op">~</span> <span class="va">.</span>, </span>
<span>  data <span class="op">=</span> <span class="va">crowdflower_train</span>, </span>
<span>  method <span class="op">=</span> <span class="st">"glmnet"</span>,</span>
<span>  tuneGrid <span class="op">=</span> <span class="va">tg</span>,</span>
<span>  trControl <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/trainControl.html">trainControl</a></span><span class="op">(</span><span class="st">"cv"</span>, number <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># Prepare Hippocorpus data to run model</span></span>
<span><span class="va">hippocorpus_features</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_dfm_surprise</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">convert</span><span class="op">(</span><span class="st">"data.frame"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">left_join</span><span class="op">(</span></span>
<span>    <span class="fu">convert</span><span class="op">(</span><span class="va">hippocorpus_corp</span>, <span class="st">"data.frame"</span><span class="op">)</span>,</span>
<span>    by <span class="op">=</span> <span class="st">"doc_id"</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># Run model on Hippocorpus for surprise estimation</span></span>
<span><span class="va">surprise_pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">surprise_ridge</span>, newdata <span class="op">=</span> <span class="va">hippocorpus_features</span><span class="op">)</span></span>
<span><span class="va">hippocorpus_features</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_features</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>surprise_pred <span class="op">=</span> <span class="va">surprise_pred</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now that we have a new machine-learning-powered estimate of surprise for the Hippocorpus data, we can retest our hypothesis that true autobiographical stories include more surprise than imagined stories.</p>
<div class="cell">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">surprise_mod_ml</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">surprise_pred</span> <span class="op">~</span> <span class="va">memType</span>, </span>
<span>                      data <span class="op">=</span> <span class="va">hippocorpus_features</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">surprise_mod_ml</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = surprise_pred ~ memType, data = hippocorpus_features)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;       Min        1Q    Median        3Q       Max </span></span>
<span><span class="co">#&gt; -0.004668 -0.004625 -0.004251  0.002972  0.066572 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;                   Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)      5.735e-02  1.458e-04 393.268   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; memTyperecalled -4.174e-04  2.058e-04  -2.028   0.0426 *  </span></span>
<span><span class="co">#&gt; memTyperetold   -4.367e-05  2.563e-04  -0.170   0.8647    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 0.007655 on 6851 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.0006732,  Adjusted R-squared:  0.0003814 </span></span>
<span><span class="co">#&gt; F-statistic: 2.308 on 2 and 6851 DF,  p-value: 0.09958</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We find that imagined stories have significantly less surprise-related language than autobiographical stories (p = 0.043)!</p>
<p>Despite the exciting result, we should be careful with this newfangled approach. As with dictionary-based methods, beware of problems with generalization—there is no guarantee that surprise in Tweets will look similar to surprise in autobiographical accounts. Likewise, keep in mind all of the regular challenges of machine learning. Notice for example that the intercept of this model is extremely low (0.057; surprise is measured between 0 and 1). This reflects the fact that the Crowdflower dataset is not balanced; there are very few Tweets labeled with surprise relative to the size of the dataset.</p>
<p><strong>An example of machine learning approaches in research:</strong> <span class="citation" data-cites="zamani_etal_2018">Zamani et al. (<a href="#ref-zamani_etal_2018" role="doc-biblioref">2018</a>)</span> extracted n-grams from Facebook status updates. They then computed TF-IDF scores, binary tokenization, and LDA topics (<a href="decontextualized-embeddings.html#sec-lda"><span>Section&nbsp;18.2.2</span></a>), subjected all of these values to a dimensionality reduction process to reduce overfitting, and used the resulting features to train a ridge regression model to predict questionnaire-based measures of trustfulness. This regression model could then be used on novel texts to estimate the trustworthiness of their authors.</p>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advantages of Machine Learning Approaches
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Accuracy</strong></li>
<li>
<strong>Regularization:</strong> Machine learning algorithms focus on mitigating the influence of outliers. This can sometimes help generalize across datasets too.</li>
<li>
<strong>Avoid Statistical Troubles:</strong> The output of machine learning models is often continuous and more or less normally distributed. This means standard linear regression is usually sufficient for hypothesis testing.</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Disadvantages of Machine Learning Approaches
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Require a Relevant Training Dataset</strong></li>
<li><strong>Difficult to Interpret</strong></li>
<li><strong>May Fail to Generalize Across Datasets</strong></li>
</ul>
</div>
</div>
<hr>


<div id="refs" class="references csl-bib-body hanging-indent" data-line-spacing="2" role="list">
<div id="ref-baayen_2001" class="csl-entry" role="listitem">
Baayen, R. H. (2001). <em>Word frequency distributions</em>. Springer Netherlands. <a href="https://link.springer.com/book/10.1007/978-94-010-0844-0">https://link.springer.com/book/10.1007/978-94-010-0844-0</a>
</div>
<div id="ref-gale_sampson_1995" class="csl-entry" role="listitem">
Gale, W. A., &amp; Sampson, G. (1995). Good‐turing frequency estimation without tears. <em>Journal of Quantitative Linguistics</em>, <em>2</em>(3), 217–237. <a href="https://doi.org/10.1080/09296179508590051">https://doi.org/10.1080/09296179508590051</a>
</div>
<div id="ref-giuntini_etal_2020" class="csl-entry" role="listitem">
Giuntini, F. T., Cazzolato, M. T., Reis, M. de J. D. dos, Campbell, A. T., Traina, A. J. M., &amp; Ueyama, J. (2020). A review on recognizing depression in social networks: Challenges and opportunities. <em>Journal of Ambient Intelligence and Humanized Computing</em>, <em>11</em>(11), 4713–4729.
</div>
<div id="ref-good_1953" class="csl-entry" role="listitem">
Good, I. J. (1953). The <span>Population</span> <span>Frequencies</span> of <span>Species</span> and the <span>Estimation</span> of <span>Population</span> <span>Parameters</span>. <em>Biometrika</em>, <em>40</em>(3/4), 237–264. <a href="https://doi.org/10.2307/2333344">https://doi.org/10.2307/2333344</a>
</div>
<div id="ref-good_2000" class="csl-entry" role="listitem">
Good, I. J. (2000). Turing’s anticipation of empirical bayes in connection with the cryptanalysis of the naval enigma. <em>Journal of Statistical Computation and Simulation</em>, <em>66</em>(2), 101–111. <a href="https://doi.org/10.1080/00949650008812016">https://doi.org/10.1080/00949650008812016</a>
</div>
<div id="ref-holtgraves_2011" class="csl-entry" role="listitem">
Holtgraves, T. (2011). Text messaging, personality, and the social context. <em>Journal of Research in Personality</em>, <em>45</em>(1), 92–99. https://doi.org/<a href="https://doi.org/10.1016/j.jrp.2010.11.015">https://doi.org/10.1016/j.jrp.2010.11.015</a>
</div>
<div id="ref-kronmal_1993" class="csl-entry" role="listitem">
Kronmal, R. A. (1993). Spurious correlation and the fallacy of the ratio standard revisited. <em>Journal of the Royal Statistical Society Series A</em>, <em>156</em>(3), 379–392. <a href="https://www.jstor.org/stable/2983064">https://www.jstor.org/stable/2983064</a>
</div>
<div id="ref-laplace_1816" class="csl-entry" role="listitem">
Laplace, P. S. de. (1816). <em>Essai philosophique sur les probabilités; par <span>M</span>. Le comte <span>Laplace</span> ..</em> M.me v.e Courcier, impr.-libr. pour les mathématiques et la marine, quai des Augustins, no 57. <a href="http://archive.org/details/bub_gb_vVzdR0tuoWAC">http://archive.org/details/bub_gb_vVzdR0tuoWAC</a>
</div>
<div id="ref-mehl_etal_2006" class="csl-entry" role="listitem">
Mehl, M. R., Gosling, S. D., &amp; Pennebaker, J. W. (2006). Personality in its natural habitat: Manifestations and implicit folk theories of personality in daily life. <em>Journal of Personality and Social Psychology</em>, <em>90 5</em>, 862–877. <a href="https://api.semanticscholar.org/CorpusID:2932332">https://api.semanticscholar.org/CorpusID:2932332</a>
</div>
<div id="ref-schwartz_etal_2013" class="csl-entry" role="listitem">
Schwartz, H. A., Eichstaedt, J. C., Kern, M. L., Dziurzynski, L., Ramones, S. M., Agrawal, M., Shah, A., Kosinski, M., Stillwell, D., Seligman, M. E. P., &amp; Ungar, L. H. (2013). Personality, gender, and age in the language of social media: The open-vocabulary approach. <em>PLOS ONE</em>, <em>8</em>(9), 1–16. <a href="https://doi.org/10.1371/journal.pone.0073791">https://doi.org/10.1371/journal.pone.0073791</a>
</div>
<div id="ref-sparckjones_1972" class="csl-entry" role="listitem">
Sparck Jones, K. (1972). A statistical interpretation of term specificity and its application in retrieval. <em>Journal of Documentation</em>, <em>28</em>(1), 11–21. <a href="https://doi.org/10.1108/eb026526">https://doi.org/10.1108/eb026526</a>
</div>
<div id="ref-sumner_etal_2011" class="csl-entry" role="listitem">
Sumner, C., Byers, A., &amp; Shearing, M. (2011). Determining personality traits &amp; privacy concerns from facebook activity. <em>Black Hat Brief</em>, <em>11</em>.
</div>
<div id="ref-zamani_etal_2018" class="csl-entry" role="listitem">
Zamani, M., Buffone, A., &amp; Schwartz, H. A. (2018). Predicting human trustfulness from <span>F</span>acebook language. In K. Loveys, K. Niederhoffer, E. Prud’hommeaux, R. Resnik, &amp; P. Resnik (Eds.), <em>Proceedings of the fifth workshop on computational linguistics and clinical psychology: From keyboard to clinic</em> (pp. 174–181). Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/W18-0619">https://doi.org/10.18653/v1/W18-0619</a>
</div>
</div>
</section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr>
<ol>
<li id="fn1"><p>For a good overview of different methods, see <span class="citation" data-cites="giuntini_etal_2020">Giuntini et al. (<a href="#ref-giuntini_etal_2020" role="doc-biblioref">2020</a>)</span><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./dla.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Open Vocabulary Word Counting</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./vectorspace-intro.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Introduction to Vector Space</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">Data Science for Psychology was written by <a href="https://rimonim.github.io">Louis Teitelbaum</a> and <a href="https://almogsi.com">Almog Simchon</a>.</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">This book was built with <a href="https://quarto.org/">Quarto</a> and is powered by <a href="https://www.netlify.com/">Netlify</a></div>
  </div>
</footer>


</body></html>