[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Psychology",
    "section": "",
    "text": "Welcome\nThis is the website for Data Science for Psychology.\nThis book will teach you state-of-the-art methods for analyzing psychological properties of text, and will cover the fundamentals of data visualization, data collection, and scientific methodology necessary to produce meaningful work in the field.\nAt every step of the way, we will give examples in R, using the tools and rules of the tidyverse. This book will also teach you the basics of the quanteda and text packages for natural language processing (NLP), and the vosonSML package for collecting data from popular social media sites.\nUnless otherwise noted, all figures in this book were generated by us using ggplot2. The full, reproducible code for their generation can be viewed by clicking the “View Source” button at the bottom of each page.\n\nOn the Cover: Posts on Reddit’s r/relationship_advice, distributed by their emotional content according to the Pleasure-Arousal-Dominance model. The x axis represents pleasure, the y axis represents dominance, and the color scale represents arousal. The size of the points represents the number of comments responding to each post. For more detail, see the source code."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction: Why Does Psychology Need Data Science?",
    "section": "",
    "text": "“Language is the most common and reliable way for people to translate their internal thoughts and emotions into a form that others can understand. Words and language, then, are the very stuff of psychology and communication.” (Tausczik & Pennebaker, 2010)\n\nenhance experimental control by matching stimuli according to semantic similarity (e.g. Gagné et al., 2005)\nexamine text generated from experimental manipulations (e.g. Garcia & Sikström, 2013)\nfind correlations with neural organizations in the brain (e.g. Millet et al., 2022)\n\n\n\n\n\nGagné, C., Spalding, T., & Ji, H. (2005). Re-examining evidence for the use of independent relational representations during conceptual combination. Journal of Memory and Language, 53, 445–455. https://doi.org/10.1016/j.jml.2005.03.006\n\n\nGarcia, D., & Sikström, S. (2013). Quantifying the Semantic Representations of Adolescents’ Memories of Positive and Negative Life Events. Journal of Happiness Studies, 14(4), 1309–1323. https://doi.org/10.1007/s10902-012-9385-8\n\n\nMillet, J., Caucheteux, C., Orhan, P., Boubenec, Y., Gramfort, A., Dunbar, E., Pallier, C., & King, J.-R. (2022). Toward a realistic model of speech processing in the brain with self-supervised learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, & A. Oh (Eds.), Advances in neural information processing systems (Vol. 35, pp. 33428–33443). Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2022/file/d81ecfc8fb18e833a3fa0a35d92532b8-Paper-Conference.pdf\n\n\nTausczik, Y. R., & Pennebaker, J. W. (2010). The psychological meaning of words: LIWC and computerized text analysis methods. Journal of Language and Social Psychology, 29(1), 24–54. https://doi.org/10.1177/0261927X09351676"
  },
  {
    "objectID": "ethics.html#anonymization-is-hard",
    "href": "ethics.html#anonymization-is-hard",
    "title": "2  The Ethics of Data Science in Psychology",
    "section": "\n2.1 Anonymization is Hard",
    "text": "2.1 Anonymization is Hard\nSharing data is an important way for researchers to stay accountable to their colleagues and to promote further research. Nevertheless, data sharing can become problematic when individual subjects can be identified. This is especially true in psychology, which often deals with sensitive personal information. As such, it is important to anonymize data before sharing it. You might think that removing personal names would be enough to accomplish this. It is not.\nIn August 2006, the online service provider AOL released the search queries of 657,000 users over a 3-month period. The dataset was anonymized by replacing personal names with a numeric user ID. Within days, New York Times reporters were able to identify user No. 4417749 as a 62-year-old widow from Lilburn, Georgia by putting together searches involving place names, family names, and ages. AOL quickly took the dataset down, but it was too late. The data are still widely available on the internet, and many more users have been identified based on their search histories.\nAs technology improves, data that previously seemed innocuous can be leveraged to reveal personal information. For example, Facebook users’ “likes” were once public information. Kosinski et al. (2013) then showed that likes alone could be used to predict a user’s age, gender, sexual orientation, ethnicity, religion, intelligence, drug use, and more. Facebook now makes likes private by default.\nKosinski et al. (2013) did their work without the aid of deep neural networks. With more advanced language processing algorithms emerging every day, text data in particular are becoming increasingly difficult to anonymize. The text that people write (and read) is a window into their soul. This is why NLP is so useful for psychology, but it is also a reason to be vigilant."
  },
  {
    "objectID": "ethics.html#text-based-psychology-is-powerful",
    "href": "ethics.html#text-based-psychology-is-powerful",
    "title": "2  The Ethics of Data Science in Psychology",
    "section": "\n2.2 Text-Based Psychology is Powerful",
    "text": "2.2 Text-Based Psychology is Powerful\nCambridge Analytica is the prime example of the power of data science in psychology. In the 2010s, Cambridge Analytica used an app to collect demographic and psychological data from tens of millions of Facebook users, and paired this with users’ behavior on Facebook. They then used the resulting psychological measures (based on the well-known Big Five personality traits) to create tailored advertisements for political campaigns. The revelation of this privacy breach created an international scandal for both Facebook and Cambridge Analytica.\nCambridge Analytica used methods not unlike many of those described in this book—methods for extracting psychological characteristics from naturalistic online behavior. In fact, due to developments in the field over the last decade, many of the methods described in this book can be quite a bit more powerful than those employed by Cambridge Analytica. Be careful—the research you conduct can be used for the kind of things that create international scandals."
  },
  {
    "objectID": "ethics.html#what-to-do-about-it",
    "href": "ethics.html#what-to-do-about-it",
    "title": "2  The Ethics of Data Science in Psychology",
    "section": "\n2.3 What to do About it",
    "text": "2.3 What to do About it\nThere are no universally accepted rules for ethical text data usage. Many countries have developed data protection laws, for example those of the European Data Protection Supervisor (EDPS) or Israel’s Privacy Protection Authority. Nevertheless, as with any ethical problem, the best policy is to think for yourself, weighing risks against benefits.\nIf you want to share your data widely, but are worried about sensitive private information contained in it, consider using one of many advanced anonymization techniques, such as those that leverage generative AI models to create synthetic data while maintaining statistical properties of the original. These techniques are sometimes costly or labor-intensive, but can be worthwhile for high-impact studies.\nThis chapter is far from a thorough treatment of ethical problems and possible solutions for data collection on the internet. For further reading, we suggest the Association of Internet Researchers Ethical Guidelines.\n\n\n\n\n\nKosinski, M., Stillwell, D., & Graepel, T. (2013). Private traits and attributes are predictable from digital records of human behavior. Proceedings of the National Academy of Sciences, 110(15), 5802–5805. https://doi.org/10.1073/pnas.1218772110"
  },
  {
    "objectID": "data-viz.html",
    "href": "data-viz.html",
    "title": "Data Visualization",
    "section": "",
    "text": "There are no objective rules for how to make a good data visualization. This is because not all data visualizations have the same purpose.\nHere are three common purposes for data visualization:\n\nGetting a quick, intuitive understanding of the data you are working with\nLooking for aspects of your data that common statistics might miss (like outliers or nonlinear relationships)\nCommunicating something to people who don’t have a deep understanding of your data\n\nThe first two purposes are an important aspects of exploratory data analysis (EDA). They will be discussed in Chapter 13. The third one - communication - is the topic of this unit.\nCommunication is hard. It is especially hard for scientists, who often need to balance the needs of multiple target audiences. While insiders in a particular field may want detailed, objective analysis of results and the uncertainty surrounding them, everyone else wants a story.\nA story is clear, oversimplified and sensational. Stories are what grab people’s attention and hold it. Even for experts, stories are what make one study (or news article, or social media post) stand out among many. If you tell a good story, people will want to learn more about the details. If you don’t tell a good story, nobody will want to read your supplementary materials. In this way, all scientists are journalists."
  },
  {
    "objectID": "aesthetics.html",
    "href": "aesthetics.html",
    "title": "3  Why Aesthetic Choices are Important",
    "section": "",
    "text": "For most people, aesthetics is the art of making things pleasant to look at. To the data visualizer though, “aesthetics” means something much more precise: Aesthetics are the visual representation of variables.\nJust as journalists need to decide which words to use to express their ideas, data visualizers need to decide which aesthetics to use to express their variables.\nThere are many options. Below are six different ways to represent the numbers “1”, “2”, and “3”, each mapping them to a different aesthetic.\n\n\n\n\n\nLet’s take a concrete example. Eichstaedt et al. (2015) collected Twitters posts from 935 U.S. counties, and counted the number of words related to positive emotions. This emotional measure could then be connected with known demographic measures of each county, such as race and average income.\n\nhead(twitter_counties)\n\n#&gt; # A tibble: 6 × 5\n#&gt;   county   state income posEmotions maj                        \n#&gt;   &lt;chr&gt;    &lt;chr&gt;  &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;                      \n#&gt; 1 Autauga  AL     55165     0.00549 Majority Non-Black/Hispanic\n#&gt; 2 Baldwin  AL     50006     0.00670 Majority Non-Black/Hispanic\n#&gt; 3 Blount   AL     43450     0.00336 Majority Non-Black/Hispanic\n#&gt; 4 Butler   AL     29769     0.00407 Majority Non-Black/Hispanic\n#&gt; 5 Calhoun  AL     38473     0.00448 Majority Non-Black/Hispanic\n#&gt; 6 Chambers AL     30546     0.00523 Majority Non-Black/Hispanic\n\n\nThere are many ways to present this information graphically. Each choice emphasizes a different aspect of the data, and tells a different story.\nIn the following visualization,\n\n\nincome is mapped to the “x” position aesthetic\n\nposEmotions (positive emotions) is mapped to the “y” position aesthetic\n\nmaj (racial majority) is mapped to the color aesthetic\n\n\nlibrary(ggborderline) # for making the lines pop\n\ntwitter_counties |&gt; \n  ggplot(\n    aes(income, posEmotions, \n        # rearrange the categorical variable so that the order \n        # in the legend matches the order in the plot\n        color = factor(\n          maj, \n          levels = c(\"Majority Non-Black/Hispanic\", \n                     \"Majority Hispanic\", \n                     \"Majority Black\")\n          )\n        )\n    ) +\n    # scatterplot\n    geom_point(alpha = .5, \n               # draw sample such that \"Majority Non-Black/Hispanic\" \n               # points don't overpower the others\n               data = twitter_counties |&gt; \n                 group_by(maj) |&gt; \n                 slice_sample(n = 100)) +\n    # loess regression\n    stat_smooth(\n      # borders that match the lines, but are slightly darker\n      aes(bordercolor = after_scale(colorspace::darken(color))), \n      se = FALSE, geom = \"borderline\", \n      linewidth = 1, lineend = \"square\"\n      ) +\n    theme_bw() +\n    # nicer color palette\n    scale_color_brewer(\n      palette = \"Paired\", \n      direction = -1\n      ) +\n    # proper formatting for income\n    scale_x_continuous(labels=scales::dollar_format()) +\n    labs(\n      x = \"County Average Income\",\n      y = \"Positive Emotional Words in Twitter Posts\\n(proportion of total words)\",\n      color = \"\"\n      )\n\n\n\n\nThis is the most intuitive way to organize the three variables. By mapping income to the x axis, we lightly suggest that it is the cause of whatever is happening on the y axis—in this case positive emotion. The idea that higher income causes positive emotion is intuitive—any people believe they would be happier with a higher income. People accustomed to languages that are written left-to-right, like English, will tend to think about what happens as they move left to right on the graph. Three LOESS regression lines encourage the viewer to compare the slopes of the three color groups, which they will go through from top to bottom:\n\nIn counties without a Black or Hispanic majority, greater income means more positive emotion, up to about $60,000 a year, when the line starts flattening out.\nIn counties with a majority Hispanic population, greater income means dramatically more positive emotion, on average.\nIn counties with a majority Black population, greater income doesn’t seem to make much of a difference.\n\nBut just because this scheme is the most intuitive does not mean it is the best one.\nThe next visualization shows the same data but tells a different story. This one also has x, y, and color, but they are mapped to the variables differently:\n\n\nincome is binned and mapped to the “x” position aesthetic\n\nmaj (racial majority) is mapped to the “y” position aesthetic\n\nposEmotions (positive emotions) is mapped to the “fill” color aesthetic\n\n\ntwitter_counties |&gt; \n  # hand-made bins (note the inconsistent bin width to \n  # give more space to the center of the distribution) \n  mutate(\n    income = factor( \n      case_when(\n        income &gt; 100000 ~ \"$100,000+\",\n        income &gt; 80000 ~ \"$80,000-$100,000\",\n        income &gt; 60000 ~ \"$60,000-$80,000\",\n        income &gt; 50000 ~ \"$50,000-$60,00\",\n        income &gt; 40000 ~ \"$40,000-$50,00\",\n        .default = \"$20,000-$40,00\"),\n      levels = c(\"$20,000-$40,00\", \n                 \"$40,000-$50,00\",\n                 \"$50,000-$60,00\", \n                 \"$60,000-$80,000\",\n                 \"$80,000-$100,000\",\n                 \"$100,000+\")\n      )\n    ) |&gt; \n  # aggregate by the new bins\n  group_by(income, maj) |&gt; \n  summarise(\n    posEmotions = mean(posEmotions, \n                       na.rm = TRUE)\n    ) |&gt; \n  # plot\n  ggplot(aes(income, maj, fill = posEmotions)) +\n    # tiles with a little bit of space in between\n    geom_tile(width = .95, height = .95) +\n    # minimal theme\n    theme_minimal() +\n    # color scale to emphasize differences between extremes\n    scale_fill_gradient2(low = \"blue\", \n                         mid = \"green\", \n                         high = \"yellow\", \n                         midpoint = .0055) +\n    labs(\n      x = \"County Average Income\",\n      y = \"\",\n      fill = \"Positive Emotional Words\\nin Twitter Posts\\n(proportion of total words)\"\n      ) +\n    theme(\n      # angles x axis text to fit it all in\n      axis.text.x = element_text(angle = 30, hjust = 1), \n      axis.title = element_text(size = 8),\n      legend.title = element_text(size = 8)\n      ) +\n    # constrain the tiles to be perfectly square\n    coord_equal()\n\n\n\n\nUsing fill makes it harder to see the slope within each racial group, and easier to see the differences between them. The vertical ordering from most positive emotion (in majority non-Black/Hispanic counties) to least positive emotion (majority Black counties) emphasizes this even more. The blank squares on the grid also make the point that there are no majority Black or Hispanic counties with average incomes above $80,000.\nLet’s try one more way to present these data. In this visualization,\n\n\nincome is mapped to the “y” position aesthetic\n\nposEmotions (positive emotions) is mapped to the “color” aesthetic\n\nmaj (racial majority) is mapped to the “x” position aesthetic\n\n\nset.seed(2023)\ntwitter_counties |&gt; \n  ggplot(aes(maj, income, fill = posEmotions)) +\n    # sina plot\n    ggbeeswarm::geom_quasirandom(\n      aes(color = after_scale(colorspace::darken(fill, .3))), \n      alpha = .5, method = \"pseudorandom\", \n      shape = 21, varwidth = TRUE\n      ) +\n    # color scheme which maximizes the visibility of different \n    # values among the crowd (this is a losing battle)\n    scale_fill_gradient2(low = \"red\", \n                         mid = \"white\", \n                         high = \"blue\", \n                         midpoint = .005) +\n    # unintrusive theme\n    theme_bw() +\n    # log scale, and proper formatting for income\n    scale_y_continuous(\n      labels=scales::dollar_format(), \n      trans = \"log10\"\n      ) +\n    labs(\n      x = \" \",\n      y = \"County Average Income\",\n      fill = \"Positive Emotional Words\\nin Twitter Posts\\n(proportion of total words)\"\n      ) +\n    theme(axis.text.x = element_text(angle = 30, hjust = 1))\n\n\n\n\nThis is a sina plot (also known as a beeswarm plot), in which point clouds are arranged by a continuous variable on one axis, a categorical variable on the other axis, and spread out in proportion to their density along the spaces in between the categories.\nSina plots are a good way to compare distributions of different groups (they are almost always more informative than box plots or violin plots); this plot emphasizes that counties with a majority Black population tend to have relatively low average incomes. The other story that this plot tells is the uneven sizes of the three groups—by separating out the points in each group, this visualization emphasizes the fact that there are very few US counties with majority Black or Hispanic population.\nThis plot makes it very difficult to learn anything about positive emotion in Twitter posts. The colors themselves give some guidance: the white in the middle of the scale suggests that it represents some sort of zero point—a “normal” amount of positive emotion. Nevertheless, the viewer will have to squint in order to notice the trend for higher income counties to be happier. The emotional variable is not adding much to this visualization.\nIn this chapter we have seen how choices about which aesthetics to map to variables make a big difference in the way a data visualization is interpreted. Three visualization of the same data can emphasize tell very different stories.\n\nPress the “View Source” button below to see the hidden code blocks in this chapter.\n\n\n\n\n\nEichstaedt, J. C., Schwartz, H. andrew, Kern, M. L., Park, G., Labarthe, D. R., Merchant, R. M., Jha, S., Agrawal, M., Dziurzynski, L. A., Sap, M., Weeg, C., Larson, E. E., Ungar, L. H., & Seligman, M. E. P. (2015). Psychological language on twitter predicts county-level heart disease mortality. Psychological Science, 26(2), 159–169. https://doi.org/10.1177/0956797614557867"
  },
  {
    "objectID": "telling-a-story.html#sec-simplify-the-story",
    "href": "telling-a-story.html#sec-simplify-the-story",
    "title": "4  Don’t Distract From the Story",
    "section": "\n4.1 A Small, Compelling Story is Better Than a Big, Confusing One",
    "text": "4.1 A Small, Compelling Story is Better Than a Big, Confusing One\nSchwartz et al. (2013) collected 15.4 million Facebook status updates from participants who had filled out a variety of questionnaires on the My Personality application (discussed in Chapter 2). They analyzed the frequencies of millions of words, phrases, and topics as they correlate with gender, age, and personality traits of the author. The resulting paper focused on methodology, but Schwartz et al. (2013) nevertheless understood the importance of telling a good story. Here is their figure 5B:\n\n\n\n\nThis is a beautiful data visualization. The story it tells is so clear and simple that it doesn’t need a caption: Older people use “I” less and “we” more. The unstated implication is either that people get less individualistic with age, or that the young people of today are self-centered. Both are excellent stories.\nHow did Schwartz et al. achieve such a clear story? Let’s take a closer look at some of their choices:\nFirst, out of millions of words, phrases, and topics in their analysis, they chose to focus this visualization on only two. This is the first step of story-telling with data: remove distractions. A small, compelling story is better than a big, confusing one.\nSecond, they chose not to show the data points themselves, but to represent the overall trends with regression lines. This is a major sacrifice, since it makes the graph much less informative—any good scientist will wonder about the distributions surrounding these lines: How rare are community-oriented 20-year-olds? What about self-centered 60-year-olds? Nevertheless, Schwartz et al. decided that including a scatter plot behind the lines would make the graph too confusing to look at, and distract from the main story.\nThird, they chose to use bendy LOESS regression lines, even though the main analysis of the paper was conducted with linear regression. This was a great choice because it makes the story more convincing. The fact that even LOESS lines show near-linear trends is impressive. Even though there are no data points to be seen, those steady lines give the impression that the underlying data are reliable. Also, the the LOESS lines give the viewer the opportunity to notice nuances in the story without distracting from the big picture (it is fascinating that “we” reaches it’s all-time low around the time most people move out of their parents’ house, and not before).\nLastly, let’s take a look at the y axis: What is “Standardized Frequency”? We have an intuitive idea that higher means using the word more and lower means using it less. But this intuitive simplicity did not come easily—it had to be carefully constructed by the authors of the paper. Actually, “Standardized Frequency” is calculated using this formula:\n\n\n\n\nDon’t understand any of this? That’s OK. We’ll cover methods of standardizing word frequencies in Chapter 15. For now, the point is this: Sometimes you have to do something complicated to make something simple. If Schwartz et al. had not performed them, “I” would likely be much higher frequency than “we” at all ages, and the story, which requires the viewer to focus on the slopes of the lines, would be much harder to appreciate."
  },
  {
    "objectID": "telling-a-story.html#engineer-your-aesthetics",
    "href": "telling-a-story.html#engineer-your-aesthetics",
    "title": "4  Don’t Distract From the Story",
    "section": "\n4.2 Engineer Your Aesthetics",
    "text": "4.2 Engineer Your Aesthetics\nWe have just seen in Schwartz et al.’s beautiful data visualization (Section 4.1) that choosing to map the “frequency” variable to the y position aesthetic was not enough. In order to make the story clear, they carefully engineered the scale on which they measured frequency. In their case, this required some complicated standardization tailored to the particular statistics underlying their data. Often though, the solution is much more straightforward.\nThe remainder of this chapter outlines some common ways to engineer aesthetics that can help make a story clear and intuitive.\n\n4.2.1 Nonlinear Axes\nOften a simple log scale is enough to reveal a much clearer presentation of data. The following graph uses data from Buechel et al. (2018), in which participants read news stories, rated their own empathy and distress after reading them, and then described their thoughts in their own words.\nThis visualization tells a story about the most and least common words in participant’s responses.\n\nlibrary(ggrepel)\nlibrary(patchwork)\n\ndistressed_texts_binary_ordered &lt;- distressed_texts_binary |&gt; \n  # ratio of distressed frequency to non-distressed frequency\n  mutate(distressed_freq_ratio = distressed_count/nondistressed_count) |&gt; \n  # refactor in descending order\n  arrange(distressed_freq_ratio) |&gt; \n  mutate(word = factor(word, levels = word))\n\nset.seed(2023)\nbadplot1 &lt;- distressed_texts_binary_ordered |&gt; \n  ggplot(aes(word, distressed_freq_ratio, label = word)) +\n    geom_point(color = \"blue3\", \n               size = 1, \n               alpha = .2) +\n    geom_text_repel(\n      size = 3, \n      data = distressed_texts_binary_ordered |&gt; \n        filter(distressed_freq_ratio &gt; 1.75)) +\n    labs(title = \"Linear Scale\",\n         x = \"Words, from least to most distressed\",\n         y = \"Distressed frequency / non-distressed frequency\") +\n    scale_y_continuous(breaks = seq(-.5, 2, .5)) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(color = \"red3\", hjust = 1, size = 20),\n      panel.grid.major.x = element_blank(),\n      axis.text.x = element_blank(),\n      axis.ticks.x = element_blank(),\n      axis.title.x = element_text(size = 15)\n      )\n\nset.seed(2023)\ngoodplot1 &lt;- distressed_texts_binary_ordered |&gt; \n  ggplot(aes(word, distressed_freq_ratio, \n             label = word, \n             color = distressed_freq_ratio &lt; 1)\n         ) +\n    geom_point(size = 1, alpha = .2) +\n    geom_text_repel(\n      size = 3, \n      data = distressed_texts_binary_ordered |&gt; \n        filter(distressed_freq_ratio &gt; 1.75 | distressed_freq_ratio &lt; 1/2),\n      max.overlaps = 20) +\n    geom_hline(linetype = 2, yintercept = 1) +\n    labs(title = \"Log Scale\",\n         x = \"Words, from least to most distressed\",\n         y = \"Distressed frequency / non-distressed frequency\") +\n    scale_y_continuous(\n      breaks = c(2^(-6:6)), \n      trans = \"log2\", \n      labels = ~MASS::fractions(.x)\n      ) +\n    guides(color = \"none\") +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(color = \"green3\", hjust = 1, size = 20),\n      panel.grid.major.x = element_blank(),\n      axis.text.x = element_blank(),\n      axis.ticks.x = element_blank(),\n      axis.title.x = element_text(size = 15)\n      )\n\ngoodplot1 + badplot1\n\n\n\n\nWhen plotting ratios, it is almost always a good idea to use a log scale (left). This way, the viewer can compare the largest and the smallest relative values. Without the log scale (right), the smallest values are squished into oblivion.\n\n4.2.2 Ordering Categorical Variables\nTake another look at the graph labeled “Log Scale” above, and notice the ordering along the x axis. Words, on their own, are an unordered categorical variable. Nevertheless, in the context of a story, even unordered variables have an order. Ordering the categorical variable along the continuous variable of interest calls attention to the distribution and removes confusion.\n\nlibrary(ggrepel)\nlibrary(patchwork)\n\nset.seed(2023)\nbadplot2 &lt;- distressed_texts_binary |&gt; \n  # ratio of distressed frequency to non-distressed frequency\n  mutate(distressed_freq_ratio = distressed_count/nondistressed_count) |&gt; \n  ggplot(\n    aes(\n      word, distressed_freq_ratio, \n      label = word, \n      color = distressed_freq_ratio &lt; 1\n      )\n    ) +\n    geom_point(size = 1, alpha = .7) +\n    geom_text_repel(\n      size = 3, \n      data = distressed_texts_binary_ordered |&gt; \n        filter(distressed_freq_ratio &gt; 1.75 | distressed_freq_ratio &lt; .5),\n      max.overlaps = 20\n      ) +\n    geom_hline(linetype = 2, yintercept = 1) +\n    labs(\n      title = \"Unordered\",\n      x = \"Words\",\n      y = \"Distressed frequency / non-distressed frequency\"\n      ) +\n    scale_y_continuous(\n      breaks = c(2^(-6:6)), \n      trans = \"log2\", \n      labels = ~MASS::fractions(.x)\n      ) +\n    guides(color = \"none\") +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(color = \"red3\", hjust = 1, size = 20),\n      panel.grid.major.x = element_blank(),\n      axis.text.x = element_blank(),\n      axis.ticks.x = element_blank(),\n      axis.title.x = element_text(size = 15)\n      )\n\ngoodplot2 &lt;- goodplot1 + labs(title = \"Ordered\")\n\ngoodplot2 + badplot2\n\n\n\n\n\n4.2.3 Color Scales\nWe have already seen how a data visualizer can clarify a story by spatially ordering a categorical variable. A carefully tailored color scale can be an even more powerful communicator than an x or y axis. This is because color, even though it is usually treated as a single aesthetic, actually has many dimensions: luminosity, saturation, redness, blueness, etc.\nThe first step in choosing a color scale for any variable is to consider whether the thing being measured is diverging, sequential, or qualitative.\nDiverging scales measure something with a neutral center. This center is often represented by zero, but beware! Sometimes a neutral center is 4 on a seven point Likert scale (see “Agreement” in the figure below). When dealing with fractions, the neutral center is 1 (see “Frequency Ratio” in the figure below).\nWhen applying diverging scales, keep in mind any associations people might have with the colors involved. For example, red should always be bad and blue/green good (see the plot in Section 4.2.2, in which red = distress = bad).\n\n\n\n\n\nSequential scales measure something that has an order, but no neutral center. Often, one side of the scale is at zero, so that the scale goes from nothing to something. In these cases, the appropriate color scale will represent amount with luminosity, where zero is the lightest (see “Frequency” and “Anxiety” in the figure below). This way, the lower amounts have lower contrast against the white background of the plot (if using a non-white plot background, make sure the low end of the scale matches).\nSometimes sequential scales do not measure amount, as in “Weekdays” in the figure below. Weekdays have an order—rom the beginning of the week to the end—ut it would be a mistake to use a scale with one side blending in to the background and the other intensely dark, since that would suggest that Thursday is somehow ‘more’ than Wednesday. Likewise, there is no neutral center (there’s nothing neutral about Wednesday). In such case, the scale should go from one noticeable color to another. The chart below uses a palette reminiscent of sunset to give the impression of time passing.\n\n\n\n\n\nTo emphasize the point about weekdays, consider the following two versions of the same graph (data taken from 1000 top Reddit communities):\n\n\n\n\n\nBoth versions are confusing to look at (these data might be better represented as a heat map), but the one with the sequential color scale is much better. Whereas the qualitative scale requires the viewer to look constantly back and forth between the legend and the plot, the sequential scale maps to an intuitive understanding of beginning-of-week vs. end-of-week.\nWe have seen that many seemingly unordered variables should be ordered in the context of a story. Nevertheless, some variables are truly qualitative. In these cases, the color scale should maximize contrast between neighboring values without accidentally suggesting an order. For example, “Parts of Speech” in the figure below are all soft pastel colors. If some were darker or more saturated, it might suggest that there is an important difference between the groups.\nAgain, keep in mind any associations people might have with colors involved. For example, countries should be represented by colors that appear in their flags. This is of course sometimes difficult—all of the countries in the figure below have red in their flag, and all but China have blue and white. Nevertheless, try your best. The Wikipedia page for national colors was helpful in making the chart below.\n\n\n\n\n\n\n4.2.4 Accent colors\nBecause color has many dimensions, it can sometimes be used to represent two scales at the same time. One common tactic is to use luminosity or saturation to emphasize certain values and de-emphasize others. Below, we have redrawn the frequency ratio plot from earlier in this chapter (Section 4.2.1) to tell a story about two words in particular. By using accent colors to emphasize the two words of interest, we remove distractors while maintaining the broader context of the story.1\n\nlibrary(ggnewscale)\n\nset.seed(2023)\ndistressed_texts_binary_ordered |&gt; \n  ggplot(aes(word, distressed_freq_ratio, \n             label = word, \n             color = distressed_freq_ratio &lt; 1)) +\n    geom_point() +\n    geom_hline(linetype = 2, yintercept = 1) +\n    scale_color_discrete(\n      type = colorspace::lighten(c(\"#F8766D\", \"#00BFC4\"), .7)) +\n    geom_text_repel(\n      size = 3, \n      data = distressed_texts_binary_ordered |&gt; \n        filter(distressed_freq_ratio &gt; 3.6 \n               | distressed_freq_ratio &lt; .2),\n      max.overlaps = 20) +\n    guides(color = \"none\") +\n    new_scale_color() +\n    geom_point(\n      aes(color = distressed_freq_ratio &lt; 1), \n      size = 3,\n      data = distressed_texts_binary_ordered |&gt; \n        filter(word %in% c(\"i\", \"we\"))) +\n    geom_text_repel(\n      size = 4, \n      data = distressed_texts_binary_ordered |&gt; \n        filter(word %in% c(\"i\", \"we\")),\n      max.overlaps = 20) +\n    labs(title = '\"We\" is a Sign of Distress',\n         x = \"Words, from least to most distressed\",\n         y = \"Distressed frequency / non-distressed frequency\") +\n    scale_y_continuous(breaks = c(2^(-6:6)), \n                       trans = \"log2\", \n                       labels = ~MASS::fractions(.x)) +\n    guides(color = \"none\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = .5, size = 18),\n          panel.grid.major.x = element_blank(),\n          axis.text.x = element_blank(),\n          axis.ticks.x = element_blank())\n\n\n\n\n\n4.2.5 Aspect Ratios\nSimchon et al. (2021) investigated whether COVID-19 concern among New Yorkers resulted in higher or lower levels of certainty, as expressed in language on Twitter. Their story: Higher concern leads to greater expressions of certainty, since people use certainty as a coping mechanism. Here is their Figure 3, reproduced in three different aspect ratios:\n\nlibrary(patchwork)\n\nplot &lt;- covid_concern |&gt; \n  mutate(z_cert = as.numeric(scale(cert)),\n         z_concern = as.numeric(scale(ny_net_concern))) |&gt; \n  pivot_longer(cols = c(z_cert, z_concern)) |&gt; \n  mutate(name = if_else(name==\"z_cert\", \"Certainty\", \"NY Net Concern\")) |&gt; \n  ggplot() +\n    geom_smooth(aes(date, value, \n                    linetype = name), \n                se = FALSE, method = \"loess\", \n                color = \"black\", span = 1/3, \n                method.args = list(degree=1)) +\n    ylab(\"Z-score\") + \n    xlab(\"Date\") + \n    scale_colour_grey() +\n    cowplot::theme_cowplot() + \n    labs(linetype =c(\"\"))\n\nplot_squished &lt;- plot + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n(plot_squished + plot_squished +  \n    plot_layout(widths = c(1, 2))) / \n  plot + plot_layout(heights = c(2, 1)) +\n  plot_annotation(tag_levels = 'A')\n\n\n\n\nWhich aspect ratio is the right one? A good aspect ratio is one that communicates the meaning of the variables in question. Since months are spread out over time (by definition), it makes sense to make the x-axis longer so that viewers have the feeling of time passing as they scan it. But it shouldn’t be too wide, since the aspect ratio should also emphasize important differences in position (here, the positive slope of both lines). Something in between B and C seems appropriate. Indeed, this is the figure printed in the final paper:\n\n\nAs always, press the “View Source” button below to see the hidden code blocks in this chapter.\n\n\n\n\n\nBuechel, S., Buffone, A., Slaff, B., Ungar, L. H., & Sedoc, J. (2018). Modeling empathy and distress in reaction to news stories. CoRR, abs/1808.10399. http://arxiv.org/abs/1808.10399\n\n\nSchwartz, H. andrew, Eichstaedt, J. C., Kern, M. L., Dziurzynski, L., Ramones, S. M., Agrawal, M., Shah, A., Kosinski, M., Stillwell, D., Seligman, M. E. P., & Ungar, L. H. (2013). Personality, gender, and age in the language of social media: The open-vocabulary approach. PLOS ONE, 8(9), 1–16. https://doi.org/10.1371/journal.pone.0073791\n\n\nSimchon, A., Turkin, C., Svoray, T., Kloog, I., Dorman, M., & Gilead, M. (2021). Beyond doubt in a dangerous world: The effect of existential threats on the certitude of societal discourse. Journal of Experimental Social Psychology, 97, 104221. https://doi.org/https://doi.org/10.1016/j.jesp.2021.104221"
  },
  {
    "objectID": "telling-a-story.html#footnotes",
    "href": "telling-a-story.html#footnotes",
    "title": "4  Don’t Distract From the Story",
    "section": "",
    "text": "In the example here, we used ggnewscale to control the accented an non-accented color scales separately. If you’d like a simpler method for accenting values without using layered geoms, we recommend the gghighlight package.↩︎"
  },
  {
    "objectID": "word-viz.html#frequencyfrequency-plots",
    "href": "word-viz.html#frequencyfrequency-plots",
    "title": "5  Visualizing Distributions of Words",
    "section": "\n5.1 Frequency/Frequency Plots",
    "text": "5.1 Frequency/Frequency Plots\nA scatterplot is the most obvious choice for visualizing the relationship between two variables. For text data, this approach is commonly associated with the scattertext Python library (Kessler, 2017), but the same effect is easily accomplished in ggplot2.\nSince we are comparing frequency in one group to frequency in another, we can put each frequency variable on an axis. We will call this a frequency/frequency plot, or F/F plot. To emphasize words that are more frequent in one group than the other, we represent the ratio between the two frequencies with a diverging color scale.\n\nlibrary(ggrepel)\n\ndistressed_texts_binary |&gt; \n  ggplot(aes(nondistressed_count, distressed_count, \n             label = word,\n             color = distressed_freq_ratio)) +\n    geom_point() +\n    geom_text_repel(max.overlaps = 20) +\n    scale_x_continuous(trans = \"log10\", n.breaks = 5) +\n    scale_y_continuous(trans = \"log10\", n.breaks = 6) +\n    scale_color_gradient2(\n      low = \"blue4\", \n      mid = \"#E2E2E2\", \n      high = \"red4\", \n      trans = \"log2\", # log scale for ratios\n      limits = c(.25, 4), \n      breaks = c(.25, 1, 4),\n      labels = c(\"Characteristically\\nNon-Distressed\",\n                 \"Equal Proportion\",\n                 \"Characteristically\\nDistressed\")\n      ) +\n    labs(\n      title = \"Stop Words in Distressed and Non-Distressed Texts\",\n      x = \"Occurrences in Non-Distressed Texts\",\n      y = \"Occurrences in Distressed Texts\",\n      color = \"\"\n      ) +\n    coord_fixed() +\n    theme_minimal()\n\n\n\n\nThis plot has the advantage of showing not just which words are characteristic of one group or the other, but also which are more common in both.\nTo allow viewers to explore these patterns in greater detail, we can make the plot interactive using the ggiraph package. Hover over the points to show the words they represent!\n\nlibrary(ggiraph, verbose = FALSE)\nlibrary(ggrepel)\n\np &lt;- distressed_texts_binary |&gt; \n  ggplot(aes(nondistressed_count, distressed_count, \n             label = word,\n             color = distressed_freq_ratio,\n             tooltip = word, \n             data_id = word # aesthetics for interactivity\n             )) +\n    geom_point_interactive() +\n    geom_text_repel_interactive() +\n    scale_x_continuous(trans = \"log10\", n.breaks = 5) +\n    scale_y_continuous(trans = \"log10\", n.breaks = 6) +\n    scale_color_gradient2(\n      low = \"blue4\", \n      mid = \"#E2E2E2\", \n      high = \"red4\", \n      trans = \"log2\", # log scale for ratios\n      limits = c(.25, 4), \n      breaks = c(.25, 1, 4),\n      labels = c(\"Characteristically\\nNon-Distressed\",\n                 \"Equal Proportion\",\n                 \"Characteristically\\nDistressed\")\n    ) +\n    labs(\n      title = \"Stop Words in Distressed and Non-Distressed Texts\",\n      x = \"Occurrences in Non-Distressed Texts\",\n      y = \"Occurrences in Distressed Texts\",\n      color = \"\"\n    ) +\n    # fixed coordinates since x and y use the same units\n    coord_fixed() + \n    theme_minimal()\n\ngirafe_options(\n  girafe(ggobj = p),\n  opts_tooltip(css = \"font-family:sans-serif;font-size:1em;color:Black;\")\n  ) \n\n\n\n\n\n\n5.1.1 Rotated Frequency/Frequency Plots\nA disadvantage of simple F/F plots: When people see a scatterplot, they think, “Aha! A correlation!” Any two samples of text in the same language will have highly correlated word frequencies. This boring story about the correlation is distracting from the more interesting stories about words that are especially characteristic of one group or another. This distraction can be removed by “rotating” the axes. Mathematically, we achieve this by plotting the average of the two frequencies (nondistressed_count + distressed_count)/2 on the y axis, and the ratio between the two frequencies on the x axis. The result is a much more intuitive plot. Remember, sometimes you have to do something complicated to make something simple Section 4.1.\n\nlibrary(ggiraph, verbose = FALSE)\nlibrary(ggrepel)\n\np1 &lt;- distressed_texts_binary |&gt; \n  mutate(common = (nondistressed_count + distressed_count)/2) |&gt; \n  ggplot(aes(distressed_freq_ratio, common, \n             label = word,\n             color = distressed_freq_ratio,\n             tooltip = word, data_id = word # aesthetics for interactivity\n             )) +\n    geom_point_interactive() +\n    geom_text_repel_interactive() +\n    scale_y_continuous(trans = \"log2\", breaks = ~.x,\n                       minor_breaks = ~2^(seq(0,log2(.x[2]))),\n                       labels = c(\"Rare\", \"Common\")) +   \n    scale_x_continuous(trans = \"log2\", limits = c(1/6,6),\n                       breaks = c(.25, 1, 4),\n                       labels = c(\"Characteristically\\nNon-Distressed\",\n                                  \"Equal Proportion\",\n                                  \"Characteristically\\nDistressed\")) +\n    scale_color_gradient2(low = \"blue4\", \n                          mid = \"#E2E2E2\", \n                          high = \"red4\", \n                          trans = \"log2\", # log scale for ratios\n                          guide = \"none\") +\n    labs(title = \"Stop Words in Distressed and Non-Distressed Texts\",\n         x = \"\",\n         y = \"Average Frequency\",\n         color = \"\") +\n    theme_minimal()\n\ngirafe_options(\n  girafe(ggobj = p1),\n  opts_tooltip(css = \"font-family:sans-serif;font-size:1em;color:Black;\")\n  )\n\n\n\n\n\nBecause we love these rotated F/F plots so much, we couldn’t help showing off one more example, this time with data from the Corpus of Contemporary American English (Davies, 2009):\n\n# get frequency data\nhttr::GET(\"https://www.wordfrequency.info/files/genres_sample.xls\",\n          httr::write_disk(tf &lt;- tempfile(fileext = \".xls\")))\nword_freqs &lt;- readxl::read_excel(tf) |&gt; \n  select(lemma, ACADEMIC, SPOKEN)\n\n\np2 &lt;- word_freqs |&gt; \n  filter(ACADEMIC != 0, SPOKEN != 0) |&gt; \n  # generate tooltip text\n  mutate(rep = if_else(ACADEMIC/SPOKEN &gt; 1, \n                       \"more common in academic texts\",\n                       \"more common in spoken texts\"),\n         mult = if_else(ACADEMIC/SPOKEN &gt; 1, \n                        as.character(round(ACADEMIC/SPOKEN, 2)),\n                        as.character(round(SPOKEN/ACADEMIC, 2))),\n         tooltip = paste0(\"&lt;b&gt;\",lemma, \"&lt;/b&gt;\", \"&lt;br/&gt;\", \n                          mult, \"x \", rep)) |&gt; \n  ggplot(aes(ACADEMIC/SPOKEN, (ACADEMIC + SPOKEN)/2, \n             label = lemma,\n             color = ACADEMIC/SPOKEN,\n             tooltip = tooltip, \n             data_id = lemma # aesthetics for interactivity\n             )) +\n    geom_point_interactive() +\n    scale_x_continuous(trans = \"log2\", \n                       breaks = c(1/100, 1, 100),\n                          labels = c(\"Characteristically\\nSpoken\",\n                                     \"Equal Proportion\",\n                                     \"Characteristically\\nAcademic\")) +\n    scale_y_continuous(trans = \"log2\", \n                       breaks = ~.x, \n                       minor_breaks = ~2^(seq(0, log2(.x[2]))),\n                       labels = c(\"Rare\", \"Common\")) +\n    scale_color_gradientn(limits = c(1/740, 740),\n                          colors = c(\"#023903\", \n                                     \"#318232\",\n                                     \"#E2E2E2\", \n                                     \"#9B59A7\",\n                                     \"#492050\"), \n                          trans = \"log2\", # log scale for ratios\n                          guide = \"none\") +\n    labs(title = \"Academic vs. Spoken English\",\n         x = \"\", y = \"\",\n         color = \"\") +\n    theme_minimal()\n\ngirafe_options(\n  girafe(ggobj = p2),\n  opts_tooltip(css = \"font-family:sans-serif;font-size:1em;color:Black;\")\n  )   \n\n\n\n\n\n\nThis is a particularly good visualization, because it tells an interesting story. The story it tells is “Academic English is very different from spoken English”. The wide aspect ratio and the tooltip text with “X more common in  texts” especially emphasize this point. The F/F plot is an appropriate visualization method because this story pertains to the full distribution of words—look at how many are more than 10 times more common in one or the other!\n\n5.1.2 Advanced Frequency/Frequency Plots\nFor more information about frequency/frequency plots and other related plot types incorporating various statistics, see the scattertext tutorial (Kessler, 2017). While the tutorial is intended for the scattertext library in Python, almost all examples can be produced in R with ggplot2 and ggiraph. Search bars and other responsive interactivity can be accomplished with shiny.\n\n\n\n\n\n\n\nAdvantages of Frequency/Frequency Plots\n\n\n\n\n\nSpatial Mapping: F/F plots use axes, which make it easy to compare values of different words.\n\nReadability: The layout of F/F plots is easy to interpret, especially when rotated and properly labeled.\n\nFull Picture: F/F plots convey the full shape of the frequency distribution, rather than singling out words most characteristic of one side or the other. This can be useful when the distribution itself is interesting.\n\n\n\n\n\n\n\n\n\nDisadvantages of Frequency/Frequency Plots\n\n\n\n\n\nInteractivity Required: F/F plots require interactivity to be maximally informative.\n\nMessy: Without interactivity, labels can be messy and confusing.\n\nImplies that Correlation is Interesting: F/F plots may imply that the point of the graph is to show the correlation between frequencies in the two texts. Rotating the axes mostly solves this problem.\n\nVague: By showing many words at the same time, F/F plots make it difficult to focus in on particular stories (unless the story is about the distribution itself)."
  },
  {
    "objectID": "word-viz.html#word-clouds",
    "href": "word-viz.html#word-clouds",
    "title": "5  Visualizing Distributions of Words",
    "section": "\n5.2 Word Clouds",
    "text": "5.2 Word Clouds\nWord clouds are commonly used for purposes like:\n\nSummarizing text using word frequencies\nDecorating placemats at cheap restaurants\nComparing word usage in two groups of texts\nCorrelating word usage with a construct of interest\n\nWord clouds are not a good tool for summarizing text. They are a perfectly fine tool for kitschy placemats, but those are beyond the scope of this textbook. In the world of data science, there are only two legitimate uses for word clouds: comparing words across two groups of texts, and correlating word frequencies with a construct of interest. Even these legitimate uses break a fundamental rule of data visualization, since by showing many words at the same time they are telling many stories at the same time, each distracting from the others. Nevertheless, analyses often include so many words (or other units of text) that producing a visualization for each one is unfeasible, and a summary graphic is necessary.\n\n5.2.1 Word Clouds for Comparing Two Groups\nWord clouds generally have three aesthetics: label, color, and size:\n\n\nlabel will always be the text of the words.\n\ncolor is appropriate for representing relative frequency, since it has a neutral center (where the frequencies in both groups are the same and distressed_freq_ratio = 1. Such a neutral center calls for a diverging color scale (Section 4.2.3). Because we are representing the ratio of two frequencies, it is appropriate to use a log scale (see Section 4.2.1). This will make the scale symmetrical for values above and below the neutral center.\n\nsize is technically unnecessary, since the diverging color scale already represents both the valence and the magnitude of the relative frequency. In practice though, we are generally most interested in the largest differences. Size is therefore used to emphasize words with greater a discrepancy between the groups. This magnitude value is calculated as abs(log2(distressed_freq_ratio)).\n\n\ndistressed_texts_binary &lt;- distressed_texts_binary |&gt; \n  mutate(freq_ratio_log_magnitude = abs(log2(distressed_freq_ratio)))\n\nhead(distressed_texts_binary)\n\n#&gt; # A tibble: 6 × 5\n#&gt;   word  distressed_count nondistressed_count distressed_freq_ratio\n#&gt;   &lt;chr&gt;            &lt;int&gt;               &lt;int&gt;                 &lt;dbl&gt;\n#&gt; 1 the               3297                2985                 1.10 \n#&gt; 2 to                2556                2415                 1.06 \n#&gt; 3 and               2125                1856                 1.14 \n#&gt; 4 of                1592                1416                 1.12 \n#&gt; 5 i                 1587                1874                 0.847\n#&gt; 6 a                 1547                1603                 0.965\n#&gt; # ℹ 1 more variable: freq_ratio_log_magnitude &lt;dbl&gt;\n\n\nFor creating word clouds in R we use the ggwordcloud package, with its geom_text_wordcloud geom:\n\nlibrary(ggwordcloud)\nset.seed(2)\n\ndistressed_texts_binary |&gt; \n  # top 100 highest discrepancy words\n  arrange(desc(freq_ratio_log_magnitude)) |&gt; \n  slice_head(n = 150) |&gt; \n  # plot\n  ggplot(aes(label = word, \n             size = freq_ratio_log_magnitude, \n             color = distressed_freq_ratio)) +\n    geom_text_wordcloud(show.legend = TRUE) + # wordcloud geom\n    scale_radius(range = c(2, 18), guide = \"none\") + # control text size\n    scale_color_gradient2(\n      name = \"Distressed /\\nNon-distressed\\nFrequency\",\n      labels = ~ MASS::fractions(.x), # show legend labels as fractions\n      low = \"blue3\", mid = \"white\", high = \"red3\", # set diverging color scale\n      trans = \"log2\" # log scale\n      ) +\n    theme_void() # blank background\n\n\n\n\nWe can now easily see that the word most representative of non-distressed texts is “interesting”, which is far more representative of one group than any other word in the analysis.\nThe angle_group aesthetic can be used to separate out the words more frequent in distressed texts from those more frequent in non-distressed texts:\n\nset.seed(2)\n\ndistressed_texts_binary |&gt; \n  # top 100 highest discrepancy words\n  arrange(desc(freq_ratio_log_magnitude)) |&gt; \n  slice_head(n = 150) |&gt; \n  # plot\n  ggplot(aes(label = word, \n             size = freq_ratio_log_magnitude, \n             color = distressed_freq_ratio,\n             angle_group = distressed_freq_ratio &gt; 1)) +\n    geom_text_wordcloud(show.legend = TRUE) + # wordcloud geom\n    scale_radius(range = c(2, 18), guide = \"none\") + # control text size\n    scale_color_gradient2(\n      name = \"Distressed /\\nNon-distressed\\nFrequency\",\n      labels = ~ MASS::fractions(.x), # show legend labels as fractions\n      low = \"blue3\", mid = \"grey\", high = \"red3\", # set diverging color scale\n      trans = \"log2\" # log scale\n      ) +\n    theme_void() # blank background\n\n\n\n\nAlternatively, we can specify an original position for each label (as x and y aesthetics) to create multiple clouds:\n\nset.seed(2)\n\ndistressed_texts_binary |&gt; \n  # top 100 highest discrepancy words\n  arrange(desc(freq_ratio_log_magnitude)) |&gt; \n  slice_head(n = 150) |&gt; \n  # plot\n  ggplot(aes(label = word, \n             size = freq_ratio_log_magnitude, \n             color = distressed_freq_ratio,\n             x = distressed_freq_ratio &lt; 1,\n             y = distressed_freq_ratio &gt; 1)) +\n    # wordcloud geom\n    geom_text_wordcloud(show.legend = TRUE) + \n    # control text size\n    scale_radius(range = c(2, 18), guide = \"none\") + \n    scale_color_gradient2(\n      name = \"Distressed /\\nNon-distressed\\nFrequency\",\n      # show legend labels as fractions\n      labels = ~ MASS::fractions(.x), \n      # set diverging color scale\n      low = \"blue3\", mid = \"grey\", high = \"red3\", \n      # log scale\n      trans = \"log2\" \n      ) +\n    theme_void() # blank background\n\n\n\n\n\n5.2.2 Word Clouds for Continuous Variables of Interest\nRecently, some have advocated using correlation coefficients instead of frequency ratios in word clouds. This approach has three advantages:\n\nCorrelation coefficients take variance into account.\nSince correlation coefficients are more commonly used, it is easier to perform significance testing on them. This way we can include only significant results in the visualization.\nUnlike frequency ratios, which always compare two groups, correlation coefficients can be applied to continuous variables of interest.\n\nTo apply this method to the data from Buechel et al. (2018), we can use participants’ continuous distress ratings for each text. We count the occurrences of each word in each text, and measure the correlation between these frequency variables and the corresponding distress ratings. Since the association may be non-linear, we use the Kendall rank correlation. You can see the full calculations by pressing the “View Source” button at the bottom of this page.\nWe can now map the strength of the correlation (i.e. abs(cor)) to size, and use color to show the direction of the correlation.\n\ndistress_cor &lt;- read_csv(\"data/distress_cor.csv\", show_col_types = FALSE)\nhead(distress_cor)\n\n#&gt; # A tibble: 6 × 2\n#&gt;   word      cor\n#&gt;   &lt;chr&gt;   &lt;dbl&gt;\n#&gt; 1 and    0.0983\n#&gt; 2 from   0.0680\n#&gt; 3 how    0.0448\n#&gt; 4 is     0.0422\n#&gt; 5 it    -0.0354\n#&gt; 6 me     0.0473\n\nset.seed(2)\n\ndistress_cor |&gt; \n  arrange(desc(abs(cor))) |&gt; \n  ggplot(aes(label = word, \n             color = cor, \n             size = abs(cor),\n             angle_group = cor &lt; 0)) +\n    geom_text_wordcloud(eccentricity = 1.2, show.legend = TRUE) +\n    scale_radius(range = c(4, 15), guide = \"none\") +\n    labs(caption = \"All correlations passed significance testing at p &lt; .05\") + \n    scale_color_gradient2(\n        name = \"Correlation\\nwith distress\\n(Kendall's τ)\",\n        low = \"blue3\", mid = \"white\", high = \"red3\", # set diverging color scale\n        ) +\n    theme_void()\n\n\n\n\n“Interesting” is still the most highly correlated word, indicating lack of distress, but now we can see that “and” and “we” are highly indicative of distress. The assurance that all correlations passed significance testing makes for a particularly convincing graphic.\n\n5.2.3 Advanced Word Clouds\nFor more information about how word clouds are generated and how to customize them, see Pennec (2023). Be careful though - any customization of your word clouds should be in the service of communicating information effectively.\n\n\n\n\n\n\n\nAdvantages of Word Clouds\n\n\n\n\n\nTo the Point: Word clouds emphasize words most characteristic of the variable of interest.\n\nNo Interactivity Required: Word clouds show many words at once without requiring interactivity.\nLooks Fancy\n\n\n\n\n\n\n\n\n\nDisadvantages of Word Clouds\n\n\n\n\n\nHard to Interpret Proportions: Size and color aesthetics make it extremely difficult to compare values of different words (e.g. Is x twice as blue as y?).\n\nVague: By showing many words at the same time, word clouds make it difficult to focus in on particular stories.\n\n\n\n\nPress the “View Source” button below to see the hidden code blocks in this chapter.\n\n\n\n\n\nBuechel, S., Buffone, A., Slaff, B., Ungar, L. H., & Sedoc, J. (2018). Modeling empathy and distress in reaction to news stories. CoRR, abs/1808.10399. http://arxiv.org/abs/1808.10399\n\n\nDavies, M. (2009). The 385+ million word corpus of contemporary american english (1990―2008+): Design, architecture, and linguistic insights. International Journal of Corpus Linguistics, 14, 159–190. https://www.english-corpora.org//coca/\n\n\nHellman, A. B. (2011). Vocabulary size and depth of word knowledge in adult-onset second language acquisition. International Journal of Applied Linguistics, 21(2), 162–182.\n\n\nKessler, J. S. (2017). Scattertext: A browser-based tool for visualizing how corpora differ. https://github.com/JasonKessler/scattertext\n\n\nPennec, E. le. (2023). Ggwordcloud: A word cloud geom for ggplot2. In lepennec.github.io/ggwordcloud/. https://lepennec.github.io/ggwordcloud/articles/ggwordcloud.html"
  },
  {
    "objectID": "data-viz-resources.html#general-data-visualization-materials",
    "href": "data-viz-resources.html#general-data-visualization-materials",
    "title": "6  Additional Resources for Data Visualization",
    "section": "\n6.1 General Data Visualization Materials",
    "text": "6.1 General Data Visualization Materials\n\n\nFundamentals of Data Visualization: A textbook on dataviz methodology, covering topics outlined here in much greater detail\n\nR Graph Gallery: Hundreds of example data visualization with tutorials on how to make them in R\n\nR Graphics Cookbook: A textbook with detailed instructions for creating graphs in R."
  },
  {
    "objectID": "data-viz-resources.html#advanced-methods",
    "href": "data-viz-resources.html#advanced-methods",
    "title": "6  Additional Resources for Data Visualization",
    "section": "\n6.2 Advanced Methods",
    "text": "6.2 Advanced Methods\n\n\nColoring in R’s blind spot: Tutorial introducing more than 100 palettes now included with base R, as well as functions for manipulating colors (used in multiple visualizations in this chapter)\n\nggnewscale: An intuitive method for incorporating multiple color scales in a single ggplot2 graphic (used in multiple visualizations in this chapter)\n\nggrepel: Tools for avoiding overlapping text labels in ggplot2 (used in multiple visualizations in this chapter)\n\ngganimate: A principled approach to animating with ggplot2\n\nggiraph: Dynamic and interactive html graphics for ggplot2\n\nggforce: Various advanced shapes, lines, scales, and plot types for ggplot2\n\nggborderline Lines that pop in ggplot2 (used in multiple visualizations in this chapter)\n\nrayshader: Cinematic 3D graphics for ggplot2"
  },
  {
    "objectID": "data-viz-resources.html#packages-for-unusual-plot-types",
    "href": "data-viz-resources.html#packages-for-unusual-plot-types",
    "title": "6  Additional Resources for Data Visualization",
    "section": "\n6.3 Packages for Unusual Plot Types",
    "text": "6.3 Packages for Unusual Plot Types\n\n\nggwordcloud: Word clouds\n\nggbeeswarm: Sina (AKA beeswarm) plots—almost always an improvement on box plots or violin plots\n\nggcorrplot: Correlation matrices\n\nDifferent Ways of Plotting U.S. Map in R: A book chapter comparing eight different methods for plotting map data in R\n\nggridges: Ridgeline plots\n\nggnetwork: Network data\n\nwormsplot: step charts with smooth transitions (in development by one of the authors of this book)\n\nAdditional ggplot2 extensions can be found here."
  },
  {
    "objectID": "data-sources.html",
    "href": "data-sources.html",
    "title": "Sources of Data",
    "section": "",
    "text": "Language is everywhere. The aspiring data science in psychology researcher can find data in a variety of sources, ranging from novel experiments to web scraping. In this unit, we introduce the most popular sources of data, along with the advantages and disadvantages of each one.\nIn the sections on APIs and web scraping, we give a detailed tutorial on accessing these resources with code."
  },
  {
    "objectID": "experiments.html",
    "href": "experiments.html",
    "title": "7  Experiments",
    "section": "",
    "text": "Experiments are the backbone of causal inference, and text analysis is no exception. Carefully controlled experimental designs, whether in a laboratory or on Amazon’s Mechanical Turk, are a good way to mitigate the effects of confounding variables.\nFor example, Sap et al. (2020) had online participants write either true stories that happened to them recently, or fictional stories about the same topic. They then used a large language model, GPT, to measure two likelihoods for each sentence in the story: the likelihood of the sentence given the previous sentence, and the likelihood of the sentence given a rough summary of the story. The ratio of these two likelihoods is a measure of how predictably the story flows from one point to another. Sap et al. (2020) found that fictional stories flow much more predictably than true ones. They also found that true stories begin to become flow more predictably when they are retold 2-3 months later. Sap et al. (2022) reproduced these findings using a more advanced language model, GPT-3.\n\n\n\n\n\n\nAdvantages of Experimental Data Collection\n\n\n\n\nControl: Experiments mitigate the effects of confounding variables.\nCustomization: Experimenters can tailor the experiment to fit their particular research questions.\n\n\n\n\n\n\n\n\n\nDisadvantages of Experimental Data Collection\n\n\n\n\nExpensive\nTime-Consuming\nSmall Sample Size: Because they are costly and time-consuming, experiments generally result in small datasets.\n\n\n\n\n\n\n\n\nSap, M., Horvitz, E., Choi, Y., Smith, N. A., & Pennebaker, J. (2020). Recollection versus imagination: Exploring human memory and cognition via neural language models. In D. Jurafsky, J. Chai, N. Schluter, & J. Tetreault (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics (pp. 1970–1978). Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.acl-main.178\n\n\nSap, M., Jafarpour, A., Choi, Y., Smith, N. A., Pennebaker, J. W., & Horvitz, E. (2022). Quantifying the narrative flow of imagined versus autobiographical stories. Proceedings of the National Academy of Sciences, 119(45), e2211715119. https://doi.org/10.1073/pnas.2211715119"
  },
  {
    "objectID": "corpora.html#archived-experimental-data",
    "href": "corpora.html#archived-experimental-data",
    "title": "8  Corpus Data",
    "section": "8.1 Archived Experimental Data",
    "text": "8.1 Archived Experimental Data\nScientists who run large experiments often publish their data online for use in further research. For example, data from Sap et al. (2020) and Sap et al. (2022), summarized in Chapter 7, are available online as the Hippocorpus dataset. Another good example is the Empathic Reactions dataset (Buechel et al., 2018), used in Section 4.2.1 and Chapter 5, in which participants read news stories, rated their own empathy and distress after reading them, and then described their thoughts about it verbally.\nOpen experimental data are often linked in published papers, especially since the founding of the Center for Open Science in 2013. Many psychology-related datasets can be browsed freely on osf.io, the Harvard Dataverse, and other locations.\n\n\n\n\n\n\nAdvantages of Archived Experimental Data\n\n\n\n\nProfessional: Experiments conducted by trained academics are generally well designed.\nWell-Documented: Datasets used in published papers have extensive documentation of the methods used to produce them.\n\n\n\n\n\n\n\n\n\nDisadvantages of Archived Experimental Data\n\n\n\n\nSometimes Not Documented: Datasets not used in published papers often have poor documentation.\nSmall Sample Size: Experiments often result in relatively small datasets, which can pose problems for certain NLP methods."
  },
  {
    "objectID": "corpora.html#linguistics-corpora",
    "href": "corpora.html#linguistics-corpora",
    "title": "8  Corpus Data",
    "section": "8.2 Linguistics Corpora",
    "text": "8.2 Linguistics Corpora\nThe field of linguistics has a long tradition of corpus data. Linguistics corpora provide extensive records of spoken and written speech in a wide range of contexts. These corpora are often very large and professionally curated, making them ideal for the techniques described in this book. On the other hand, they are generally curated with linguistics in mind, not psychology. This means that applying them to psychological questions requires some ingenuity.\nOne popular semi-experimental linguistics corpus is the HCRC Map Task Corpus (Anderson et al., 1991), in which pairs of participants collaborated in a communication game. In each pair, one partner could see a treasure map with a path through various landmarks, while the other partner had a similar map without a path. The first partner explained to the second how to draw the path. The partners’ communication accuracy can be measured as the distance between the drawn path and the original. Full dialogue transcriptions, as well as accuracy scores, are available online. The Map Task Corpus has been reproduced in many languages, including Hebrew and is commonly used in psychology. For example, Dideriksen et al. (2023) used a Danish version of the Map Task Corpus, along with other dialogue corpora, to track the ways that speakers collaborate to achieve mutual understanding in different contexts.\n\nEnglish-Corpora.org: A list of the most widely used corpora of naturalistic English speech and writing, with download links for each. Also included preprocessed data, such as word frequency counts for nearly 100 genres, from the Corpus of Contemporary American English (Davies, 2009), used in Section 5.1.1.\nUniversity of British Columbia Language Corpora List: Links to written and spoken language data in dozens of languages, including from bilingual and multilingual speakers.\nWikipedia’s List of Text Corpora\nList of NLP Corpora: Links to useful corpora for NLP tasks like task-oriented dialogue, translation, and sentiment analysis.\n\n\n\n\n\n\n\nAdvantages of Linguistics Corpora\n\n\n\n\nProfessional: Linguistics corpora are generally well-curated and well-documented.\nEcological Validity: Corpora are often large and naturalistic—including for spoken dialogue, a domain that is otherwise out of reach for NLP.\n\n\n\n\n\n\n\n\n\nDisadvantages of Linguistics Corpora\n\n\n\n\nDomain-Specific: Linguistics corpora are generally created by linguists for linguists."
  },
  {
    "objectID": "corpora.html#data-gathered-from-the-internet",
    "href": "corpora.html#data-gathered-from-the-internet",
    "title": "8  Corpus Data",
    "section": "8.3 Data Gathered From the Internet",
    "text": "8.3 Data Gathered From the Internet\nThe internet is full of text, and you are not the first one to want to use it for research. Many corpora of online text data are free to download.\nSome sets of internet data are professionally curated and well balanced. For example, the Blog Authorship Corpus (Schler et al., 2006) includes 681,288 blog posts annotated with age group (binned into ages 13-17, 23-27, and 33-47) and gender of author, with an equal number of male and female bloggers in each age group. Similarly, the 20 Newsgroups data set includes 20,000 newsgroup documents, partitioned evenly across 20 different newsgroups.\nSome sets of internet data are available only post-processing. For example, Eichstaedt et al. (2015) published Twitter n-gram (and LDA topic) frequencies by US county, along with corresponding measures of well-being featured in 3.\nSome sets of internet data are very lightly curated. For example, the Reddit Top 2.5 Million dataset contains the top 1,000 all-time posts from the top 2,500 subreddits in August 2013, excluding NSFW subreddits.\nSome sets of archived internet data are not curated at all. These are sometimes referred to as data dumps. For example, Baumgartner et al. (2020) published all Reddit Submissions and Comments posted during April 2019. Even more extensive data dumps of Reddit, covering historical data back to Reddit’s inception, can be found in records of Pushshift Reddit. Similar archives exist for Twitter.\nMost research topics in psychology do not require up-to-date data. As such, historical archives can be an invaluable resource. For example, Biester et al. (2022) used patterns curated by Cohan et al. (2018) to search Pushshift Reddit for users who publicly shared a depression diagnosis (e.g. “I have been diagnosed with depression”). They then used dictionary-based methods Chapter 14 to measure various emotional qualities in users’ post during the weeks leading their declaration of the depression diagnosis, and in the weeks following. They found that anxiety, sadness, and cognitive processing increases in the weeks leading up to the declaration, and decreases afterwards.\n\n\n\n\n\n\nAdvantages of Archival Internet Data\n\n\n\n\nEasy: Pre-gathered datasets are low-cost and low-effort, often for very large sample sizes.\nUnintrusive: With pre-gathered datasets, you don’t have to worry about API usage limits or web scraping etiquette.\n\n\n\n\n\n\n\n\n\nDisadvantages of Archival Internet Data\n\n\n\n\nOld: Archival data do not reflect current events or recent trends.\n\n\n\n\n\n\n\n\n\nA Disclaimer on Social Media Data Dumps\n\n\n\nSince Reddit and Twitter restricted their API access in 2023, the legal status of large archival data dumps from those platforms (such as Pushshift Reddit) has been unclear. We are not qualified to give legal advice, but as long as you are not using the data for profit, you are unlikely to get in trouble."
  },
  {
    "objectID": "corpora.html#other-public-data-sources",
    "href": "corpora.html#other-public-data-sources",
    "title": "8  Corpus Data",
    "section": "8.4 Other Public Data Sources",
    "text": "8.4 Other Public Data Sources\n\nKaggle: An online hub for data science, including many text- and psychology-related datasets\nHathiTrust: A digital library of 18+ million digitized books, including many curated collections\nForbes list of 30 Amazing (And Free) Public Data Sources\n\n\n\n\n\n\nAnderson, A. H., Bader, M., Bard, E. G., Boyle, E., Doherty, G., Garrod, S., Isard, S., Kowtko, J., McAllister, J., Miller, J., Sotillo, C., Thompson, H. S., & Weinert, R. (1991). The HCRC map task corpus. Language and Speech, 34(4), 351–366. https://doi.org/10.1177/002383099103400404\n\n\nBaumgartner, J., Zannettou, S., Keegan, B., Squire, M., & Blackburn, J. (2020). The pushshift reddit dataset. Zenodo. https://doi.org/10.5281/zenodo.3608135\n\n\nBiester, L., Pennebaker, J., & Mihalcea, R. (2022). Emotional and cognitive changes surrounding online depression identity claims. PLOS ONE, 17(12), 1–20. https://doi.org/10.1371/journal.pone.0278179\n\n\nBuechel, S., Buffone, A., Slaff, B., Ungar, L. H., & Sedoc, J. (2018). Modeling empathy and distress in reaction to news stories. CoRR, abs/1808.10399. http://arxiv.org/abs/1808.10399\n\n\nCohan, A., Desmet, B., Yates, A., Soldaini, L., MacAvaney, S., & Goharian, N. (2018). SMHD: A large-scale resource for exploring online language usage for multiple mental health conditions. In E. M. Bender, L. Derczynski, & P. Isabelle (Eds.), Proceedings of the 27th international conference on computational linguistics (pp. 1485–1497). Association for Computational Linguistics. https://aclanthology.org/C18-1126\n\n\nDavies, M. (2009). The 385+ million word corpus of contemporary american english (1990―2008+): Design, architecture, and linguistic insights. International Journal of Corpus Linguistics, 14, 159–190. https://www.english-corpora.org//coca/\n\n\nDideriksen, C., Christiansen, M. H., Tylén, K., Dingemanse, M., & Fusaroli, R. (2023). Quantifying the interplay of conversational devices in building mutual understanding. Journal of Experimental Psychology: General, 152, 864–889. https://doi.org/10.1037/xge0001301\n\n\nEichstaedt, J. C., Schwartz, H. andrew, Kern, M. L., Park, G., Labarthe, D. R., Merchant, R. M., Jha, S., Agrawal, M., Dziurzynski, L. A., Sap, M., Weeg, C., Larson, E. E., Ungar, L. H., & Seligman, M. E. P. (2015). Psychological language on twitter predicts county-level heart disease mortality. Psychological Science, 26(2), 159–169. https://doi.org/10.1177/0956797614557867\n\n\nSap, M., Horvitz, E., Choi, Y., Smith, N. A., & Pennebaker, J. (2020). Recollection versus imagination: Exploring human memory and cognition via neural language models. In D. Jurafsky, J. Chai, N. Schluter, & J. Tetreault (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics (pp. 1970–1978). Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.acl-main.178\n\n\nSap, M., Jafarpour, A., Choi, Y., Smith, N. A., Pennebaker, J. W., & Horvitz, E. (2022). Quantifying the narrative flow of imagined versus autobiographical stories. Proceedings of the National Academy of Sciences, 119(45), e2211715119. https://doi.org/10.1073/pnas.2211715119\n\n\nSchler, J., Koppel, M., Argamon, S., & Pennebaker, J. (2006). Effects of age and gender on blogging. 199–205."
  },
  {
    "objectID": "apis.html#api-basic-concepts",
    "href": "apis.html#api-basic-concepts",
    "title": "9  Web APIs",
    "section": "\n9.1 API Basic Concepts",
    "text": "9.1 API Basic Concepts\n\n\nRequests: Each time you visit a URL associated with an API, you are submitting a request for data.\n\nEndpoints: Every API has at least one endpoint, a contact point for particular types of requests.\n\nRate Limits: Many APIs set limits on the number of requests you can make per minute (or per second). This is because processing requests costs time and money for the host. If you go beyond the rate limit, the API will return an error like “429 Too Many Requests”.\n\nAuthentication: Some APIs are not open to the public, instead requiring users to apply for access or pay for a subscription. When accessing these APIs, you need an API key or an access token. This is your password for the API.\n\n\n9.1.1 vosonSML\n\nlibrary(vosonSML)\n\nFor accessing social media APIs with vosonSML, you only need two functions:\n\n\nAuthenticate() creates a credential object that contains any keys or access tokens needed to access a particular API. This credential object can be reused as long as your credentials don’t change.\n\nCollect() initiates a series of API requests and stores the results as a dataframe or list of dataframes.\n\nvosonSML also provides tools for working with network data (i.e. the ways in which users or posts are connected to one another), but these will not be covered in this textbook."
  },
  {
    "objectID": "apis.html#reddit",
    "href": "apis.html#reddit",
    "title": "9  Web APIs",
    "section": "\n9.2 Reddit",
    "text": "9.2 Reddit\nReddit generated over 3 billion posts and comments in 2022. Many of these contain long-form text. And its API is free. These traits make it very useful to researchers.\nReddit content exists on three levels:\n\n\nCommunities, called “subreddits” are spaces for users to post about a specific topic. Individual subreddits are referred to as “r/SUBREDDIT”. For example, r/dataisbeautiful is for data visualizations, r/NaturalLanguage is for posts about natural language processing, and r/SampleSize is a place to gather volunteer participants for surveys and polls. Communities are policed by moderators, users who can remove posts or ban other users from the community.\n\nPosts are posted by users to a particular subreddit. Each post has a title, which is always text, and content, which can contain text, images, and videos.\n\nComments are responses to posts, responses to responses to posts, responses to responses to responses to posts, etc. These are always text.\n\n\n9.2.1 The Reddit Algorithm\nSee Proferes et al. (2021)\n\n9.2.2 Demographics on Reddit\ne.g. “(25M)”, “[25M]”\n\n9.2.3 Threads\nA post with all of its associated comments is called a thread.\nXiao & Mensah (2022)\n\n9.2.4 Communities\nAshokkumar & Pennebaker (2022) used both experimental data and naturalistic data retrieved from the Reddit API (r/The_Donald and r/hillaryclinton)\n\n\n\n\n\n\nAdvantages of Reddit Data\n\n\n\n\nDiverse Communities:\nLong-form Responses:\n\nAnonymity: Users can remain relatively anonymous, which might encourage more honest and open sharing of experiences.\n\n\n\n\n\n\n\n\n\nDisadvantages of Reddit Data\n\n\n\n\n\nSelection Bias: Certain subreddits may attract specific demographics, leading to potential selection bias in the data.\n\nLimited Generalizability: Findings from Reddit may not generalize to the broader population due to the specific demographics and interests of its user base."
  },
  {
    "objectID": "apis.html#twitter",
    "href": "apis.html#twitter",
    "title": "9  Web APIs",
    "section": "\n9.3 Twitter",
    "text": "9.3 Twitter\n\n\n\n\n\n\nAdvantages of Twitter Data\n\n\n\n\n\nHashtag Tracking: Researchers can track specific hashtags related to psychological phenomena.\nCharacter Limit:\n\n\n\n\n\n\n\n\n\nDisadvantages of Twitter Data\n\n\n\n\n\nCharacter Limit: The character limit on tweets may limit the depth of responses and the ability to convey complex psychological experiences.\n\nSampling Bias: Twitter users may not be representative of the general population, leading to potential sampling bias.\n\nLimited Context: Tweets may lack context, making it challenging to fully understand the meaning behind short messages."
  },
  {
    "objectID": "apis.html#mastadon",
    "href": "apis.html#mastadon",
    "title": "9  Web APIs",
    "section": "\n9.4 Mastadon",
    "text": "9.4 Mastadon\nMany servers have a theme based on a specific interest. It is also common for servers to be based around a particular locality, region, ethnicity, or country.\nTwitter and Mastodon use hashtags to categorize and organize content. subreddits on Reddit, instances on Mastodon\nOn a standard Mastodon instance, these messages can include up to 500 text-based characters, greater than Twitter’s 280-character limit.\nMastodon uses community-based moderation, in which each server can limit or filter out undesirable types of content, while Twitter uses a single, global policy on content moderation.\n\norganized into servers for topic-specific users\n\n\n\n\n\n\n\n\nAdvantages of Mastadon Data\n\n\n\n\n\nHashtag Tracking: Researchers can track specific hashtags related to psychological phenomena.\nCharacter Limit:\n\n\n\n\n\n\n\n\n\nDisadvantages of Mastadon Data\n\n\n\n\n\nCharacter Limit: The character limit on tweets may limit the depth of responses and the ability to convey complex psychological experiences.\n\nSampling Bias: Twitter users may not be representative of the general population, leading to potential sampling bias.\n\nLimited Context: Tweets may lack context, making it challenging to fully understand the meaning behind short messages.\nMASTADON Customization: Users can choose or create instances with specific rules and communities, allowing for more targeted data collection.\nMASTADON Smaller User Base: Mastodon has a smaller user base compared to major platforms, potentially limiting the diversity of data.\nMASTADON Less Visibility: The decentralized nature may make it harder to discover relevant conversations or trends."
  },
  {
    "objectID": "apis.html#youtube",
    "href": "apis.html#youtube",
    "title": "9  Web APIs",
    "section": "\n9.5 YouTube",
    "text": "9.5 YouTube\nRosenbusch et al. (2019)\n\n\n\n\n\n\nAdvantages of YouTube Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDisadvantages of YouTube Data"
  },
  {
    "objectID": "apis.html#sec-other-apis",
    "href": "apis.html#sec-other-apis",
    "title": "9  Web APIs",
    "section": "\n9.6 Other Web APIs",
    "text": "9.6 Other Web APIs\nhttps://en.wikipedia.org/wiki/Comparison_of_microblogging_and_similar_services https://the-federation.info\n\nWikipedia (including user data)\nFacebook\nSemantic Scholar\nCORE\n\n\n\n\n\n\nAshokkumar, A., & Pennebaker, J. W. (2022). Tracking group identity through natural language within groups. PNAS Nexus, 1(2), pgac022. https://doi.org/10.1093/pnasnexus/pgac022\n\n\nProferes, N., Jones, N., Gilbert, S., Fiesler, C., & Zimmer, M. (2021). Studying reddit: A systematic overview of disciplines, approaches, methods, and ethics. Social Media + Society, 7(2), 20563051211019004. https://doi.org/10.1177/20563051211019004\n\n\nRosenbusch, H., Evans, A. M., & Zeelenberg, M. (2019). Multilevel emotion transfer on YouTube: Disentangling the effects of emotional contagion and homophily on video audiences. Social Psychological and Personality Science, 10(8), 1028–1035. https://doi.org/10.1177/1948550618820309\n\n\nXiao, L., & Mensah, H. (2022). How does the thread level of a comment affect its perceived persuasiveness? A reddit study. In K. Arai (Ed.), Intelligent computing (pp. 800–813). Springer International Publishing."
  },
  {
    "objectID": "scraping.html",
    "href": "scraping.html",
    "title": "10  Web Scraping",
    "section": "",
    "text": "https://r4ds.hadley.nz/webscraping\nuse the polite package by Dmytro Perepolkin"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "13  Recommendations for EDA",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "embeddings.html",
    "href": "embeddings.html",
    "title": "17  Working with Embeddings",
    "section": "",
    "text": "O. N. E. Kjell et al. (2021)\nO. Kjell et al. (2022)\n\n\n\n\nKjell, O. N. E., Giorgi, S., & Schwartz, H. A. (2021). The text-package: An r-package for analyzing and visualizing human language using natural language processing and deep learning. PsyArXiv. https://doi.org/10.31234/osf.io/293kt\n\n\nKjell, O., Sikström, S., Kjell, K., & Schwartz, H. (2022). Natural language analyzed with AI-based transformers predict traditional subjective well-being measures approaching the theoretical upper limits in accuracy. Scientific Reports, 12, 3918. https://doi.org/10.1038/s41598-022-07520-w"
  }
]