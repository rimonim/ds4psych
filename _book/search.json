[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Psychology: Natural Language",
    "section": "",
    "text": "Welcome\nThis is the website for Data Science for Psychology: Natural Language.\nThis book will teach you state-of-the-art methods for analyzing psychological properties of text. It will also cover the fundamentals of data visualization, data collection, and scientific methodology necessary to produce meaningful work in the field.\nAt every step of the way, we will give examples in R, using the tools and rules of the tidyverse. This book will also teach you the basics of the quanteda and text packages for natural language processing (NLP), and the vosonSML package for collecting data from popular social media sites.\nUnless otherwise noted, all figures in this book were generated by us using ggplot2. The full, reproducible code for their generation can be viewed by clicking the “View Source” button at the bottom of each page.\nTo obtain practice assignments that go along with the book, or for any other questions, contact us at info@ds4psych.com\nHow to Cite This Book:\n\nTeitelbaum, L., & Simchon, A. (2024). Data Science for Psychology: Natural Language. https://zenodo.org/doi/10.5281/zenodo.10908366\n\n\nThis book was written for the “Data Science for Psychology Lab” course in the psychology department at Ben-Gurion University of the Negev (BGU), and is supported by funding from BGU.\nThis book is an open source project and uses a Contributor Code of Conduct. By contributing to this book, you agree to abide by its terms.\n\nOn the Cover: Posts on Reddit’s r/relationship_advice, distributed by their emotional content according to the Pleasure-Arousal-Dominance model. The x axis represents pleasure, the y axis represents dominance, and the color scale represents arousal. The size of the points represents the number of comments responding to each post. For more details, see the source code.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction: Why Does Psychology Need Natural Language?",
    "section": "",
    "text": "1.1 “The Mind’s Messenger”\nPsychology is often referred to as a “behavioral science”. Humans engage in many behaviors: eating, sleeping, pressing buttons, coming in five minutes early to work… All sorts of behaviors can reveal secrets of the mind, but one type of behavior in particular has always struck people as greater than all others in its ability to express the richness of thought: language. The 11th century philosopher Bahaye ibn Paquda wrote:\nSomewhat more recently, Tausczik & Pennebaker (2010) expressed the same sentiment:\nWhat exactly is the relationship between psychology and language? The 14th century philosopher William of Ockham proposed that thought itself has an essentially linguistic structure, with subjects, objects, and verbs. According to Ockham then, spoken or written language is a sort of rough reflection of inner language. Theories like Ockham’s, in which language is a straightforward representation of inner psychological life, have been common over the history of philosophy. Nowadays, studies of neurological patients have made it clear that linguistic abilities are not necessary for complex thought (Fedorenko & Varley, 2016). Likewise, now-classic research has made it clear that people cannot be trusted to accurately report their own thought processes (Nisbett & Wilson, 1977). Nevertheless, language is without doubt a centrally important mode of human behavior. Humans are constantly talking to each other or listening to each other talk. Even if this talk cannot be construed as reliable reporting about internal states, it must reflect those states in some way, in the same way that any other behavior must do so.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction: Why Does Psychology Need Natural Language?</span>"
    ]
  },
  {
    "objectID": "intro.html#the-minds-messenger",
    "href": "intro.html#the-minds-messenger",
    "title": "1  Introduction: Why Does Psychology Need Natural Language?",
    "section": "",
    "text": "Speech and the orderly arrangement of words [are the ways in which a human] gives expression to what is in his mind and soul and understands the conditions of others. The tongue is the heart’s pen and the mind’s messenger. Without speech, there would be no social relations between one person and another.1\n\n\n\nLanguage is the most common and reliable way for people to translate their internal thoughts and emotions into a form that others can understand. Words and language, then, are the very stuff of psychology and communication.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction: Why Does Psychology Need Natural Language?</span>"
    ]
  },
  {
    "objectID": "intro.html#problems-with-language-data-and-why-data-science-solves-them",
    "href": "intro.html#problems-with-language-data-and-why-data-science-solves-them",
    "title": "1  Introduction: Why Does Psychology Need Natural Language?",
    "section": "1.2 Problems With Language Data, and Why Data Science Solves Them",
    "text": "1.2 Problems With Language Data, and Why Data Science Solves Them\nPsychologists have long insisted that talk therapy can heal, and that questionnaires can accurately measure psychological phenomena. These are language-based techniques, which rely on the assumption that language processing is linked to more fundamental internal states. Even so, psychological research has generally been unable to study naturalistic language, i.e. the sort of language that people produce in their day-to-day lives. There are three good reasons for this. Let’s go through each one and explain how data science solves the problem.\n\n1.2.1 Language is Hard to Record\nBefore the invention of audio recording technology, using language in scientific research was nearly impossible. Early efforts by linguists to record language samples from representative populations were heroic; starting in 1896, Edmond Edmont spent four years traveling around France on a bicycle conducting specially designed interviews to collect data for the Atlas linguistique de la France. He collected data from 700 participants in total (Crystal, 1997). Since then, microphones have made it easier to record speech, but even simple quantitative measurements like word counts have still required painstaking hours of listening to recordings and marking each occurrence of the word.\nThe advent of transformer neural networks has made working with audio data much easier than it once was, but the largest boon to our language recording abilities has come through a different medium: text.\nOnly a few decades ago, public access to text was limited to highly edited long-form productions like books, magazines, and newspapers. Psychologists tend to be more interested in accessing people’s thoughts and feelings as they happen, so these texts held little interest for them. Some psychologists studied diaries or personal letters (e.g. Allport, 1942; Creegan, 1944), but personal documents like these are hard to collect at scale. This all changed with the advent of the Internet. Now more than ever before, people communicate through text—not just in long-distance correspondence, but for day-to-day socializing with friends and family. Moreover, much of this textual communication is synchronous and shares many of the same features as face-to-face spoken conversation (Placiński & Żywiczyński, 2023). Most importantly, much of this textual communication is freely available to researchers, through social media platforms like Twitter, Reddit, and YouTube. Data science techniques allow researchers to access these texts and transform them into manageable datasets with APIs (Chapter 9) and web scraping (Chapter 10).\n\n\n1.2.2 Language is Hard to Quantify\nEven when interesting texts were available to psychologists of the past, they were rarely able to make use of them in quantitative analysis. Language is complex, with near-infinite ways to describe the same thing. There are no clear rules for measuring the extent to which a text reflects depression, anxiety, mania, introspection, or any other psychological construct. The few researchers who tried to extract quantitative psychological dimensions of text were nearly as heroic as Edmond Edmont on his four year journey around France. For example, Peterson & Seligman (1984) administered a questionnaire that prompted participants to write short explanations of various hypothetical scenarios. They then carefully read each response, noted each time a phrase like “because of” or “as a result of” was mentioned, and marked the accompanying explanation. These explanations were then typed by hand and shown one at a time to four trained judges who rated them on various 7-point scales. Finally, the agreement between the judges was assessed and their ratings were aggregated into the final variable used in their analysis of risk factors for depression. Today, this sort of analysis could be performed in a matter of seconds using dictionary-based word counts (Chapter 14), neural embeddings (Chapter 19), or other methods covered in this book.\n\n\n1.2.3 Language is Hard to Control\nLanguage is a social phenomenon. People do not write or speak in a vacuum, they participate in conversations or group discussions, considering their audiences as they form their words. For the researcher, this means that language is full of uncontrolled, confounding variables: Is the speaker responding to another speaker? Who is the other speaker? How many participants are there in the conversation? Researchers in the field of psycholinguistics have tried to solve these problems by isolating speakers in a laboratory setting, contriving situations in which participants process and produce speech without the uncontrolled variability of conversational partners (O’Connell & Kowal, 2003). Nevertheless, the inherently social nature of language has made it difficult to analyze language behavior in even remotely naturalistic settings.\nToday, the highly structured nature of interaction on social media has made the social context of utterances easily measurable. For example, comments on Reddit are always associated with a well-defined community, are responding to a known original post, and are directly responding to previous comments in a tree-like structure (Section 9.2.3). Researchers can leverage this structure to provide robust statistical control by using it in tandem with new methods for quantifying the relationships between utterances. A few decades ago the question “How similar are Daniel’s utterances to Amos’s utterances?” would have seemed hopelessly ill-defined. Similar in what way? Today, answering this question is simple with vector-based semantic embeddings (Chapters 17—19). Methods like these can now make sense of nuanced features of language use in dialogue (see Duran et al., 2019 for an example in psycholinguistics).\nThis book focuses primarily on how to extract psychological dimensions from text. The statistical analysis of these dimensions will not be covered in depth here. Even so, the methods presented in the book can be used in tandem with modern methods of statistical analysis (e.g. Kenny et al., 2006) to draw inferences from complex social interactions between pairs or even larger groups of people.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction: Why Does Psychology Need Natural Language?</span>"
    ]
  },
  {
    "objectID": "intro.html#what-this-textbook-is-useful-for",
    "href": "intro.html#what-this-textbook-is-useful-for",
    "title": "1  Introduction: Why Does Psychology Need Natural Language?",
    "section": "1.3 What This Textbook is Useful For",
    "text": "1.3 What This Textbook is Useful For\nThe tools presented in this book are useful for many fields of psychology, cognitive science, and neuroscience. Most of these fields are related to language, but not all. The following are a few examples of ways to apply these tools in practice:\n\nEnhance experimental control by matching word stimuli according to semantic similarity (e.g. Gagné et al., 2005). See Chapter 18.\nFind similarities between large language models and neural processing in the brain (e.g. Millet et al., 2022). See Chapter 19.\nMeasure the degree to which a therapist and patient build mutual understanding over the course of a session (cf. Duran et al., 2019). See Chapter 17.\nAnalyze emotional responses to current events on social media (e.g. Simchon et al., 2020). See Chapter 9 and Section 20.4.\nFind links between group members’ language and their probability of leaving the group (Ashokkumar & Pennebaker, 2022). See Chapter 8 and Chapter 14.\nValidate personality assessments with individuals’ behavior on social media (Schwartz et al., 2013). See Chapter 15.\n\n\n\n\n\n\nAllport, G. W. (1942). The use of personal documents in psychological science. Social Science Research Council Bulletin, 49, xix + 210–xix + 210.\n\n\nAshokkumar, A., & Pennebaker, J. W. (2022). Tracking group identity through natural language within groups. PNAS Nexus, 1(2), pgac022. https://doi.org/10.1093/pnasnexus/pgac022\n\n\nCreegan, R. F. (1944). The phenomenological analysis of personal documents. The Journal of Abnormal and Social Psychology, 39(2), 244–266. https://doi.org/10.1037/h0062816\n\n\nCrystal, D. (1997). The cambridge encyclopedia of language (Second edition.). Cambridge University Press.\n\n\nDuran, N. D., Paxton, A., & Fusaroli, R. (2019). ALIGN: Analyzing linguistic interactions with generalizable techNiques-a python library. Psychological Methods, 24(4), 419–438.\n\n\nFedorenko, E., & Varley, R. (2016). Language and thought are not the same thing: Evidence from neuroimaging and neurological patients. Annals of the New York Academy of Sciences, 1369. https://doi.org/10.1111/nyas.13046\n\n\nGagné, C., Spalding, T., & Ji, H. (2005). Re-examining evidence for the use of independent relational representations during conceptual combination. Journal of Memory and Language, 53, 445–455. https://doi.org/10.1016/j.jml.2005.03.006\n\n\nKenny, D. A., Kashy, D. A., & Cook, W. L. (2006). Dyadic data analysis. (pp. xix, 458–xix, 458). Guilford Press.\n\n\nMillet, J., Caucheteux, C., Orhan, P., Boubenec, Y., Gramfort, A., Dunbar, E., Pallier, C., & King, J.-R. (2022). Toward a realistic model of speech processing in the brain with self-supervised learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, & A. Oh (Eds.), Advances in neural information processing systems (Vol. 35, pp. 33428–33443). Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2022/file/d81ecfc8fb18e833a3fa0a35d92532b8-Paper-Conference.pdf\n\n\nNisbett, R., & Wilson, T. (1977). Telling more than we can know: Verbal reports on mental processes. Psychological Review, 84, 231–259. https://doi.org/10.1037/0033-295X.84.3.231\n\n\nO’Connell, D., & Kowal, S. (2003). Psycholinguistics: A half century of monologism. The American Journal of Psychology, 116, 191–212. https://doi.org/10.2307/1423577\n\n\nPeterson, C., & Seligman, M. E. (1984). Causal explanations as a risk factor for depression: Theory and evidence. Psychological Review, 91(3), 347–374.\n\n\nPlaciński, M., & Żywiczyński, P. (2023). Modality effect in interactive alignment: Differences between spoken and text-based conversation. Lingua, 293, 103592. https://doi.org/https://doi.org/10.1016/j.lingua.2023.103592\n\n\nSchwartz, H. A., Eichstaedt, J. C., Kern, M. L., Dziurzynski, L., Ramones, S. M., Agrawal, M., Shah, A., Kosinski, M., Stillwell, D., Seligman, M. E. P., & Ungar, L. H. (2013). Personality, gender, and age in the language of social media: The open-vocabulary approach. PLOS ONE, 8(9), 1–16. https://doi.org/10.1371/journal.pone.0073791\n\n\nSimchon, A., Guntuku, S. C., Simhon, R., Ungar, L. H., Hassin, R. R., & Gilead, M. (2020). Political depression? A big-data, multimethod investigation of americans’ emotional response to the trump presidency. Journal of Experimental Psychology. General. https://doi.org/10.1037/xge0000767\n\n\nTausczik, Y. R., & Pennebaker, J. W. (2010). The psychological meaning of words: LIWC and computerized text analysis methods. Journal of Language and Social Psychology, 29(1), 24–54. https://doi.org/10.1177/0261927X09351676",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction: Why Does Psychology Need Natural Language?</span>"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "1  Introduction: Why Does Psychology Need Natural Language?",
    "section": "",
    "text": "Duties of the Heart, Second Treatise on Examination (Chapter 5), trans. Rabbi Moses Hyamson, New York, 1925↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction: Why Does Psychology Need Natural Language?</span>"
    ]
  },
  {
    "objectID": "ethics.html",
    "href": "ethics.html",
    "title": "2  The Ethics of Data Science in Psychology",
    "section": "",
    "text": "2.1 Anonymization is Hard\nSharing data is an important way for researchers to stay accountable to their colleagues and to promote further research. Nevertheless, data sharing can become problematic when individual subjects can be identified. This is especially true in psychology, which often deals with sensitive personal information. As such, it is important to anonymize data before sharing it. You might think that removing personal names would be enough to accomplish this. It is not.\nIn August 2006, the online service provider AOL released the search queries of 657,000 users over a 3-month period. The dataset was anonymized by replacing personal names with a numeric user ID. Within days, New York Times reporters were able to identify user No. 4417749 as a 62-year-old widow from Lilburn, Georgia by putting together searches involving place names, family names, and ages. AOL quickly took the dataset down, but it was too late. The data are still widely available on the internet, and many more users have been identified based on their search histories.\nAs technology improves, data that previously seemed innocuous can be leveraged to reveal personal information. For example, Facebook users’ “likes” were once public information. Kosinski et al. (2013) then showed that likes alone could be used to predict a user’s age, gender, sexual orientation, ethnicity, religion, intelligence, drug use, and more. Facebook now makes page likes accessible only to friends by default.\nKosinski et al. (2013) did their work without the aid of deep neural networks. With more advanced language processing algorithms emerging every day, text data in particular are becoming increasingly difficult to anonymize. The text that people write (and read) is a window into their soul. This is why NLP is so useful for psychology, but it is also a reason to be vigilant.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Ethics of Data Science in Psychology</span>"
    ]
  },
  {
    "objectID": "ethics.html#text-based-psychology-is-powerful",
    "href": "ethics.html#text-based-psychology-is-powerful",
    "title": "2  The Ethics of Data Science in Psychology",
    "section": "\n2.2 Text-Based Psychology is Powerful",
    "text": "2.2 Text-Based Psychology is Powerful\nCambridge Analytica is the prime example of the power of data science in psychology. In the 2010s, Cambridge Analytica used an app to collect demographic and psychological data from tens of millions of Facebook users, and paired this with users’ behavior on Facebook. They then used the resulting psychological measures (based on the well-known Big Five personality traits) to create tailored advertisements for political campaigns. The revelation of this privacy breach created an international scandal for both Facebook and Cambridge Analytica.\nCambridge Analytica used methods not unlike many of those described in this book—methods for extracting psychological characteristics from naturalistic online behavior. In fact, due to developments in the field over the last decade, many of the methods described in this book can be quite a bit more powerful than those employed by Cambridge Analytica. Be careful—the research you conduct can be used for the kind of things that create international scandals.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Ethics of Data Science in Psychology</span>"
    ]
  },
  {
    "objectID": "ethics.html#what-to-do-about-it",
    "href": "ethics.html#what-to-do-about-it",
    "title": "2  The Ethics of Data Science in Psychology",
    "section": "\n2.3 What to do About it",
    "text": "2.3 What to do About it\nThere are no universally accepted rules for ethical text data usage. Many countries have developed data protection laws, for example those of the European Data Protection Supervisor (EDPS) or Israel’s Privacy Protection Authority. Nevertheless, as with any ethical problem, the best policy is to think for yourself, weighing risks against benefits.\nIf you want to share your data widely, but are worried about sensitive private information contained in it, consider using one of many advanced anonymization techniques, such as those that leverage generative AI models to create synthetic data while maintaining statistical properties of the original. These techniques are sometimes costly or labor-intensive, but can be worthwhile for high-impact studies.\nThis chapter is far from a thorough treatment of ethical problems and possible solutions for data collection on the internet. For further reading, we suggest the Association of Internet Researchers Ethical Guidelines.\n\n\n\n\n\nKosinski, M., Stillwell, D., & Graepel, T. (2013). Private traits and attributes are predictable from digital records of human behavior. Proceedings of the National Academy of Sciences, 110(15), 5802–5805. https://doi.org/10.1073/pnas.1218772110",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Ethics of Data Science in Psychology</span>"
    ]
  },
  {
    "objectID": "data-viz.html",
    "href": "data-viz.html",
    "title": "Data Visualization",
    "section": "",
    "text": "There are no objective rules for how to make a good data visualization. This is because not all data visualizations have the same purpose.\nHere are three common purposes for data visualization:\n\nGetting a quick, intuitive understanding of the data you are working with\nLooking for aspects of your data that common statistics might miss (like outliers or nonlinear relationships)\nCommunicating something to people who don’t have a deep understanding of your data\n\nThe first two purposes are an important aspects of exploratory data analysis (EDA). They will be discussed in 15  Open Vocabulary Word Counting. The third one - communication - is the topic of this unit.\nCommunication is hard. It is especially hard for scientists, who often need to balance the needs of multiple target audiences. While insiders in a particular field may want detailed, objective analysis of results and the uncertainty surrounding them, everyone else wants a story.\nA story is clear, oversimplified and sensational. Stories are what grab people’s attention and hold it. Even for experts, stories are what make one study (or news article, or social media post) stand out among many. If you tell a good story, people will want to learn more about the details. If you don’t tell a good story, nobody will want to read your supplementary materials. In this way, all scientists are journalists.",
    "crumbs": [
      "Data Visualization"
    ]
  },
  {
    "objectID": "aesthetics.html",
    "href": "aesthetics.html",
    "title": "3  Why Aesthetic Choices are Important",
    "section": "",
    "text": "Take a moment to appreciate this comic from xkcd.com:\n\nFor most people, aesthetics is the art of making things pleasant to look at. To the data visualizer though, “aesthetics” means something much more precise: Aesthetics are the visual representation of variables.\nJust as journalists need to decide which words to use to express their ideas, data visualizers need to decide which aesthetics to use to express their variables.\nThere are many options. Below are six different ways to represent the numbers “1”, “2”, and “3”, each mapping them to a different aesthetic.\n\n\n\n\n\n\n\n\nLet’s take a concrete example. Eichstaedt et al. (2015) collected Twitters posts from 935 U.S. counties, and counted the number of words related to positive emotions. This emotional measure could then be connected with known demographic measures of each county, such as race and average income.\n\nhead(twitter_counties)\n\n#&gt; # A tibble: 6 × 5\n#&gt;   county   state income posEmotions maj                        \n#&gt;   &lt;chr&gt;    &lt;chr&gt;  &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;                      \n#&gt; 1 Autauga  AL     55165     0.00549 Majority Non-Black/Hispanic\n#&gt; 2 Baldwin  AL     50006     0.00670 Majority Non-Black/Hispanic\n#&gt; 3 Blount   AL     43450     0.00336 Majority Non-Black/Hispanic\n#&gt; 4 Butler   AL     29769     0.00407 Majority Non-Black/Hispanic\n#&gt; 5 Calhoun  AL     38473     0.00448 Majority Non-Black/Hispanic\n#&gt; 6 Chambers AL     30546     0.00523 Majority Non-Black/Hispanic\n\n\nThere are many ways to present this information graphically. Each choice emphasizes a different aspect of the data, and tells a different story.\nIn the following visualization,\n\n\nincome is mapped to the “x” position aesthetic\n\nposEmotions (positive emotions) is mapped to the “y” position aesthetic\n\nmaj (racial majority) is mapped to the color aesthetic\n\n\nlibrary(ggborderline) # for making the lines pop\n\ntwitter_counties |&gt; \n  ggplot(\n    aes(income, posEmotions, \n        # rearrange the categorical variable so that the order \n        # in the legend matches the order in the plot\n        color = factor(\n          maj, \n          levels = c(\"Majority Non-Black/Hispanic\", \n                     \"Majority Hispanic\", \n                     \"Majority Black\")\n          )\n        )\n    ) +\n    # scatterplot\n    geom_point(alpha = .5, \n               # draw sample such that \"Majority Non-Black/Hispanic\" \n               # points don't overpower the others\n               data = twitter_counties |&gt; \n                 group_by(maj) |&gt; \n                 slice_sample(n = 100)) +\n    # loess regression\n    stat_smooth(\n      # borders that match the lines, but are slightly darker\n      aes(bordercolor = after_scale(colorspace::darken(color))), \n      se = FALSE, geom = \"borderline\", \n      linewidth = 1, lineend = \"square\"\n      ) +\n    theme_bw() +\n    # nicer color palette\n    scale_color_brewer(\n      palette = \"Paired\", \n      direction = -1\n      ) +\n    # proper formatting for income\n    scale_x_continuous(labels=scales::dollar_format()) +\n    labs(\n      x = \"County Average Income\",\n      y = \"Positive Emotional Words in Twitter Posts\\n(proportion of total words)\",\n      color = \"\"\n      )\n\n\n\n\n\n\n\nThis is the most intuitive way to organize the three variables. By mapping income to the x axis, we lightly suggest that it is the cause of whatever is happening on the y axis—in this case positive emotion. The idea that higher income causes positive emotion is intuitive—any people believe they would be happier with a higher income. People accustomed to languages that are written left-to-right, like English, will tend to think about what happens as they move left to right on the graph. Three LOESS regression lines encourage the viewer to compare the slopes of the three color groups, which they will go through from top to bottom:\n\nIn counties without a Black or Hispanic majority, greater income means more positive emotion, up to about $60,000 a year, when the line starts flattening out.\nIn counties with a majority Hispanic population, greater income means dramatically more positive emotion, on average.\nIn counties with a majority Black population, greater income doesn’t seem to make much of a difference.\n\nBut just because this scheme is the most intuitive does not mean it is the best one.\nThe next visualization shows the same data but tells a different story. This one also has x, y, and color, but they are mapped to the variables differently:\n\n\nincome is binned and mapped to the “x” position aesthetic\n\nmaj (racial majority) is mapped to the “y” position aesthetic\n\nposEmotions (positive emotions) is mapped to the “fill” color aesthetic\n\n\ntwitter_counties |&gt; \n  # hand-made bins (note the inconsistent bin width to \n  # give more space to the center of the distribution) \n  mutate(\n    income = factor( \n      case_when(\n        income &gt; 100000 ~ \"$100,000+\",\n        income &gt; 80000 ~ \"$80,000-$100,000\",\n        income &gt; 60000 ~ \"$60,000-$80,000\",\n        income &gt; 50000 ~ \"$50,000-$60,00\",\n        income &gt; 40000 ~ \"$40,000-$50,00\",\n        .default = \"$20,000-$40,00\"),\n      levels = c(\"$20,000-$40,00\", \n                 \"$40,000-$50,00\",\n                 \"$50,000-$60,00\", \n                 \"$60,000-$80,000\",\n                 \"$80,000-$100,000\",\n                 \"$100,000+\")\n      )\n    ) |&gt; \n  # aggregate by the new bins\n  group_by(income, maj) |&gt; \n  summarise(\n    posEmotions = mean(posEmotions, \n                       na.rm = TRUE)\n    ) |&gt; \n  # plot\n  ggplot(aes(income, maj, fill = posEmotions)) +\n    # tiles with a little bit of space in between\n    geom_tile(width = .95, height = .95) +\n    # minimal theme\n    theme_minimal() +\n    # color scale to emphasize differences between extremes\n    scale_fill_gradient2(low = \"blue\", \n                         mid = \"green\", \n                         high = \"yellow\", \n                         midpoint = .0055) +\n    labs(\n      x = \"County Average Income\",\n      y = \"\",\n      fill = \"Positive Emotional Words\\nin Twitter Posts\\n(proportion of total words)\"\n      ) +\n    theme(\n      # angles x axis text to fit it all in\n      axis.text.x = element_text(angle = 30, hjust = 1), \n      axis.title = element_text(size = 8),\n      legend.title = element_text(size = 8)\n      ) +\n    # constrain the tiles to be perfectly square\n    coord_equal()\n\n\n\n\n\n\n\nUsing fill makes it harder to see the slope within each racial group, and easier to see the differences between them. The vertical ordering from most positive emotion (in majority non-Black/Hispanic counties) to least positive emotion (majority Black counties) emphasizes this even more. The blank squares on the grid also make the point that there are no majority Black or Hispanic counties with average incomes above $80,000.\nLet’s try one more way to present these data. In this visualization,\n\n\nincome is mapped to the “y” position aesthetic\n\nposEmotions (positive emotions) is mapped to the “color” aesthetic\n\nmaj (racial majority) is mapped to the “x” position aesthetic\n\n\nset.seed(2023)\ntwitter_counties |&gt; \n  ggplot(aes(maj, income, fill = posEmotions)) +\n    # sina plot\n    ggbeeswarm::geom_quasirandom(\n      aes(color = after_scale(colorspace::darken(fill, .3))), \n      alpha = .5, method = \"pseudorandom\", \n      shape = 21, varwidth = TRUE\n      ) +\n    # color scheme which maximizes the visibility of different \n    # values among the crowd (this is a losing battle)\n    scale_fill_gradient2(low = \"red\", \n                         mid = \"white\", \n                         high = \"blue\", \n                         midpoint = .005) +\n    # unintrusive theme\n    theme_bw() +\n    # log scale, and proper formatting for income\n    scale_y_continuous(\n      labels=scales::dollar_format(), \n      trans = \"log10\"\n      ) +\n    labs(\n      x = \" \",\n      y = \"County Average Income\",\n      fill = \"Positive Emotional Words\\nin Twitter Posts\\n(proportion of total words)\"\n      ) +\n    theme(axis.text.x = element_text(angle = 30, hjust = 1))\n\n\n\n\n\n\n\nThis is a sina plot (also known as a beeswarm plot), in which point clouds are arranged by a continuous variable on one axis, a categorical variable on the other axis, and spread out in proportion to their density along the spaces in between the categories.\nSina plots are a good way to compare distributions of different groups (they are almost always more informative than box plots or violin plots); this plot emphasizes that counties with a majority Black population tend to have relatively low average incomes. The other story that this plot tells is the uneven sizes of the three groups—by separating out the points in each group, this visualization emphasizes the fact that there are very few US counties with majority Black or Hispanic population.\nThis plot makes it very difficult to learn anything about positive emotion in Twitter posts. The colors themselves give some guidance: the white in the middle of the scale suggests that it represents some sort of zero point—a “normal” amount of positive emotion. Nevertheless, the viewer will have to squint in order to notice the trend for higher income counties to be happier. The emotional variable is not adding much to this visualization.\nIn this chapter we have seen how choices about which aesthetics to map to variables make a big difference in the way a data visualization is interpreted. Three visualization of the same data can emphasize tell very different stories.\n\nPress the “View Source” button below to see the hidden code blocks in this chapter.\n\n\n\n\n\nEichstaedt, J. C., Schwartz, H. A., Kern, M. L., Park, G., Labarthe, D. R., Merchant, R. M., Jha, S., Agrawal, M., Dziurzynski, L. A., Sap, M., Weeg, C., Larson, E. E., Ungar, L. H., & Seligman, M. E. P. (2015). Psychological language on twitter predicts county-level heart disease mortality. Psychological Science, 26(2), 159–169. https://doi.org/10.1177/0956797614557867",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Why Aesthetic Choices are Important</span>"
    ]
  },
  {
    "objectID": "telling-a-story.html",
    "href": "telling-a-story.html",
    "title": "4  Don’t Distract From the Story",
    "section": "",
    "text": "4.1 A Small, Compelling Story is Better Than a Big, Confusing One\nSchwartz et al. (2013) collected 15.4 million Facebook status updates from participants who had filled out a variety of questionnaires on the My Personality application (discussed in Chapter 2). They analyzed the frequencies of millions of words, phrases, and topics as they correlate with gender, age, and personality traits of the author. The resulting paper focused on methodology, but Schwartz et al. (2013) nevertheless understood the importance of telling a good story. Here is their figure 5B:\nThis is a beautiful data visualization. The story it tells is so clear and simple that it doesn’t need a caption: Older people use “I” less and “we” more. The unstated implication is either that people get less individualistic with age, or that the young people of today are self-centered. Both are excellent stories.\nHow did Schwartz et al. achieve such a clear story? Let’s take a closer look at some of their choices:\nFirst, out of millions of words, phrases, and topics in their analysis, they chose to focus this visualization on only two. This is the first step of story-telling with data: remove distractions. A small, compelling story is better than a big, confusing one.\nSecond, they chose not to show the data points themselves, but to represent the overall trends with regression lines. This is a major sacrifice, since it makes the graph much less informative—any good scientist will wonder about the distributions surrounding these lines: How rare are community-oriented 20-year-olds? What about self-centered 60-year-olds? Nevertheless, Schwartz et al. decided that including a scatter plot behind the lines would make the graph too confusing to look at, and distract from the main story.\nThird, they chose to use bendy LOESS regression lines, even though the main analysis of the paper was conducted with linear regression. This was a great choice because it makes the story more convincing. The fact that even LOESS lines show near-linear trends is impressive. Even though there are no data points to be seen, those steady lines give the impression that the underlying data are reliable. Also, the the LOESS lines give the viewer the opportunity to notice nuances in the story without distracting from the big picture (it is fascinating that “we” reaches it’s all-time low around the time most people move out of their parents’ house, and not before).\nLastly, let’s take a look at the y axis: What is “Standardized Frequency”? We have an intuitive idea that higher means using the word more and lower means using it less. But this intuitive simplicity did not come easily—it had to be carefully constructed by the authors of the paper. Actually, “Standardized Frequency” is calculated using this formula:\nDon’t understand any of this? That’s OK. We’ll cover methods of standardizing word frequencies in Chapter 16. For now, the point is this: Sometimes you have to do something complicated to make something simple. If Schwartz et al. had not performed them, “I” would likely be much higher frequency than “we” at all ages, and the story, which requires the viewer to focus on the slopes of the lines, would be much harder to appreciate.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Don't Distract From the Story</span>"
    ]
  },
  {
    "objectID": "telling-a-story.html#sec-simplify-the-story",
    "href": "telling-a-story.html#sec-simplify-the-story",
    "title": "4  Don’t Distract From the Story",
    "section": "",
    "text": "Figure from Schwartz et al. (2013)\n\n\n\n\n\n\n\n\n\nFigure from Schwartz et al. (2013)",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Don't Distract From the Story</span>"
    ]
  },
  {
    "objectID": "telling-a-story.html#engineer-your-aesthetics",
    "href": "telling-a-story.html#engineer-your-aesthetics",
    "title": "4  Don’t Distract From the Story",
    "section": "\n4.2 Engineer Your Aesthetics",
    "text": "4.2 Engineer Your Aesthetics\nWe have just seen in Schwartz et al.’s beautiful data visualization (Section 4.1) that choosing to map the frequency variable to the y position aesthetic was not enough. In order to make the story clear, they carefully engineered the scale on which they measured frequency. In their case, this required some complicated standardization tailored to the particular statistics underlying their data. Often though, the solution is much more straightforward.\nThe remainder of this chapter outlines some common ways to engineer aesthetics that can help make a story clear and intuitive.\n\n4.2.1 Nonlinear Axes\nOften a simple log scale is enough to reveal a much clearer presentation of data. The following graph uses data from Buechel et al. (2018), in which participants read news stories, rated their own empathy and distress after reading them, and then described their thoughts in their own words.\nThis visualization tells a story about the most and least common words in participant’s responses.\n\nlibrary(ggrepel)\nlibrary(patchwork)\n\ndistressed_texts_binary_ordered &lt;- distressed_texts_binary |&gt; \n  # ratio of distressed frequency to non-distressed frequency\n  mutate(distressed_freq_ratio = distressed_count/nondistressed_count) |&gt; \n  # refactor in descending order\n  arrange(distressed_freq_ratio) |&gt; \n  mutate(word = factor(word, levels = word))\n\nset.seed(2023)\nbadplot1 &lt;- distressed_texts_binary_ordered |&gt; \n  ggplot(aes(word, distressed_freq_ratio, label = word)) +\n    geom_point(color = \"blue3\", \n               size = 1, \n               alpha = .2) +\n    geom_text_repel(\n      size = 3, \n      data = distressed_texts_binary_ordered |&gt; \n        filter(distressed_freq_ratio &gt; 1.75)) +\n    labs(title = \"Linear Scale\",\n         x = \"Words, from least to most distressed\",\n         y = \"Distressed frequency / non-distressed frequency\") +\n    scale_y_continuous(breaks = seq(-.5, 2, .5)) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(color = \"red3\", hjust = 1, size = 20),\n      panel.grid.major.x = element_blank(),\n      axis.text.x = element_blank(),\n      axis.ticks.x = element_blank(),\n      axis.title.x = element_text(size = 15)\n      )\n\nset.seed(2023)\ngoodplot1 &lt;- distressed_texts_binary_ordered |&gt; \n  ggplot(aes(word, distressed_freq_ratio, \n             label = word, \n             color = distressed_freq_ratio &lt; 1)\n         ) +\n    geom_point(size = 1, alpha = .2) +\n    geom_text_repel(\n      size = 3, \n      data = distressed_texts_binary_ordered |&gt; \n        filter(distressed_freq_ratio &gt; 1.75 | distressed_freq_ratio &lt; 1/2),\n      max.overlaps = 20) +\n    geom_hline(linetype = 2, yintercept = 1) +\n    labs(title = \"Log Scale\",\n         x = \"Words, from least to most distressed\",\n         y = \"Distressed frequency / non-distressed frequency\") +\n    scale_y_continuous(\n      breaks = c(2^(-6:6)), \n      trans = \"log2\", \n      labels = ~MASS::fractions(.x)\n      ) +\n    guides(color = \"none\") +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(color = \"green3\", hjust = 1, size = 20),\n      panel.grid.major.x = element_blank(),\n      axis.text.x = element_blank(),\n      axis.ticks.x = element_blank(),\n      axis.title.x = element_text(size = 15)\n      )\n\ngoodplot1 + badplot1\n\n\n\n\n\n\n\nWhen plotting ratios, it is almost always a good idea to use a log scale (left). This way, the viewer can compare the largest and the smallest relative values. Without the log scale (right), the smallest values are squished into oblivion.\n\n4.2.2 Ordering Categorical Variables\nTake another look at the graph labeled “Log Scale” above, and notice the ordering along the x axis. Words, on their own, are an unordered categorical variable. Nevertheless, in the context of a story, even unordered variables have an order. Ordering the categorical variable along the continuous variable of interest calls attention to the distribution and removes confusion.\n\nlibrary(ggrepel)\nlibrary(patchwork)\n\nset.seed(2023)\nbadplot2 &lt;- distressed_texts_binary |&gt; \n  # ratio of distressed frequency to non-distressed frequency\n  mutate(distressed_freq_ratio = distressed_count/nondistressed_count) |&gt; \n  ggplot(\n    aes(\n      word, distressed_freq_ratio, \n      label = word, \n      color = distressed_freq_ratio &lt; 1\n      )\n    ) +\n    geom_point(size = 1, alpha = .7) +\n    geom_text_repel(\n      size = 3, \n      data = distressed_texts_binary_ordered |&gt; \n        filter(distressed_freq_ratio &gt; 1.75 | distressed_freq_ratio &lt; .5),\n      max.overlaps = 20\n      ) +\n    geom_hline(linetype = 2, yintercept = 1) +\n    labs(\n      title = \"Unordered\",\n      x = \"Words\",\n      y = \"Distressed frequency / non-distressed frequency\"\n      ) +\n    scale_y_continuous(\n      breaks = c(2^(-6:6)), \n      trans = \"log2\", \n      labels = ~MASS::fractions(.x)\n      ) +\n    guides(color = \"none\") +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(color = \"red3\", hjust = 1, size = 20),\n      panel.grid.major.x = element_blank(),\n      axis.text.x = element_blank(),\n      axis.ticks.x = element_blank(),\n      axis.title.x = element_text(size = 15)\n      )\n\ngoodplot2 &lt;- goodplot1 + labs(title = \"Ordered\")\n\ngoodplot2 + badplot2\n\n\n\n\n\n\n\n\n4.2.3 Color Scales\nWe have already seen how a data visualizer can clarify a story by spatially ordering a categorical variable. A carefully tailored color scale can be an even more powerful communicator than an x or y axis. This is because color, even though it is usually treated as a single aesthetic, actually has many dimensions: luminosity, saturation, redness, blueness, etc.\nThe first step in choosing a color scale for any variable is to consider whether the thing being measured is diverging, sequential, or qualitative.\nDiverging scales measure something with a neutral center. This center is often represented by zero, but beware! Sometimes a neutral center is 4 on a seven point Likert scale (see “Agreement” in the figure below). When dealing with fractions, the neutral center is 1 (see “Frequency Ratio” in the figure below).\nWhen applying diverging scales, keep in mind any associations people might have with the colors involved. For example, red should always be bad and blue/green good (see the plot in Section 4.2.2, in which red = distress = bad).\n\n\n\n\n\n\n\n\nSequential scales measure something that has an order, but no neutral center. Often, one side of the scale is at zero, so that the scale goes from nothing to something. In these cases, the appropriate color scale will represent amount with luminosity, where zero is the lightest (see “Frequency” and “Anxiety” in the figure below). This way, the lower amounts have lower contrast against the white background of the plot (if using a non-white plot background, make sure the low end of the scale matches).\nSometimes sequential scales do not measure amount, as in “Weekdays” in the figure below. Weekdays have an order—rom the beginning of the week to the end—ut it would be a mistake to use a scale with one side blending in to the background and the other intensely dark, since that would suggest that Thursday is somehow ‘more’ than Wednesday. Likewise, there is no neutral center (there’s nothing neutral about Wednesday). In such case, the scale should go from one noticeable color to another. The chart below uses a palette reminiscent of sunset to give the impression of time passing.\n\n\n\n\n\n\n\n\nTo emphasize the point about weekdays, consider the following two versions of the same graph (data taken from Reddit users associated with subreddits for 9 US cities):\n\n\n\n\n\n\n\n\nBoth versions are confusing to look at (these data might be better represented as a heat map), but the one with the sequential color scale is much better. Whereas the qualitative scale requires the viewer to look constantly back and forth between the legend and the plot, the sequential scale maps to an intuitive understanding of beginning-of-week vs. end-of-week.\nWe have seen that many seemingly unordered variables should be ordered in the context of a story. Nevertheless, some variables are truly qualitative. In these cases, the color scale should maximize contrast between neighboring values without accidentally suggesting an order. For example, “Parts of Speech” in the figure below are all soft pastel colors. If some were darker or more saturated, it might suggest that there is an important difference between the groups.\nAgain, keep in mind any associations people might have with colors involved. For example, countries should be represented by colors that appear in their flags. This is of course sometimes difficult—all of the countries in the figure below have red in their flag, and all but China have blue and white. Nevertheless, try your best. The Wikipedia page for national colors was helpful in making the chart below.\n\n\n\n\n\n\n\n\n\n4.2.4 Accent colors\nBecause color has many dimensions, it can sometimes be used to represent two scales at the same time. One common tactic is to use luminosity or saturation to emphasize certain values and de-emphasize others. Below, we have redrawn the frequency ratio plot from earlier in this chapter (Section 4.2.1) to tell a story about two words in particular. By using accent colors to emphasize the two words of interest, we remove distractors while maintaining the broader context of the story.1\n\nlibrary(ggnewscale)\n\nset.seed(2023)\ndistressed_texts_binary_ordered |&gt; \n  ggplot(aes(word, distressed_freq_ratio, \n             label = word, \n             color = distressed_freq_ratio &lt; 1)) +\n    geom_point() +\n    geom_hline(linetype = 2, yintercept = 1) +\n    scale_color_discrete(\n      type = colorspace::lighten(c(\"#F8766D\", \"#00BFC4\"), .7)) +\n    geom_text_repel(\n      size = 3, \n      data = distressed_texts_binary_ordered |&gt; \n        filter(distressed_freq_ratio &gt; 3.6 \n               | distressed_freq_ratio &lt; .2),\n      max.overlaps = 20) +\n    guides(color = \"none\") +\n    new_scale_color() +\n    geom_point(\n      aes(color = distressed_freq_ratio &lt; 1), \n      size = 3,\n      data = distressed_texts_binary_ordered |&gt; \n        filter(word %in% c(\"i\", \"we\"))) +\n    geom_text_repel(\n      size = 4, \n      data = distressed_texts_binary_ordered |&gt; \n        filter(word %in% c(\"i\", \"we\")),\n      max.overlaps = 20) +\n    labs(title = '\"We\" is a Sign of Distress',\n         x = \"Words, from least to most distressed\",\n         y = \"Distressed frequency / non-distressed frequency\") +\n    scale_y_continuous(breaks = c(2^(-6:6)), \n                       trans = \"log2\", \n                       labels = ~MASS::fractions(.x)) +\n    guides(color = \"none\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = .5, size = 18),\n          panel.grid.major.x = element_blank(),\n          axis.text.x = element_blank(),\n          axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\n4.2.5 Aspect Ratios\nSimchon et al. (2021) investigated whether COVID-19 concern among New Yorkers resulted in higher or lower levels of certainty, as expressed in language on Twitter. Their story: Higher concern leads to greater expressions of certainty, since people use certainty as a coping mechanism. Here is their Figure 3, reproduced in three different aspect ratios:\n\nlibrary(patchwork)\n\nplot &lt;- covid_concern |&gt; \n  mutate(z_cert = as.numeric(scale(cert)),\n         z_concern = as.numeric(scale(ny_net_concern))) |&gt; \n  pivot_longer(cols = c(z_cert, z_concern)) |&gt; \n  mutate(name = if_else(name==\"z_cert\", \"Certainty\", \"NY Net Concern\")) |&gt; \n  ggplot() +\n    geom_smooth(aes(date, value, \n                    linetype = name), \n                se = FALSE, method = \"loess\", \n                color = \"black\", span = 1/3, \n                method.args = list(degree=1)) +\n    ylab(\"Z-score\") + \n    xlab(\"Date\") + \n    scale_colour_grey() +\n    cowplot::theme_cowplot() + \n    labs(linetype =c(\"\"))\n\nplot_squished &lt;- plot + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n(plot_squished + plot_squished +  \n    plot_layout(widths = c(1, 2))) / \n  plot + plot_layout(heights = c(2, 1)) +\n  plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\n\nWhich aspect ratio is the right one? A good aspect ratio is one that communicates the meaning of the variables in question. Since months are spread out over time (by definition), it makes sense to make the x-axis longer so that viewers have the feeling of time passing as they scan it. But it shouldn’t be too wide, since the aspect ratio should also emphasize important differences in position (here, the positive slope of both lines). Something in between B and C seems appropriate. Indeed, this is the figure printed in the final paper:\n\n\nAs always, press the “View Source” button below to see the hidden code blocks in this chapter.\n\n\n\n\n\nBuechel, S., Buffone, A., Slaff, B., Ungar, L. H., & Sedoc, J. (2018). Modeling empathy and distress in reaction to news stories. CoRR, abs/1808.10399. http://arxiv.org/abs/1808.10399\n\n\nSchwartz, H. A., Eichstaedt, J. C., Kern, M. L., Dziurzynski, L., Ramones, S. M., Agrawal, M., Shah, A., Kosinski, M., Stillwell, D., Seligman, M. E. P., & Ungar, L. H. (2013). Personality, gender, and age in the language of social media: The open-vocabulary approach. PLOS ONE, 8(9), 1–16. https://doi.org/10.1371/journal.pone.0073791\n\n\nSimchon, A., Turkin, C., Svoray, T., Kloog, I., Dorman, M., & Gilead, M. (2021). Beyond doubt in a dangerous world: The effect of existential threats on the certitude of societal discourse. Journal of Experimental Social Psychology, 97, 104221. https://doi.org/https://doi.org/10.1016/j.jesp.2021.104221",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Don't Distract From the Story</span>"
    ]
  },
  {
    "objectID": "telling-a-story.html#footnotes",
    "href": "telling-a-story.html#footnotes",
    "title": "4  Don’t Distract From the Story",
    "section": "",
    "text": "In the example here, we used ggnewscale to control the accented an non-accented color scales separately. If you’d like a simpler method for accenting values without using layered geoms, we recommend the gghighlight package.↩︎",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Don't Distract From the Story</span>"
    ]
  },
  {
    "objectID": "word-viz.html",
    "href": "word-viz.html",
    "title": "5  Visualizing Distributions of Words",
    "section": "",
    "text": "5.1 Frequency/Frequency Plots\nA scatterplot is the most obvious choice for visualizing the relationship between two variables. For text data, this approach is commonly associated with the scattertext Python library (Kessler, 2017), but the same effect is easily accomplished in ggplot2.\nSince we are comparing frequency in one group to frequency in another, we can put each frequency variable on an axis. We will call this a frequency/frequency plot, or F/F plot. To emphasize words that are more frequent in one group than in the other, we represent the ratio between the two frequencies with a diverging color scale.\nlibrary(ggrepel)\n\nset.seed(2023)\ndistressed_texts_binary |&gt; \n  ggplot(aes(nondistressed_count, distressed_count, \n             label = word,\n             color = distressed_freq_ratio)) +\n    geom_point() +\n    geom_text_repel(max.overlaps = 20) +\n    scale_x_continuous(trans = \"log10\", n.breaks = 5) +\n    scale_y_continuous(trans = \"log10\", n.breaks = 6) +\n    scale_color_gradient2(\n      low = \"blue4\", \n      mid = \"#E2E2E2\", \n      high = \"red4\", \n      trans = \"log2\", # log scale for ratios\n      limits = c(.25, 4), \n      breaks = c(.25, 1, 4),\n      labels = c(\"Characteristically\\nNon-Distressed\",\n                 \"Equal Proportion\",\n                 \"Characteristically\\nDistressed\")\n      ) +\n    labs(\n      title = \"Stop Words in Distressed and Non-Distressed Texts\",\n      x = \"Occurrences in Non-Distressed Texts\",\n      y = \"Occurrences in Distressed Texts\",\n      color = \"\"\n      ) +\n    coord_fixed() +\n    theme_minimal()\nThis plot has the advantage of showing not just which words are characteristic of one group or the other, but also which are more common in both.\nTo allow viewers to explore these patterns in greater detail, we can make the plot interactive using the ggiraph package. Hover over the points to show the words they represent!\nlibrary(ggiraph, verbose = FALSE)\nlibrary(ggrepel)\n\nset.seed(2023)\np &lt;- distressed_texts_binary |&gt; \n  ggplot(aes(nondistressed_count, distressed_count, \n             label = word,\n             color = distressed_freq_ratio,\n             tooltip = word, \n             data_id = word # aesthetics for interactivity\n             )) +\n    geom_point_interactive() +\n    geom_text_repel_interactive() +\n    scale_x_continuous(trans = \"log10\", n.breaks = 5) +\n    scale_y_continuous(trans = \"log10\", n.breaks = 6) +\n    scale_color_gradient2(\n      low = \"blue4\", \n      mid = \"#E2E2E2\", \n      high = \"red4\", \n      trans = \"log2\", # log scale for ratios\n      limits = c(.25, 4), \n      breaks = c(.25, 1, 4),\n      labels = c(\"Characteristically\\nNon-Distressed\",\n                 \"Equal Proportion\",\n                 \"Characteristically\\nDistressed\")\n    ) +\n    labs(\n      title = \"Stop Words in Distressed and Non-Distressed Texts\",\n      x = \"Occurrences in Non-Distressed Texts\",\n      y = \"Occurrences in Distressed Texts\",\n      color = \"\"\n    ) +\n    # fixed coordinates since x and y use the same units\n    coord_fixed() + \n    theme_minimal()\n\ngirafe_options(\n  girafe(ggobj = p),\n  opts_tooltip(css = \"font-family:sans-serif;font-size:1em;color:Black;\")\n  )",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualizing Distributions of Words</span>"
    ]
  },
  {
    "objectID": "word-viz.html#sec-freq-freq",
    "href": "word-viz.html#sec-freq-freq",
    "title": "5  Visualizing Distributions of Words",
    "section": "",
    "text": "5.1.1 Rotated Frequency/Frequency Plots\nA disadvantage of simple F/F plots: When people see a scatterplot, they think, “Aha! A correlation!” Any two samples of text in the same language will have highly correlated word frequencies. This boring story about the correlation is distracting from the more interesting stories about words that are especially characteristic of one group or another. This distraction can be removed by “rotating” the axes. Mathematically, we achieve this by plotting the average of the two frequencies (nondistressed_count + distressed_count)/2 on the y axis, and the ratio between the two frequencies on the x axis. The result is a much more intuitive plot with a clear binary comparison. Remember, sometimes you have to do something complicated to make something simple (Section 4.1).\n\nlibrary(ggiraph, verbose = FALSE)\nlibrary(ggrepel)\n\nset.seed(2023)\np1 &lt;- distressed_texts_binary |&gt; \n  mutate(common = (nondistressed_count + distressed_count)/2) |&gt; \n  ggplot(aes(distressed_freq_ratio, common, \n             label = word,\n             color = distressed_freq_ratio,\n             tooltip = word, data_id = word # aesthetics for interactivity\n             )) +\n    geom_point_interactive() +\n    geom_text_repel_interactive() +\n    scale_y_continuous(trans = \"log2\", breaks = ~.x,\n                       minor_breaks = ~2^(seq(0,log2(.x[2]))),\n                       labels = c(\"Rare\", \"Common\")) +   \n    scale_x_continuous(trans = \"log2\", limits = c(1/6,6),\n                       breaks = c(.25, 1, 4),\n                       labels = c(\"Characteristically\\nNon-Distressed\",\n                                  \"Equal Proportion\",\n                                  \"Characteristically\\nDistressed\")) +\n    scale_color_gradient2(low = \"blue4\", \n                          mid = \"#E2E2E2\", \n                          high = \"red4\", \n                          trans = \"log2\", # log scale for ratios\n                          guide = \"none\") +\n    labs(title = \"Stop Words in Distressed and Non-Distressed Texts\",\n         x = \"\",\n         y = \"Average Frequency\",\n         color = \"\") +\n    theme_minimal()\n\ngirafe_options(\n  girafe(ggobj = p1),\n  opts_tooltip(css = \"font-family:sans-serif;font-size:1em;color:Black;\")\n  )\n\n\n\n\n\nBecause we love these rotated F/F plots so much, we couldn’t help showing off one more example, this time with data from the Corpus of Contemporary American English (Davies, 2009):\n\n# get frequency data\nhttr::GET(\"https://www.wordfrequency.info/files/genres_sample.xls\",\n          httr::write_disk(tf &lt;- tempfile(fileext = \".xls\")))\nword_freqs &lt;- readxl::read_excel(tf) |&gt; \n  select(lemma, ACADEMIC, SPOKEN)\n\n\np2 &lt;- word_freqs |&gt; \n  filter(ACADEMIC != 0, SPOKEN != 0) |&gt; \n  # generate tooltip text\n  mutate(rep = if_else(ACADEMIC/SPOKEN &gt; 1, \n                       \"more common in academic texts\",\n                       \"more common in spoken texts\"),\n         mult = if_else(ACADEMIC/SPOKEN &gt; 1, \n                        as.character(round(ACADEMIC/SPOKEN, 2)),\n                        as.character(round(SPOKEN/ACADEMIC, 2))),\n         tooltip = paste0(\"&lt;b&gt;\",lemma, \"&lt;/b&gt;\", \"&lt;br/&gt;\", \n                          mult, \"x \", rep)) |&gt; \n  ggplot(aes(ACADEMIC/SPOKEN, (ACADEMIC + SPOKEN)/2, \n             label = lemma,\n             color = ACADEMIC/SPOKEN,\n             tooltip = tooltip, \n             data_id = lemma # aesthetics for interactivity\n             )) +\n    geom_point_interactive() +\n    scale_x_continuous(trans = \"log2\", \n                       breaks = c(1/100, 1, 100),\n                          labels = c(\"Characteristically\\nSpoken\",\n                                     \"Equal Proportion\",\n                                     \"Characteristically\\nAcademic\")) +\n    scale_y_continuous(trans = \"log2\", \n                       breaks = ~.x, \n                       minor_breaks = ~2^(seq(0, log2(.x[2]))),\n                       labels = c(\"Rare\", \"Common\")) +\n    scale_color_gradientn(limits = c(1/740, 740),\n                          colors = c(\"#023903\", \n                                     \"#318232\",\n                                     \"#E2E2E2\", \n                                     \"#9B59A7\",\n                                     \"#492050\"), \n                          trans = \"log2\", # log scale for ratios\n                          guide = \"none\") +\n    labs(title = \"Academic vs. Spoken English\",\n         x = \"\", y = \"\",\n         color = \"\") +\n    theme_minimal()\n\ngirafe_options(\n  girafe(ggobj = p2),\n  opts_tooltip(css = \"font-family:sans-serif;font-size:1em;color:Black;\")\n  )   \n\n\n\n\n\nThis is a particularly good visualization, because it tells an interesting story. The story it tells is: “Academic English is very different from spoken English.” The wide aspect ratio and the tooltip text with “#x more common in spoken/academic texts” especially emphasize this point. The F/F plot is an appropriate visualization method because this story pertains to the full distribution of words—look at how many are more than 10 times more common in one or the other!\n\n5.1.2 Advanced Frequency/Frequency Plots\nFor more information about frequency/frequency plots and other related plot types incorporating various statistics, see the scattertext tutorial (Kessler, 2017). While the tutorial is intended for the scattertext library in Python, almost all examples can be produced in R with ggplot2 and ggiraph. Search bars and other types of responsive interactivity can be accomplished with shiny.\n\n\n\n\n\n\n\nAdvantages of Frequency/Frequency Plots\n\n\n\n\n\nSpatial Mapping: F/F plots use axes, which make it easy to compare values of different words.\n\nReadability: The layout of F/F plots is easy to interpret, especially when rotated and properly labeled.\n\nFull Picture: F/F plots convey the full shape of the frequency distribution, rather than singling out words most characteristic of one side or the other. This can be useful when the distribution itself is interesting.\n\n\n\n\n\n\n\n\n\nDisadvantages of Frequency/Frequency Plots\n\n\n\n\n\nInteractivity Required: F/F plots require interactivity to be maximally informative.\n\nMessy: Without interactivity, labels can be messy and confusing.\n\nImplies that Correlation is Interesting: F/F plots may imply that the point of the graph is to show the correlation between frequencies in the two texts. Rotating the axes mostly solves this problem.\n\nVague: By showing many words at the same time, F/F plots make it difficult to focus in on particular stories (unless the story is about the distribution itself).",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualizing Distributions of Words</span>"
    ]
  },
  {
    "objectID": "word-viz.html#sec-word-clouds",
    "href": "word-viz.html#sec-word-clouds",
    "title": "5  Visualizing Distributions of Words",
    "section": "\n5.2 Word Clouds",
    "text": "5.2 Word Clouds\nWord clouds are commonly used for purposes like:\n\nSummarizing text using word frequencies\nDecorating placemats at cheap restaurants\nComparing word usage in two groups of texts\nCorrelating word usage with a construct of interest\n\nWord clouds are not a good tool for summarizing text. They are a perfectly fine tool for kitschy placemats, but those are beyond the scope of this textbook. In the world of data science, there are only two legitimate uses for word clouds: comparing words across two groups of texts, and correlating word frequencies with a construct of interest. Even these legitimate uses break a fundamental rule of data visualization, since by showing many words at the same time they are telling many stories at the same time, each distracting from the others. Nevertheless, analyses often include so many words (or other units of text) that producing a visualization for each one is unfeasible, and a summary graphic is necessary.\n\n5.2.1 Word Clouds for Comparing Two Groups\nWord clouds generally have three aesthetics: label, color, and size:\n\n\nlabel will always be the text of the words.\n\ncolor is appropriate for representing relative frequency, since it has a neutral center (where the frequencies in both groups are the same and distressed_freq_ratio = 1. Such a neutral center calls for a diverging color scale (Section 4.2.3). Because we are representing the ratio of two frequencies, it is appropriate to use a log scale (see Section 4.2.1). This will make the scale symmetrical for values above and below the neutral center.\n\nsize is technically unnecessary, since the diverging color scale already represents both the valence and the magnitude of the relative frequency. In practice though, we are generally most interested in the largest differences. Size is therefore used to emphasize words with greater a discrepancy between the groups. This magnitude value is calculated as abs(log2(distressed_freq_ratio)).\n\n\ndistressed_texts_binary &lt;- distressed_texts_binary |&gt; \n  mutate(freq_ratio_log_magnitude = abs(log2(distressed_freq_ratio)))\n\nhead(distressed_texts_binary)\n\n#&gt; # A tibble: 6 × 5\n#&gt;   word  distressed_count nondistressed_count distressed_freq_ratio\n#&gt;   &lt;chr&gt;            &lt;int&gt;               &lt;int&gt;                 &lt;dbl&gt;\n#&gt; 1 the               3297                2985                 1.10 \n#&gt; 2 to                2556                2415                 1.06 \n#&gt; 3 and               2125                1856                 1.14 \n#&gt; 4 of                1592                1416                 1.12 \n#&gt; 5 i                 1587                1874                 0.847\n#&gt; 6 a                 1547                1603                 0.965\n#&gt; # ℹ 1 more variable: freq_ratio_log_magnitude &lt;dbl&gt;\n\n\nFor creating word clouds in R we use the ggwordcloud package, with its geom_text_wordcloud geom:\n\nlibrary(ggwordcloud)\nset.seed(2)\n\ndistressed_texts_binary |&gt; \n  # top 100 highest discrepancy words\n  arrange(desc(freq_ratio_log_magnitude)) |&gt; \n  slice_head(n = 150) |&gt; \n  # plot\n  ggplot(aes(label = word, \n             size = freq_ratio_log_magnitude, \n             color = distressed_freq_ratio)) +\n    geom_text_wordcloud(show.legend = TRUE) + # wordcloud geom\n    scale_radius(range = c(2, 18), guide = \"none\") + # control text size\n    scale_color_gradient2(\n      name = \"Distressed /\\nNon-distressed\\nFrequency\",\n      labels = ~ MASS::fractions(.x), # show legend labels as fractions\n      low = \"blue3\", mid = \"white\", high = \"red3\", # set diverging color scale\n      trans = \"log2\" # log scale\n      ) +\n    theme_void() # blank background\n\n\n\n\n\n\n\nWe can now easily see that the word most representative of non-distressed texts is “interesting”, which is far more representative of one group than any other word in the analysis.\nThe angle_group aesthetic can be used to separate out the words more frequent in distressed texts from those more frequent in non-distressed texts:\n\nset.seed(2)\n\ndistressed_texts_binary |&gt; \n  # top 150 highest discrepancy words\n  arrange(desc(freq_ratio_log_magnitude)) |&gt; \n  slice_head(n = 150) |&gt; \n  # plot\n  ggplot(aes(label = word, \n             size = freq_ratio_log_magnitude, \n             color = distressed_freq_ratio,\n             angle_group = distressed_freq_ratio &gt; 1)) +\n    geom_text_wordcloud(show.legend = TRUE) + # wordcloud geom\n    scale_radius(range = c(2, 18), guide = \"none\") + # control text size\n    scale_color_gradient2(\n      name = \"Distressed /\\nNon-distressed\\nFrequency\",\n      labels = ~ MASS::fractions(.x), # show legend labels as fractions\n      low = \"blue3\", mid = \"grey\", high = \"red3\", # set diverging color scale\n      trans = \"log2\" # log scale\n      ) +\n    theme_void() # blank background\n\n\n\n\n\n\n\nAlternatively, we can specify an original position for each label (as x and y aesthetics) to create multiple clouds:\n\nset.seed(2)\n\ndistressed_texts_binary |&gt; \n  # top 100 highest discrepancy words\n  arrange(desc(freq_ratio_log_magnitude)) |&gt; \n  slice_head(n = 150) |&gt; \n  # plot\n  ggplot(aes(label = word, \n             size = freq_ratio_log_magnitude, \n             color = distressed_freq_ratio,\n             x = distressed_freq_ratio &lt; 1,\n             y = distressed_freq_ratio &gt; 1)) +\n    # wordcloud geom\n    geom_text_wordcloud(show.legend = TRUE) + \n    # control text size\n    scale_radius(range = c(2, 18), guide = \"none\") + \n    scale_color_gradient2(\n      name = \"Distressed /\\nNon-distressed\\nFrequency\",\n      # show legend labels as fractions\n      labels = ~ MASS::fractions(.x), \n      # set diverging color scale\n      low = \"blue3\", mid = \"grey\", high = \"red3\", \n      # log scale\n      trans = \"log2\" \n      ) +\n    theme_void() # blank background\n\n\n\n\n\n\n\n\n5.2.2 Word Clouds for Continuous Variables of Interest\nRecently, some have advocated using correlation coefficients instead of frequency ratios in word clouds. This approach has three advantages:\n\nCorrelation coefficients take variance into account.\nSince correlation coefficients are more commonly used, it is easier to perform significance testing on them. This way we can include only significant results in the visualization.\nUnlike frequency ratios, which always compare two groups, correlation coefficients can be applied to continuous variables of interest.\n\nTo apply this method to the data from Buechel et al. (2018), we can use participants’ continuous distress ratings for each text. We count the occurrences of each word in each text, and measure the correlation between these frequency variables and the corresponding distress ratings. Since the association may be non-linear, we use the Kendall rank correlation. You can see the full calculations by pressing the “View Source” button at the bottom of this page.\nWe can now map the strength of the correlation (i.e. abs(cor)) to size, and use color to show the direction of the correlation.\n\ndistress_cor &lt;- read_csv(\"data/distress_cor.csv\", show_col_types = FALSE)\nhead(distress_cor)\n\n#&gt; # A tibble: 6 × 2\n#&gt;   word      cor\n#&gt;   &lt;chr&gt;   &lt;dbl&gt;\n#&gt; 1 and    0.0983\n#&gt; 2 from   0.0680\n#&gt; 3 how    0.0448\n#&gt; 4 is     0.0422\n#&gt; 5 it    -0.0354\n#&gt; 6 me     0.0473\n\nset.seed(2)\n\ndistress_cor |&gt; \n  arrange(desc(abs(cor))) |&gt; \n  ggplot(aes(label = word, \n             color = cor, \n             size = abs(cor),\n             angle_group = cor &lt; 0)) +\n    geom_text_wordcloud(eccentricity = 1.2, show.legend = TRUE) +\n    scale_radius(range = c(4, 15), guide = \"none\") +\n    labs(caption = \"All correlations passed significance testing at p &lt; .05\") + \n    scale_color_gradient2(\n        name = \"Correlation\\nwith distress\\n(Kendall's τ)\",\n        low = \"blue3\", mid = \"white\", high = \"red3\", # set diverging color scale\n        ) +\n    theme_void()\n\n\n\n\n\n\n\n“Interesting” is still the most highly correlated word, indicating lack of distress, but now we can see that “and” and “we” are highly indicative of distress. The assurance that all correlations passed significance testing makes for a particularly convincing graphic.\n\n5.2.3 Advanced Word Clouds\nFor more information about how word clouds are generated and how to customize them, see Pennec (2023). Be careful though - any customization of your word clouds should be in the service of communicating information effectively.\n\n\n\n\n\n\n\nAdvantages of Word Clouds\n\n\n\n\n\nTo the Point: Word clouds emphasize words most characteristic of the variable of interest.\n\nNo Interactivity Required: Word clouds show many words at once without requiring interactivity.\nLooks Fancy\n\n\n\n\n\n\n\n\n\nDisadvantages of Word Clouds\n\n\n\n\n\nHard to Interpret Proportions: Size and color aesthetics make it extremely difficult to compare values of different words (e.g. Is x twice as blue as y?).\n\nVague: By showing many words at the same time, word clouds make it difficult to focus in on particular stories.\n\n\n\n\nPress the “View Source” button below to see the hidden code blocks in this chapter.\n\n\n\n\n\nBuechel, S., Buffone, A., Slaff, B., Ungar, L. H., & Sedoc, J. (2018). Modeling empathy and distress in reaction to news stories. CoRR, abs/1808.10399. http://arxiv.org/abs/1808.10399\n\n\nDavies, M. (2009). The 385+ million word corpus of contemporary american english (1990―2008+): Design, architecture, and linguistic insights. International Journal of Corpus Linguistics, 14, 159–190. https://www.english-corpora.org//coca/\n\n\nHellman, A. B. (2011). Vocabulary size and depth of word knowledge in adult-onset second language acquisition. International Journal of Applied Linguistics, 21(2), 162–182.\n\n\nKessler, J. S. (2017). Scattertext: A browser-based tool for visualizing how corpora differ. https://github.com/JasonKessler/scattertext\n\n\nPennec, E. le. (2023). Ggwordcloud: A word cloud geom for ggplot2. In lepennec.github.io/ggwordcloud/. https://lepennec.github.io/ggwordcloud/articles/ggwordcloud.html",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Visualizing Distributions of Words</span>"
    ]
  },
  {
    "objectID": "data-viz-resources.html",
    "href": "data-viz-resources.html",
    "title": "6  Additional Resources for Data Visualization",
    "section": "",
    "text": "6.1 General Data Visualization Materials",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Additional Resources for Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-viz-resources.html#general-data-visualization-materials",
    "href": "data-viz-resources.html#general-data-visualization-materials",
    "title": "6  Additional Resources for Data Visualization",
    "section": "",
    "text": "Fundamentals of Data Visualization: A textbook on dataviz methodology, covering topics outlined here in much greater detail\n\nR Graph Gallery: Hundreds of example data visualization with tutorials on how to make them in R\n\nR Graphics Cookbook: A textbook with detailed instructions for creating graphs in R.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Additional Resources for Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-viz-resources.html#advanced-methods",
    "href": "data-viz-resources.html#advanced-methods",
    "title": "6  Additional Resources for Data Visualization",
    "section": "\n6.2 Advanced Methods",
    "text": "6.2 Advanced Methods\n\n\nColoring in R’s blind spot: Tutorial introducing more than 100 palettes now included with base R, as well as functions for manipulating colors (used in multiple visualizations in this chapter)\n\nggnewscale: An intuitive method for incorporating multiple color scales in a single ggplot2 graphic (used in multiple visualizations in this chapter)\n\nggrepel: Tools for avoiding overlapping text labels in ggplot2 (used in multiple visualizations in this chapter)\n\ngganimate: A principled approach to animating with ggplot2\n\n\nggiraph: Dynamic and interactive html graphics for ggplot2\n\n\nggforce: Various advanced shapes, lines, scales, and plot types for ggplot2\n\n\nggborderline Lines that pop in ggplot2 (used in multiple visualizations in this chapter)\n\nrayshader: Cinematic 3D graphics for ggplot2",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Additional Resources for Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-viz-resources.html#packages-for-unusual-plot-types",
    "href": "data-viz-resources.html#packages-for-unusual-plot-types",
    "title": "6  Additional Resources for Data Visualization",
    "section": "\n6.3 Packages for Unusual Plot Types",
    "text": "6.3 Packages for Unusual Plot Types\n\n\nggwordcloud: Word clouds\n\nggbeeswarm: Sina (AKA beeswarm) plots—almost always an improvement on box plots or violin plots\n\nggcorrplot: Correlation matrices\n\nDifferent Ways of Plotting U.S. Map in R: A book chapter comparing eight different methods for plotting map data in R\n\nggridges: Ridgeline plots\n\nggnetwork: Network data\n\nwormsplot: step charts with smooth transitions (in development by one of the authors of this book)\n\nAdditional ggplot2 extensions can be found here.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Additional Resources for Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-sources.html",
    "href": "data-sources.html",
    "title": "Sources of Data",
    "section": "",
    "text": "Language is everywhere. The aspiring data-science-in-psychology researcher can find data in a variety of sources, ranging from novel experiments to web scraping. In this unit, we introduce the most popular sources of data, along with the advantages and disadvantages of each one.\nIn the sections on APIs and web scraping, we give a detailed tutorial on accessing these resources with code.",
    "crumbs": [
      "Sources of Data"
    ]
  },
  {
    "objectID": "experiments.html",
    "href": "experiments.html",
    "title": "7  Experiments",
    "section": "",
    "text": "Experiments are the backbone of causal inference, and text analysis is no exception. Whether in a laboratory or on Amazon’s Mechanical Turk, experiments can be carefully controlled and are a good way to mitigate the effects of confounding variables. Though many people associate advanced natural language processing with “big data,” the methods discussed in this book can be used effectively even in small-scale laboratory experiments.\nAn example of using experiments in quantitative language research: Sap et al. (2020) had online participants write either true stories that happened to them recently, or fictional stories about the same topic. They then used a large language model, GPT, to measure two likelihoods for each sentence in the story: the likelihood of the sentence given the previous sentence, and the likelihood of the sentence given a rough summary of the story. The ratio of these two likelihoods is a measure of how predictably the story flows from one point to another. Sap et al. (2020) found that fictional stories flow much more predictably than true ones. They also found that true stories begin to flow more predictably when they are retold 2-3 months later. Sap et al. (2022) reproduced these findings using a more advanced language model, GPT-3. We will discuss these and other methods of measuring linguistic complexity in Chapter 22.\n\n\n\n\n\n\nAdvantages of Experimental Data Collection\n\n\n\n\nControl: Experiments mitigate the effects of confounding variables.\nCustomization: Experimenters can tailor the experiment to fit their particular research questions.\n\n\n\n\n\n\n\n\n\nDisadvantages of Experimental Data Collection\n\n\n\n\nExpensive\nTime-Consuming\nSmall Sample Size: Because they are costly and time-consuming, experiments generally result in small datasets.\n\n\n\n\n\n\n\n\nSap, M., Horvitz, E., Choi, Y., Smith, N. A., & Pennebaker, J. (2020). Recollection versus imagination: Exploring human memory and cognition via neural language models. In D. Jurafsky, J. Chai, N. Schluter, & J. Tetreault (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics (pp. 1970–1978). Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.acl-main.178\n\n\nSap, M., Jafarpour, A., Choi, Y., Smith, N. A., Pennebaker, J. W., & Horvitz, E. (2022). Quantifying the narrative flow of imagined versus autobiographical stories. Proceedings of the National Academy of Sciences, 119(45), e2211715119. https://doi.org/10.1073/pnas.2211715119",
    "crumbs": [
      "Sources of Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Experiments</span>"
    ]
  },
  {
    "objectID": "corpora.html",
    "href": "corpora.html",
    "title": "8  Corpus Data",
    "section": "",
    "text": "8.1 Archived Experimental Data\nScientists who run large experiments often publish their data online for use in further research. For example, data from Sap et al. (2020) and Sap et al. (2022), summarized in Chapter 7, are available online as the Hippocorpus dataset—a dataset we will be exploring in depth in Unit 2. Another good example is the Empathic Reactions dataset (Buechel et al., 2018), used in Section 4.2.1 and Chapter 5, in which participants read news stories, rated their own empathy and distress after reading them, and then described their thoughts about them verbally.\nOpen experimental data are often linked in published papers, especially since the founding of the Center for Open Science in 2013. Many psychology-related datasets can be browsed freely on osf.io, the Harvard Dataverse, and other locations.",
    "crumbs": [
      "Sources of Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Corpus Data</span>"
    ]
  },
  {
    "objectID": "corpora.html#archived-experimental-data",
    "href": "corpora.html#archived-experimental-data",
    "title": "8  Corpus Data",
    "section": "",
    "text": "Advantages of Archived Experimental Data\n\n\n\n\nProfessional: Experiments conducted by trained academics are generally well designed.\nWell-Documented: Datasets used in published papers have extensive documentation of the methods used to produce them.\n\n\n\n\n\n\n\n\n\nDisadvantages of Archived Experimental Data\n\n\n\n\nSometimes Not Well-Documented: Datasets not used in published papers often have poor documentation.\nSmall Sample Size: Experiments often result in relatively small datasets, which can pose problems for certain NLP methods.",
    "crumbs": [
      "Sources of Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Corpus Data</span>"
    ]
  },
  {
    "objectID": "corpora.html#linguistics-corpora",
    "href": "corpora.html#linguistics-corpora",
    "title": "8  Corpus Data",
    "section": "8.2 Linguistics Corpora",
    "text": "8.2 Linguistics Corpora\nThe field of linguistics has a long tradition of corpus data. Linguistics corpora provide extensive records of spoken and written speech in a wide range of contexts. These corpora are often very large and professionally curated, making them ideal for the techniques described in this book. On the other hand, they are generally curated with linguistics in mind, not psychology. This means that applying them to psychological questions requires some ingenuity.\nOne popular semi-experimental linguistics corpus is the HCRC Map Task Corpus (Anderson et al., 1991), in which pairs of participants collaborated in a communication game. In each pair, one partner could see a treasure map with a path through various landmarks, while the other partner had a similar map without a path. The first partner explained to the second how to draw the path. The partners’ communication accuracy can be measured as the distance between the drawn path and the original. Full dialogue transcriptions, as well as accuracy scores, are available online. The Map Task Corpus has been reproduced in many languages, including Hebrew, and is commonly used in psychology. For example, Dideriksen et al. (2023) used a Danish version of the Map Task Corpus, along with other dialogue corpora, to track the ways that speakers collaborate to achieve mutual understanding in different contexts.\n\nEnglish-Corpora.org: A list of the most widely used corpora of naturalistic English speech and writing, with download links for each. Also includes preprocessed data, such as word frequency counts for nearly 100 genres, from the Corpus of Contemporary American English (Davies, 2009), used in Section 5.1.1.\nUniversity of British Columbia Language Corpora List: Links to written and spoken language data in dozens of languages, including from bilingual and multilingual speakers.\nWikipedia’s List of Text Corpora\nList of NLP Corpora: Links to useful corpora for NLP tasks like task-oriented dialogue, translation, and sentiment analysis.\nConvokit Datasets: Links to written and spoken dialogues from debates, news interviews, telephone conversations, video chats, legal trials, and more.\n\n\n\n\n\n\n\nAdvantages of Linguistics Corpora\n\n\n\n\nProfessional: Linguistics corpora are generally well curated and well documented.\nEcological Validity: Corpora are often large and naturalistic—including for spoken dialogue, a domain that is otherwise out of reach for NLP.\n\n\n\n\n\n\n\n\n\nDisadvantages of Linguistics Corpora\n\n\n\n\nDomain-Specific: Linguistics corpora are generally created by linguists for linguists.",
    "crumbs": [
      "Sources of Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Corpus Data</span>"
    ]
  },
  {
    "objectID": "corpora.html#data-gathered-from-the-internet",
    "href": "corpora.html#data-gathered-from-the-internet",
    "title": "8  Corpus Data",
    "section": "8.3 Data Gathered From the Internet",
    "text": "8.3 Data Gathered From the Internet\nThe Internet is full of text, and you are not the first one to want to use it for research. Many corpora of online text data are free to download.\nSome sets of Internet data are professionally curated and well balanced. For example, the Blog Authorship Corpus (Schler et al., 2006) includes 681,288 blog posts annotated with age group (binned into ages 13-17, 23-27, and 33-47) and gender of author, with an equal number of male and female bloggers in each age group. Similarly, the 20 Newsgroups data set includes 20,000 newsgroup documents, partitioned evenly across 20 different newsgroups.\nSome sets of Internet data are available only post-processing. For example, Eichstaedt et al. (2015) published Twitter n-gram (and LDA topic) frequencies by US county, along with corresponding measures of well-being (featured in Chapter 3).\nSome sets of Internet data are very lightly curated. For example, the Reddit Top 2.5 Million dataset contains the top 1,000 all-time posts from the top 2,500 subreddits in August 2013, excluding NSFW subreddits.\nSome sets of archived Internet data are not curated at all. These are sometimes referred to as data dumps. For example, Baumgartner et al. (2020) published all Reddit Submissions and Comments posted during April 2019. Even more extensive data dumps of Reddit, covering historical data back to Reddit’s inception, can be found in records of Pushshift Reddit. Similar archives exist for Twitter. Data dumps are usually in JSON format. A JSON file is like a list in R, but formatted slightly differently. For a tutorial on processing JSON data in R, see the relevant chaper in R for Data Science.\nMost research topics in psychology do not require up-to-date data. As such, historical archives can be an invaluable resource. Biester et al. (2022) is a great example:\nAn example of social media archives in psychology research: Biester et al. (2022) used patterns curated by Cohan et al. (2018) to search Pushshift Reddit for users who publicly shared a depression diagnosis (e.g. “I have been diagnosed with depression”). They then used dictionary-based methods (Chapter 14) to measure various emotional qualities in users’ posts during the weeks leading to their declaration of the depression diagnosis, and in the weeks following. They found that anxiety, sadness, and cognitive processing increase in the weeks leading up to the declaration, and decrease afterwards.\n\n\n\n\n\n\nAdvantages of Archival Internet Data\n\n\n\n\nEasy: Pre-gathered datasets are low-cost and low-effort, often for very large sample sizes.\nUnintrusive: With pre-gathered datasets, you don’t have to worry about API usage limits or web scraping etiquette.\n\n\n\n\n\n\n\n\n\nDisadvantages of Archival Internet Data\n\n\n\n\nOld: Archival data do not reflect current events or recent trends.\n\n\n\n\n\n\n\n\n\nA Disclaimer on Social Media Data Dumps\n\n\n\nSince Reddit and Twitter restricted their API access in 2023, the legal status of large archival data dumps from those platforms (such as Pushshift Reddit) has been unclear. We are not qualified to give legal advice, but as long as you are not using the data for profit, you are unlikely to get in trouble.",
    "crumbs": [
      "Sources of Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Corpus Data</span>"
    ]
  },
  {
    "objectID": "corpora.html#other-public-data-sources",
    "href": "corpora.html#other-public-data-sources",
    "title": "8  Corpus Data",
    "section": "8.4 Other Public Data Sources",
    "text": "8.4 Other Public Data Sources\n\nKaggle: An online hub for data science, including many text- and psychology-related datasets\nHathiTrust: A digital library of 18+ million digitized books, including many curated collections\nForbes list of 30 Amazing (And Free) Public Data Sources\n\n\n\n\n\n\nAnderson, A. H., Bader, M., Bard, E. G., Boyle, E., Doherty, G., Garrod, S., Isard, S., Kowtko, J., McAllister, J., Miller, J., Sotillo, C., Thompson, H. S., & Weinert, R. (1991). The HCRC map task corpus. Language and Speech, 34(4), 351–366. https://doi.org/10.1177/002383099103400404\n\n\nBaumgartner, J., Zannettou, S., Keegan, B., Squire, M., & Blackburn, J. (2020). The pushshift reddit dataset. Zenodo. https://doi.org/10.5281/zenodo.3608135\n\n\nBiester, L., Pennebaker, J., & Mihalcea, R. (2022). Emotional and cognitive changes surrounding online depression identity claims. PLOS ONE, 17(12), 1–20. https://doi.org/10.1371/journal.pone.0278179\n\n\nBuechel, S., Buffone, A., Slaff, B., Ungar, L. H., & Sedoc, J. (2018). Modeling empathy and distress in reaction to news stories. CoRR, abs/1808.10399. http://arxiv.org/abs/1808.10399\n\n\nCohan, A., Desmet, B., Yates, A., Soldaini, L., MacAvaney, S., & Goharian, N. (2018). SMHD: A large-scale resource for exploring online language usage for multiple mental health conditions. In E. M. Bender, L. Derczynski, & P. Isabelle (Eds.), Proceedings of the 27th international conference on computational linguistics (pp. 1485–1497). Association for Computational Linguistics. https://aclanthology.org/C18-1126\n\n\nDavies, M. (2009). The 385+ million word corpus of contemporary american english (1990―2008+): Design, architecture, and linguistic insights. International Journal of Corpus Linguistics, 14, 159–190. https://www.english-corpora.org//coca/\n\n\nDideriksen, C., Christiansen, M. H., Tylén, K., Dingemanse, M., & Fusaroli, R. (2023). Quantifying the interplay of conversational devices in building mutual understanding. Journal of Experimental Psychology: General, 152, 864–889. https://doi.org/10.1037/xge0001301\n\n\nEichstaedt, J. C., Schwartz, H. A., Kern, M. L., Park, G., Labarthe, D. R., Merchant, R. M., Jha, S., Agrawal, M., Dziurzynski, L. A., Sap, M., Weeg, C., Larson, E. E., Ungar, L. H., & Seligman, M. E. P. (2015). Psychological language on twitter predicts county-level heart disease mortality. Psychological Science, 26(2), 159–169. https://doi.org/10.1177/0956797614557867\n\n\nSap, M., Horvitz, E., Choi, Y., Smith, N. A., & Pennebaker, J. (2020). Recollection versus imagination: Exploring human memory and cognition via neural language models. In D. Jurafsky, J. Chai, N. Schluter, & J. Tetreault (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics (pp. 1970–1978). Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.acl-main.178\n\n\nSap, M., Jafarpour, A., Choi, Y., Smith, N. A., Pennebaker, J. W., & Horvitz, E. (2022). Quantifying the narrative flow of imagined versus autobiographical stories. Proceedings of the National Academy of Sciences, 119(45), e2211715119. https://doi.org/10.1073/pnas.2211715119\n\n\nSchler, J., Koppel, M., Argamon, S., & Pennebaker, J. (2006). Effects of age and gender on blogging. 199–205.",
    "crumbs": [
      "Sources of Data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Corpus Data</span>"
    ]
  },
  {
    "objectID": "apis.html",
    "href": "apis.html",
    "title": "9  Web APIs",
    "section": "",
    "text": "9.1 API Basic Concepts",
    "crumbs": [
      "Sources of Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Web APIs</span>"
    ]
  },
  {
    "objectID": "apis.html#api-basic-concepts",
    "href": "apis.html#api-basic-concepts",
    "title": "9  Web APIs",
    "section": "",
    "text": "Requests: Each time you visit a URL associated with an API, you are submitting a request for data.\n\nEndpoints: Every API has at least one endpoint, a contact point for particular types of requests.\n\nRate Limits: Many APIs set limits on the number of requests you can make per minute (or per second). This is because processing requests costs time and money for the host. If you go beyond the rate limit, the API will return an error like “429 Too Many Requests.”\n\n\nAuthentication: Some APIs are not open to the public, instead requiring users to apply for access or pay for a subscription. When accessing these APIs, you need an API key or an access token. This is your password for the API.\n\n\n9.1.1 vosonSML\n\nlibrary(vosonSML)\n\nFor accessing social media APIs with vosonSML, you only need two functions:\n\n\nAuthenticate() creates a credential object that contains any keys or access tokens needed to access a particular API. This credential object can be reused as long as your credentials don’t change.\n\nCollect() initiates a series of API requests and stores the results as a dataframe or list of dataframes.\n\nvosonSML also provides tools for working with network data (i.e. the ways in which users or posts are connected to one another), but these will not be covered in this textbook.",
    "crumbs": [
      "Sources of Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Web APIs</span>"
    ]
  },
  {
    "objectID": "apis.html#reddit",
    "href": "apis.html#reddit",
    "title": "9  Web APIs",
    "section": "\n9.2 Reddit",
    "text": "9.2 Reddit\nReddit generated over 3 billion posts and comments in 2022. Many of these contain long-form text. And its API is free. These traits make it very useful to researchers.\nReddit content exists on three levels:\n\n\nCommunities, called “subreddits” are spaces for users to post about a specific topic. Individual subreddits are referred to as “r/SUBREDDIT”. For example, r/dataisbeautiful is for data visualizations, r/NaturalLanguage is for posts about natural language processing, and r/SampleSize is a place to gather volunteer participants for surveys and polls. Communities are policed by moderators, users who can remove posts or ban other users from the community.\n\nPosts are posted by users to a particular subreddit. Each post has a title, which is always text, and content, which can contain text, images, and videos.\n\nComments are responses to posts, responses to responses to posts, responses to responses to responses to posts, etc. These are always text.\n\n\n9.2.1 The Reddit Algorithm\nReddit data are not representative samples of the global population. They are not even representative samples of Reddit users. This is partly due to the dynamics of the Reddit ranking algorithm, which gives precedent to viral content. The details of the algorithm are no longer public, but it is largely based on “upvotes” and “downvotes” from the community, and probably also incorporates the time since posting. The ranking system for comments is almost certainly different from the ranking system for posts. Reddit also has a “Karma” system, by which users who post popular content get subsequent content boosted. This creates an incentive system which is sometimes exploited by the advertising industry. The bottom line: Reddit posts are disproportionately viral. To partially counteract this when gathering Reddit data, set the API to sort by recency (sort = \"new\") rather than the default, “best” (Section 9.2.3).\n\n9.2.2 Communities\nTo retrieve the posts from a Reddit community, call Authenticate(\"reddit\") followed by Collect() with endpoint = \"listing\". To specify the particular community (or communities) in which you are interested, use the subreddits parameter. For example, the following code retrieves the 20 most recent posts from r/RedditAPIAdvocacy, a subreddit devoted to fighting restriction of the Reddit API.\n\nAPIAdvocacy_posts &lt;- \n  Authenticate(\"reddit\") |&gt;\n  Collect(\n    endpoint = \"listing\", \n    subreddits = \"RedditAPIAdvocacy\",\n    sort = \"new\",   # newest posts first\n    period = \"all\", # all time\n    max = 20,       # 20 most recent posts\n    verbose = TRUE\n    )\n\nhead(APIAdvocacy_posts)\n\nTo psychology researchers, the most interesting subreddits are often those devoted to psychological disorders, for example r/depression, r/socialanxiety, r/SuicideWatch, r/bipolarreddit, and r/opiates. Subreddits devoted to intellectual discussion, such as r/changemyview, r/IAmA, and r/ExplainLikeImFive, are also interesting subjects for research. Lastly, much research is devoted to the behavior of Redditors in political communities.\nAn example of using Reddit’s political communities in research: In a series of experiments, Ashokkumar & Pennebaker (2022) used the Reddit API and other sources to develop a dictionary-based measure of group identity strength. A dictionary is a list of words associated with a given psychological or other construct, for example associating “sleepy” and “down” with depression (see Chapter 14). Ashokkumar and Pennebaker had participants write small, free-form responses and fill out questionnaires on group identity strength. They then identified existing dictionaries that were associated with the questionnaire-based measures, and used these to construct a composite measure of group identity strength. When applied to text, they called this measure “unquestioning affiliation.” Among college students, they showed that unquestioning affiliation in writing could predict whether students would drop out of college one year later. Finally, they applied their method to naturalistic data retrieved from the Reddit API, from the political communities r/The_Donald and r/hillaryclinton, and showed that users’ unquestioning affiliation predicted the duration that they would stay in the community before leaving.\n\n9.2.3 Threads\nA post with all of its associated comments is called a thread. For example, Hadley Wickham, the founder of the tidyverse, ran a thread on the r/dataisbeautiful subreddit in 2015, in which he answered commenters’ questions. To retrieve that thread, first find its URL. Do this by finding the threat on Reddit and copying the link from your web browser. Then call Authenticate(\"reddit\") and Collect(), like so:\n\n# List of thread urls (in this case only one)\nthreads &lt;- c(\"https://www.reddit.com/r/dataisbeautiful/comments/3mp9r7/im_hadley_wickham_chief_scientist_at_rstudio_and/\")\n\n# Retrieve the data\n## Since the Reddit API is open, we don't need\n## to give any passwords to Authenticate()\nhadley_threads &lt;- \n  Authenticate(\"reddit\") |&gt;\n  Collect(\n    threadUrls = threads, \n    sort = \"new\", # newest comments first\n    verbose = TRUE # give updates while running\n    )\n\n# Peak at Hadley's responses\nhadley_threads |&gt;\n  filter(user == \"hadley\") |&gt; \n  select(structure, comment) |&gt; \n  head()\n\nThe resulting dataframe has many columns. The most useful are the following:\n\n\ncomment is the text of the comment itself.\n\nuser is user who posted the comment.\n\nstructure is the tree structure leading to the comment. For example, “137_6_2” is the 2nd comment on the 6th comment on the 137th comment on the original post.\n\ncomm_date is the date and time of the comment, in UTC. Since this is in character format, it needs to be converted to a datetime with lubridate::as_datetime().\n\nBy processing the structure values, we can conceptualize the thread as a tree, with the original post as the root and comments as branches:\n\nlibrary(ggraph)\n\nhadley_threads |&gt; \n  mutate(\n    level = str_count(structure, \"_\") + 1L,\n    parent = str_remove(structure, \"_[[:digit:]]+$\"),\n    parent = if_else(level == 1, \"0\", parent)\n    ) |&gt; \n  select(parent, structure) |&gt; \n  tidygraph::as_tbl_graph() |&gt; \n  ggraph(layout = 'tree', circular = FALSE) +\n    geom_edge_diagonal(alpha = .2, linewidth = 1) +\n    geom_node_point(shape = 21, fill = \"orangered\") +\n    theme_void()\n\nAn example of using Reddit threads in research: Xiao & Mensah (2022) collected threads from r/changemyview, a community in which the original poster (OP) makes a claim, commenters make arguments against that claim, and the OP responds with “delta points” to indicate how much their view has been changed. Xiao & Mensah (2022) analyzed the frequency of delta points at each level of the thread tree, and found that the OP’s view tended to change most after the 2nd, 4th, 6th, 8th, and 10th levels of comments—in other words, every other level. They then analyzed the semantic similarity between comments using cosine similarity (Section 17.1.2) between simple word counts. The results suggested that every-other-level comments tend to elaborate and refine the comments immediately before them, so that the latter are perceived to be more persuasive.\n\n9.2.4 Other Reddit Features\nFor most applications, vosonSML is a sufficient Reddit API wrapper. For slightly more advanced functionality like searching for subreddits with a keyword or retrieving a user’s comment history, we recommend Reddit Extractor.\n\n\n\n\n\n\nAdvantages of Reddit Data\n\n\n\n\n\nExplicit Communities: Reddit communities are clearly defined and explicit about their purposes. Reddit includes communities devoted to fictional storytelling, factual storytelling, personal reflection, technical advice, political discussion, joke telling, and much more. This makes it easy to gather a domain-specific sample.\n\nLong-form Text Responses: While some social media platforms have character limits for posts or comments, Reddit has many communities devoted to long-form text. Longer documents can make characterization of their content more accurate.\n\nAnonymity: Reddit users can remain relatively anonymous, which might encourage more honest and open sharing of experiences.\n\n\n\n\n\n\n\n\n\nDisadvantages of Reddit Data\n\n\n\n\n\nSelection Bias: Certain subreddits may attract specific demographics, leading to potential selection bias in the data.\n\nAnonymity: In some cases, anonymity may make user behavior less representative. For example, many users have multiple accounts, which they use for different activities on the platform, making users seem disproportionately narrow in their interests.",
    "crumbs": [
      "Sources of Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Web APIs</span>"
    ]
  },
  {
    "objectID": "apis.html#twitter-x",
    "href": "apis.html#twitter-x",
    "title": "9  Web APIs",
    "section": "\n9.3 Twitter / X",
    "text": "9.3 Twitter / X\nTwitter has been called the “model organism” of big data research (Tufekci, 2014). This is because, until 2023, Twitter was the largest source of free, open social text data. In contrast to Facebook, almost all Twitter activity is public. Twitter’s standardized character limit (originally 140, now 280), along with its simple network structure, make structuring and analyzing the data easy.\n\n9.3.1 Followers, Mentions, and Retweets\nWhereas Reddit users subscribe to communities, Twitter users subscribe to other users. This is called “following.” They can also “retweet” other users’ posts to their own feed, or explicitly mention other users in posts using the “@” symbol. This focus on connections between individuals means that each user’s communal associations can be characterized in a rich, nuanced way.\nAn example of using Twitter networks in research: Mosleh et al. (2021) collected Twitter IDs of participants who filled out a set of questions with intuitively compelling but incorrect answers, testing analytic thinking. They then analyzed the Twitter accounts that each participant followed, and found a large cluster of accounts followed preferentially by people with less analytic thinking. This group most prominently featured Twitter accounts of retail brands.\n\n9.3.2 Geolocation on Twitter\nTwitter allows users to specify a geographical location along their post. While these user-provided locations can sometimes be difficult to interpret, they allow analyses by geographic location.\nAn example of using Twitter geolocation in research: Takhteyev et al. (2012) used Twitter’s public feed to gather a sample of users. Researchers also collected the profiles of all the users followed by the users in the original sample. They then analyzed the distribution of these ties over geographical distance, national boundaries, and language differences. They found that Twitter users are most strongly tied to people in their own city, and that between regions, the number of airline flights was the best predictor of ties.\n\n\n\n\n\n\nAdvantages of Twitter Data\n\n\n\n\n\nCharacter Limit: Twitter’s character limit means that Tweets vary less in their length. This can decrease variance in measurements.\n\nPure Network Structure: As opposed to Reddit’s distinct communities and hierarchical comment trees, Twitter has a network-like social structure in which users are linked to other individual users or posts. This structure can reveal underlying communities and the strength of the connections between them.\n\n\n\n\n\n\n\n\n\nDisadvantages of Twitter Data\n\n\n\n\n\nCharacter Limit: The character limit on tweets may limit the depth of responses and the ability to convey complex psychological experiences. The smaller sample of words in each document can also make measurement less reliable.\n\nLimited Context: Tweets may lack context, making it challenging to fully understand the meaning behind short messages.\n\nNo more free API: While lots of historical Twitter data are available on the internet (see Chapter 8), the API has been prohibitively expensive since 2023.",
    "crumbs": [
      "Sources of Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Web APIs</span>"
    ]
  },
  {
    "objectID": "apis.html#mastodon",
    "href": "apis.html#mastodon",
    "title": "9  Web APIs",
    "section": "\n9.4 Mastodon",
    "text": "9.4 Mastodon\nIn some ways, you can think of Mastodon as open source Twitter. Indeed, a large portion of the Mastodon user base started using it as a replacement for Twitter after controversial events in 2022. Like Twitter users, Mastodon users post short posts to their feed, in which they can mention other users with “@” or use hashtags with “#.” Like Twitter users, Mastodon users can follow other users and be followed by them.\nDespite their similarities, networks of users on Mastodon and Twitter are not the same. The biggest difference is that each Mastodon user is associated with a server (also known as an instance). Mastodon servers can have hundreds of thousands of users, or just a few. Each server runs the Mastodon source code independently and hosts all of its users’ content. Many servers have a theme based on a specific interest. It is also common for servers to be based around a particular locality, region, ethnicity, or country. For example, tech.lgbt is for LGBTQIA+ technologists, SFBA.social is for the San Francisco Bay Area in California, and fediscience.org is for active scientists.\nBecause of their topic-specific organization, Mastodon servers can function a bit like Reddit communities. Like Reddit communities, Mastodon servers can set their own rules for content. They can even change the default 500-character limit. The key difference is that on Reddit, posts are associated with a community, whereas on Mastodon, users are associated with a community. In other words, community affiliation on Mastodon is less related to the content of its posts and more related to the identity of its users. This feature can be useful when collecting user activity. For example, if we wanted to study the overall behavior of members of the LGBTQIA+ community, we could collect the activity of users affiliated with the tech.lgbt and other community-specific servers.\n\n9.4.1 Servers\nMastodon server feeds are organized chronologically (unlike Reddit or Twitter, which have complex ordering algorithms). To retrieve the most recent posts on a particular server using vosonSML, we call Authenticate(\"mastodon\") followed by Collect() with the “search” endpoint. The name of the server is identified with “instance.”\n\nmast_science &lt;- \n  Authenticate(\"mastodon\") |&gt;\n  Collect(\n    endpoint = \"search\",\n    instance = \"fediscience.org\",\n    local = TRUE,\n    numPosts = 100, # number of most recent posts\n    verbose = TRUE\n  )\n\nThe global feed (i.e. across all servers) can be accessed by setting local = FALSE. You can also retrieve posts with a specific hashtag using the hashtag parameter (e.g. hashtag = \"rstats\").\nIn all cases, the call results in a list of two dataframes, “posts” and “users.” In the “posts” dataframe we have columns such as:\n\n\ncontent.text is the text of the post itself.\n\naccount is a column of one-row dataframes containing information about the user who posted each post. You can convert this into a simple account ID column with mutate(account_id = sapply(account, function(c) pull(c,id))).\n\nid is a unique ID for the post.\n\nin_reply_to_id is the ID of the post that this post is directly responding to. This can be used to build tree structures, as we did in Section 9.2.3, but see Section 9.4.2 for an alternative way to do this.\n\nin_reply_to_account_id is the account ID of the poster to whom this post is directly responding. This can be used to build networks of users as opposed to networks of threads.\n\ncreated_at is the date and time of the post in UTC, already in datetime format.\n\nThe “users” dataframe has the same information as the account column of “posts”, but formatted as a single dataframe with one row per user.\n\n9.4.2 Threads\nSince Mastodon users can mark their posts as a reply to an earlier post, these posts are essentially “comments” on the original post. A chain of posts that reply to an original post is called a thread. We can collect the contents of one or more threads with endpoint = \"thread\", by specifying the URL of the first post in the thread with threadUrls. For example, we can retrieve a thread by one the authors of this book, Almog:\n\nalmog_thread &lt;- \n  Authenticate(\"mastodon\") |&gt;\n  Collect(\n    endpoint = \"thread\",\n    threadUrls = c(\"https://fediscience.org/@almogsi/110513658753942590\"),\n    verbose = TRUE\n  )\n\nalmog_thread$posts |&gt; \n  select(created_at, content.text) |&gt; \n  head()\n\n\n9.4.3 Other Mastodon Features\nFor most applications, vosonSML is a sufficient Mastodon API wrapper. For slightly more advanced functionality like searching accounts or retrieving a user’s follows and followers, we recommend rtoot.\n\n\n\n\n\n\nAdvantages of Mastodon Data\n\n\n\n\n\nFree: Because Mastodon’s servers are decentralized (i.e. there is no single company controlling its operation), there is no concern that its data will become unavailable.\n\nNo Feed Algorithm: Mastodon does not use a black-box algorithm to decide what users see on their feed. Users have total control over their own subscriptions (e.g. users who follow more accounts have more content on their feeds). This eliminates a possible confounding factor present in many social media platforms.\n\nDomain-Specific Servers: Users are associated with a particular server, often associated with an identity or region. This allows more targeted data collection.\n\n\n\n\n\n\n\n\n\nDisadvantages of Mastodon Data\n\n\n\n\n\nSampling Bias: Because Mastodon is largely populated by users who fled from Twitter or Facebook, their demographics may be especially unrepresentative.\n\nSmaller User Base: Mastodon has a much smaller user base compared to major platforms like Twitter and Reddit.",
    "crumbs": [
      "Sources of Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Web APIs</span>"
    ]
  },
  {
    "objectID": "apis.html#youtube",
    "href": "apis.html#youtube",
    "title": "9  Web APIs",
    "section": "\n9.5 YouTube",
    "text": "9.5 YouTube\nYouTube is structurally similar to Twitter in that neither users nor posts are explicitly associated with a community. Users subscribe directly to other users’ channels. Each user can upload posts to their own channel. As on Reddit, these posts generally have a title, a description, and a space for comments.\nYouTube is different from other social media platforms in a few ways. Most obviously, posts consist primarily of videos. This means that analyzing a post’s description will not give an accurate representation of the post’s content. Analyzing the content of the video directly is generally impossible, since YouTube does not allow videos to be downloaded through their API. One possible workaround uses YouTube’s automatically generated video transcriptions—though YouTube does not allow access to full transcriptions for standard API key bearers, there are some workarounds available. Nevertheless, we will limit ourselves here to text data. This primarily consists of comments on videos.\n\n9.5.1 Video Comments\nAnother important difference between YouTube and other social media platforms is in the structure of the comments. Whereas Reddit comments (or Twitter/Mastodon threads) can respond one to another, generating a deep tree-like structure, YouTube comments have only two levels of depth. In other words, comments either respond directly to a video (top-level) or they respond to a top-level comment (level two). This feature can sometimes make conversations hard to track, since level two comments may be responding to another level two comment, even if they are not explicitly marked as such. On the other hand, this feature may constrain YouTube comments to respond more directly to the video.\nThe interpretation of YouTube comments as direct responses to the original video is enhanced by the rich video format of the stimulus, which likely draws users in more than a text-only post would (cf. Yadav et al., 2011). For this reason, YouTube can be a good platform for studying responses to shared stimuli in a social context.\nTo access the the YouTube API, you will need an API key. You can get an API key in a matter of seconds by registering your project with Google cloud. Once you have the key, you can access the API through vosonSML by calling youtube_auth &lt;- Authenticate(\"youtube\", apiKey = \"xxxxxxxxxxxxxx\"), followed by Collect(). In the code below, we’ll collect the comments on 3Blue1Brown’s excellent explanation of deep learning algorithms.\n\n# retrieve youtube_auth from separate R script \n# (so as not to publicize my key)\nsource(\"~/Projects/sandbox/youtube_auth.R\")\n\n# this could also be a vector of URLs\nvideo_url &lt;- \"https://www.youtube.com/watch?v=aircAruvnKk\"\n\n# collect comments\ndeep_learning_comments &lt;- \n  youtube_auth |&gt;\n  Collect(videoIDs = video_url,\n          maxComments = 100,\n          verbose = TRUE)\n\nThis call results in a dataframe with many columns, including the following:\n\n\nComment is the text of the comment itself.\n\nAuthorDisplayName is the username of the commenter.\n\nAuthorChannelUrl is the URL for the commenter’s own channel.\n\nPublishedAt is the date and time of the comment, in UTC. Since this is in character format, it needs to be converted to a datetime with lubridate::as_datetime().\n\nCommentID is a unique ID for the comment.\n\nParentID is the ID of the comment to which this comment is responding.\n\nIf we visualize these comments like we visualized the Reddit comments in Section 9.2.3, we see that the tree has only two levels:\n\nset.seed(2)\ndeep_learning_comments |&gt; \n  select(ParentID, CommentID) |&gt; \n  tidygraph::as_tbl_graph() |&gt; \n  ggraph::ggraph(layout = 'tree', circular = FALSE) +\n    ggraph::geom_edge_diagonal(alpha = .2, linewidth = 1) +\n    ggraph::geom_node_point(shape = 21, fill = \"orangered\") +\n    theme_void()\n\nAn example of using YouTube comments in research: Rosenbusch et al. (2019) collected comments from 20 vlogs each from 110 vloggers on YouTube, along with the transcripts of those videos. They used dictionary-based word counts (Chapter 14) to measure the emotional content of both video transcripts and video comments. Using a multilevel model, they found that both video- and channel-level emotional content independently predict commenter emotions.\n\n\n\n\n\n\nAdvantages of YouTube Data\n\n\n\n\n\nRich Stimuli: YouTube videos are extended and multimodal (i.e. they include both audio and visual stimuli). Video comments can be usefully construed as responses to that stimulus.\n\nConstrained Structure: YouTube limits its comment trees to two levels, which can simplify analyses.\n\n\n\n\n\n\n\n\n\nDisadvantages of YouTube Data\n\n\n\n\n\nMissing Context: YouTube comments respond to videos. Since videos are generally unavailable for download and are in any case of a different medium than text-based comments, measuring the relationship between a comment and the video it refers to can be difficult.\n\nConstrained Structure: Because YouTube limits its comment trees to two levels, complex referential structures can be difficult to decode.",
    "crumbs": [
      "Sources of Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Web APIs</span>"
    ]
  },
  {
    "objectID": "apis.html#sec-other-apis",
    "href": "apis.html#sec-other-apis",
    "title": "9  Web APIs",
    "section": "\n9.6 Other Web APIs",
    "text": "9.6 Other Web APIs\nIn this chapter, we have covered four social media platforms and—with the exception of Twitter—how to access their APIs using vosonSML. Nevertheless, this has been far from an exhaustive list of social media platforms that provide APIs. Many other sources of data exist, each with their own advantages and disadvantages. Many of these do not have custom R wrappers for their APIs, but reading a little API documentation shouldn’t deter you from finding your ideal dataset. The following are some of our favorite sources of psychologically interesting data accessible with public APIs.\n\n\nStackExchange is a network of question-and-answer sites like StackOverflow (for programmers), MathOverflow (for mathematicians), Arqade (for videogamers), Cross Validated (for statistics and machine learining), Science Fiction & Fantasy (for fans) and many more. StackExchange provides a public API for retrieving questions, answers, and comments from their sites.\n\nWikipedia provides a public API for retrieving content, including individial contributor data.\n\nSemantic Scholar is a website for navigating academic literature. It offers a public API for accessing abstracts, references, citations, and other information about academic publications. The API also allows access to their contextualized semantic embeddings of publications (see Chapter 19). Obtaining an API key requires a simple registration.\nLike Semantic Scholar, CORE offers access to full text articles and metadata for open access research papers. Obtaining access requires a simple registration.\nFacebook advertises a research-oriented API, which requires registration.\nMastodon is not the only open source, decentralized social media platform with an API. There are many, though most are small.\n\nTo access one of these APIs through R, we recommend using the httr2 package. Most APIs send data in JSON format. As we mentioned in Chapter 8, a JSON file is like a list in R, but formatted slightly differently. The httr2 package also has functions for converting JSON to R objects. In the example below, we write a function to retrieve all questions with a particular tag from a particular site on StackExchange, and format them as a tibble.\n\nlibrary(httr2)\n\n# StackExchange API search endpoint\nendpoint &lt;- request(\"https://api.stackexchange.com/2.2/search?\")\n\n# Function to retrieve posts with a given tag\nget_tagged_posts &lt;- function(tag, ...,\n                             # default params\n                             site = \"stackoverflow\",\n                             pagesize = 100,\n                             user = NULL,\n                             order = \"desc\",\n                             sort = \"activity\",\n                             page = 1) {\n  # define request params\n  params &lt;- list(\n    ...,\n    order = order,\n    sort = sort,\n    tagged = tag,\n    site = site,\n    pagesize = pagesize,\n    page = page\n    )\n  \n  # add user agent (this is polite for APIs)\n  req &lt;- endpoint |&gt; \n    req_user_agent(user)\n  \n  # add query parameters\n  req &lt;- req |&gt; \n    req_url_query(!!!params)\n  \n  # perform request + convert to list\n  resp &lt;- req |&gt; \n    req_perform() |&gt; \n    resp_body_json()\n  \n  # warn the user if API gave a \"backoff\" message, \n  if(!is.null(resp$backoff)){\n    warning(\"Received backoff request from API.\",\n            \"Wait at least \", resp$backoff, \" seconds\",\n            \" before trying again!\")\n  }\n  \n  # Convert list to tibble,\n  # keeping only relevant variables\n  tibble::tibble(\n    title = sapply(\n      resp$items, \n      function(x) x$title\n      ),\n    is_answered = sapply(\n      resp$items, \n      function(x) x$is_answered\n      ),\n    view_count = sapply(\n      resp$items, \n      function(x) x$view_count\n      ),\n    creation_date = sapply(\n      resp$items, \n      function(x) x$creation_date\n      ),\n    user_id = sapply(\n      resp$items, \n      function(x) x$user_id\n      ),\n    link = sapply(\n      resp$items, \n      function(x) x$link\n      )\n    )\n}\n\n# EXAMPLE USAGE: get 10 \"teen\"-related questions \n# from the \"parenting\" StackExchange site\nteen_posts &lt;- get_tagged_posts(\n  \"teen\", site = \"parenting\",\n  user = \"Data Science for Psychology (ds4psych@gmail.com)\",\n  pagesize = 10\n  )\n\nFor further details on accessing APIs with httr2 (including how to deal with rate limits), see the vignette.\n\n\n\n\n\nAshokkumar, A., & Pennebaker, J. W. (2022). Tracking group identity through natural language within groups. PNAS Nexus, 1(2), pgac022. https://doi.org/10.1093/pnasnexus/pgac022\n\n\nMosleh, M., Pennycook, G., & Arechar, A. (2021). Cognitive reflection correlates with behavior on twitter. Nature Communications, 12. https://doi.org/10.1038/s41467-020-20043-0\n\n\nRosenbusch, H., Evans, A. M., & Zeelenberg, M. (2019). Multilevel emotion transfer on YouTube: Disentangling the effects of emotional contagion and homophily on video audiences. Social Psychological and Personality Science, 10(8), 1028–1035. https://doi.org/10.1177/1948550618820309\n\n\nTakhteyev, Y., Gruzd, A., & Wellman, B. (2012). Geography of twitter networks. Social Networks, 34(1), 73–81. https://doi.org/https://doi.org/10.1016/j.socnet.2011.05.006\n\n\nTufekci, Z. (2014). Big questions for social media big data: Representativeness, validity and other methodological pitfalls. Proceedings of the 8th International Conference on Weblogs and Social Media, ICWSM 2014, 8. https://doi.org/10.1609/icwsm.v8i1.14517\n\n\nXiao, L., & Mensah, H. (2022). How does the thread level of a comment affect its perceived persuasiveness? A reddit study. In K. Arai (Ed.), Intelligent computing (pp. 800–813). Springer International Publishing.\n\n\nYadav, A., Phillips, M., Lundeberg, M., Koehler, M., Hilden, K., & Dirkin, K. (2011). If a picture is worth a thousand words is video worth a million? Differences in affective and cognitive processing of video and text cases. J. Computing in Higher Education, 23, 15–37. https://doi.org/10.1007/s12528-011-9042-y",
    "crumbs": [
      "Sources of Data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Web APIs</span>"
    ]
  },
  {
    "objectID": "scraping.html",
    "href": "scraping.html",
    "title": "10  Web Scraping",
    "section": "",
    "text": "10.1 Be Polite\nEach time you visit a website, the website’s server must send the requested data to your computer. For normal web browsing by humans, this is not a problem. But often, scraping requires your code to visit many web pages. For example, say we wanted a list of poems by English poets, along with the birth dates of their authors. We might have the computer scrape the list of English poets from the Poetry Foundation, which requires clicking through 17 pages of search results, and retrieve each author’s birth date and the URL for each author’s full list of poems. We could then have the computer visit each of those URLs and retrieve the URL for each individual poem by each author. Finally, the computer would visit each poem URL and retrieve the text of the poem. In all, this process might require visiting hundreds or even thousands of web pages.\nFor the same reasons that APIs generally have rate limits (Chapter 9), web scraping algorithms should be polite—not, for example, requesting thousands of pages in the space of a few seconds. If you are not polite, you are likely to get banned as a bot.\nTo read more about web scraping etiquette and easy ways to implement it, see the relevant chapter in R for Data Science, and the homepage of the polite package.",
    "crumbs": [
      "Sources of Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Web Scraping</span>"
    ]
  },
  {
    "objectID": "scraping.html#a-simple-example",
    "href": "scraping.html#a-simple-example",
    "title": "10  Web Scraping",
    "section": "\n10.2 A Simple Example",
    "text": "10.2 A Simple Example\nTo give you a taste for what web scraping is like, we will give a simple example of scraping a single page.\nThe page in this case will be the blog reel of one of the authors of this book, Louis.\n\nWe will scrape the name and date of each blog post using the rvest package. First, we retrieve the raw html code of the webpage.\n\nlibrary(rvest)\n\nhtml &lt;- read_html(\"https://rimonim.github.io/blog.html\")\n\nThen, we have to identify the name of the particular objects we are looking for. In most cases, this can be done by right clicking and pressing “Inspect Element” in the web browser.1\n\nNow that we know that blog titles are called “h3.no-anchor.listing-title,” we can extract those objects from the raw html using html_elements() and convert them to regular text using html_text2().\n\npost_titles &lt;- html |&gt; \n  html_elements(\"h3.no-anchor.listing-title\") |&gt; \n  html_text2()\n\nhead(post_titles)\n\n#&gt; [1] \"Americans Have Eight Kinds of Days\"                                               \n#&gt; [2] \"Advanced Machine Learning Approaches for Detecting Trolls on Twitter\"             \n#&gt; [3] \"Shockingly, Most Reddit Environmentalists are not Greta Thunberg\"                 \n#&gt; [4] \"Comparing Four Methods of Sentiment Analysis\"                                     \n#&gt; [5] \"Do Employees Tend to Have the Same First Name as Their Bosses?\"                   \n#&gt; [6] \"Designing a Poster to Visualize the Timeline of Philosophers in the Islamic World\"\n\n\nTo build a dataset with multiple variables, we repeat this process for each variable.\n\nposts &lt;- tibble(\n  date = html |&gt; \n    html_elements(\"div.listing-date\") |&gt; \n    html_text2(),\n  title = post_titles\n)\n\nhead(posts)\n\n#&gt; # A tibble: 6 × 2\n#&gt;   date         title                                                            \n#&gt;   &lt;chr&gt;        &lt;chr&gt;                                                            \n#&gt; 1 Aug 23, 2023 Americans Have Eight Kinds of Days                               \n#&gt; 2 Jul 20, 2023 Advanced Machine Learning Approaches for Detecting Trolls on Twi…\n#&gt; 3 Jul 12, 2023 Shockingly, Most Reddit Environmentalists are not Greta Thunberg \n#&gt; 4 Jul 10, 2023 Comparing Four Methods of Sentiment Analysis                     \n#&gt; 5 Jun 28, 2023 Do Employees Tend to Have the Same First Name as Their Bosses?   \n#&gt; 6 May 8, 2023  Designing a Poster to Visualize the Timeline of Philosophers in …\n\n\nThis is a very basic example. For a more in-depth tutorial on web scraping (including more complex examples), see the relevant chapter in R for Data Science.",
    "crumbs": [
      "Sources of Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Web Scraping</span>"
    ]
  },
  {
    "objectID": "scraping.html#footnotes",
    "href": "scraping.html#footnotes",
    "title": "10  Web Scraping",
    "section": "",
    "text": "Using “Inspect Element” on more complicated web pages can be difficult. For finding element names more easily, we recommend SelectorGadget↩︎",
    "crumbs": [
      "Sources of Data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Web Scraping</span>"
    ]
  },
  {
    "objectID": "quantification.html",
    "href": "quantification.html",
    "title": "Quantifying Psychological Properties of Text",
    "section": "",
    "text": "The average native English speaker has a functionally useful vocabulary size of over 20,000 words (Hellman, 2011). 20,000 words means 20,000 variables for psychologists to analyze. But that’s not all—words can be arranged in infinite orders, drastically changing their meanings in context. How do we capture all of this complexity in simple measurements that we can use for analysis?\nIn this unit, we introduce methods for quantifying psychological properties of text. We begin with fundamental skills for language processing in R, and move gradually to more advanced methods. In the final sections, we cover recent developments in the use of large language models for psychological research.\nThroughout the unit, we will provide example analyses using the Hippocorpus dataset, which contains the data from Sap et al. (2020) and Sap et al. (2022). The dataset consists primarily of texts written by online participants, who were instructed to write either true stories that happened to them recently or fictional stories about a comparable topic. Participants then retold the true stories several months later. The dataset also includes a variety of psychological and demographic characteristics of the participants, including gender and openness to experience. The dataset itself is not included in the book’s files, but you can easily download it from the website and follow along.\n\n\n\n\nHellman, A. B. (2011). Vocabulary size and depth of word knowledge in adult-onset second language acquisition. International Journal of Applied Linguistics, 21(2), 162–182.\n\n\nSap, M., Horvitz, E., Choi, Y., Smith, N. A., & Pennebaker, J. (2020). Recollection versus imagination: Exploring human memory and cognition via neural language models. In D. Jurafsky, J. Chai, N. Schluter, & J. Tetreault (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics (pp. 1970–1978). Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.acl-main.178\n\n\nSap, M., Jafarpour, A., Choi, Y., Smith, N. A., Pennebaker, J. W., & Horvitz, E. (2022). Quantifying the narrative flow of imagined versus autobiographical stories. Proceedings of the National Academy of Sciences, 119(45), e2211715119. https://doi.org/10.1073/pnas.2211715119",
    "crumbs": [
      "Quantifying Psychological Properties of Text"
    ]
  },
  {
    "objectID": "look-at-your-data.html",
    "href": "look-at-your-data.html",
    "title": "11  Look at Your Data",
    "section": "",
    "text": "Before using any theory-driven methods, it is always worthwhile to get a sense of your data more generally. This is as true for text data as it is for any other format. The process of exploring your data without a particular theory or construct in mind is called Exploratory Data Analysis (EDA).\nMany guides to EDA for text data suggest plotting histograms of text length, calculating standard metrics of valence, and generating word clouds. We love computational methods and we will explore many of them in the coming chapters, but there is no denying it—the quickest, most foolproof way to explore your data is by looking at it. Just open the raw data and spend a minute or two reading through it. This is especially true for language data, since you have been training your whole life to develop an efficient, nuanced understanding of natural language. If your data are short stories, pick two or three at random and read them. If your data are Reddit comments, pick a dozen at random and read them. It is very likely that you will notice interesting patterns, or identify quirks in the data that should be filtered out before further analysis.",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Look at Your Data</span>"
    ]
  },
  {
    "objectID": "quanteda.html",
    "href": "quanteda.html",
    "title": "12  Introduction to Quanteda",
    "section": "",
    "text": "For all analyses involving separating text into smaller units (e.g. words), this book will use the Quanteda family of packages. While it lacks some of the conceptual elegance of the tidyverse, Quanteda is indispensable because of its scope and efficiency—Quanteda functions are faster to write, faster to run, and applicable to a wider variety of uses than any other text analysis framework in R. Beyond the base quanteda package, Quanteda offers a plethora of specialized extensions, for example:\n\n\nquanteda.textstats: Metrics for similarity, readability, lexical diversity, and more\n\nquanteda.dictionaries: Dictionaries for word counting applications\n\nquanteda.sentiment: Tools for advanced dictionary-based sentiment analysis (see Chapter 16)\n\nQuanteda is also well documented. See the Quanteda tutorials webpage for details on file formats and methods not covered in this book.\n\nlibrary(quanteda)\n\nThe home-base of any Quanteda analysis is the corpus. A corpus is a static container holding a library of text documents and associated properties of those documents, called docvars. Later, when we apply complex transformations to our texts (such as splitting them into words), we will need to first store them as a corpus.\nYou can create corpora from a variety of data formats, but we will begin with a dataframe containing our text variable, story.\n\nhippocorpus_df &lt;- read_csv(\"data/hippocorpus-u20220112/hcV3-stories.csv\") |&gt; \n  select(AssignmentId, story, memType, summary, WorkerId, \n         annotatorGender, openness, timeSinceEvent)\n\n#&gt; Rows: 6854 Columns: 23\n#&gt; ── Column specification ────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr (12): AssignmentId, WorkerId, annotatorGender, annotatorRace, mainEvent,...\n#&gt; dbl (11): WorkTimeInSeconds, annotatorAge, distracted, draining, frequency, ...\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(hippocorpus_df)\n\n#&gt; # A tibble: 6 × 8\n#&gt;   AssignmentId           story memType summary WorkerId annotatorGender openness\n#&gt;   &lt;chr&gt;                  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;\n#&gt; 1 32RIADZISTQWI5XIVG5BN… Conc… imagin… My boy… E9TY34YY man                 0   \n#&gt; 2 3018Q3ZVOJCZJFDMPSFXA… The … recall… My boy… 237K2NI1 woman               1   \n#&gt; 3 3IRIK4HM3B6UQBC0HI8Q5… It s… imagin… My sis… FK5QTANB woman               0.5 \n#&gt; 4 3018Q3ZVOJCZJFDMPSFXA… Five… recall… My sis… UYOSBBRS woman               1   \n#&gt; 5 3MTMREQS4W44RBU8OMP3X… Abou… imagin… It is … 34BFLNJV man                 0.25\n#&gt; 6 3018Q3ZVOJCZJFDMPSFXA… Burn… recall… It is … L427B0E0 woman               1   \n#&gt; # ℹ 1 more variable: timeSinceEvent &lt;dbl&gt;\n\n\nTo turn this into a corpus with the corpus() constructor, specify the text variable and a unique identifier by name. All other variables will automatically become docvars.\n\nhippocorpus_corp &lt;- corpus(hippocorpus_df, \n                           docid_field = \"AssignmentId\", \n                           text_field = \"story\")\nhippocorpus_corp\n\n#&gt; Corpus consisting of 6,854 documents and 6 docvars.\n#&gt; 32RIADZISTQWI5XIVG5BN0VMYFRS4U :\n#&gt; \"Concerts are my most favorite thing, and my boyfriend knew i...\"\n#&gt; \n#&gt; 3018Q3ZVOJCZJFDMPSFXATCQ4DARA2 :\n#&gt; \"The day started perfectly, with a great drive up to Denver f...\"\n#&gt; \n#&gt; 3IRIK4HM3B6UQBC0HI8Q5TBJZLEC61 :\n#&gt; \"It seems just like yesterday but today makes five months ago...\"\n#&gt; \n#&gt; 3018Q3ZVOJCZJFDMPSFXATCQG04RAI :\n#&gt; \"Five months ago, my niece and nephew were born.  They are my...\"\n#&gt; \n#&gt; 3MTMREQS4W44RBU8OMP3XSK8NMJAWZ :\n#&gt; \"About a month ago I went to burning man. I was having a hard...\"\n#&gt; \n#&gt; 3018Q3ZVOJCZJFDMPSFXATCQG06AR3 :\n#&gt; \"Burning Man metamorphoses was perfect. I am definitely still...\"\n#&gt; \n#&gt; [ reached max_ndoc ... 6,848 more documents ]",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction to Quanteda</span>"
    ]
  },
  {
    "objectID": "tokenization.html",
    "href": "tokenization.html",
    "title": "13  Tokenization",
    "section": "",
    "text": "13.1 Tokens\nThe first step in the process of turning texts into numbers is almost always tokenization. Tokenization means splitting a long text into smaller pieces, called tokens. Once we have these smaller pieces, we will be able to count how many times each one appears in a text and turn these counts into numeric representations of our texts. There are six main types of tokens: words, stems, lemmas, n-grams, skipgrams, and shingles.",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Tokenization</span>"
    ]
  },
  {
    "objectID": "tokenization.html#tokens",
    "href": "tokenization.html#tokens",
    "title": "13  Tokenization",
    "section": "",
    "text": "13.1.1 Words\nWords are the obvious choice for tokens. The number of possible English sentences is infinite, but the number of possible English words is much smaller—only about 20,000 (Hellman, 2011). Words are especially useful for English, since English words usually do not change their form depending on their place in a sentence, and they are nearly always separated by a space. Separating an English text into words is therefore as simple as splitting it at every space character. This is essentially the default behavior of the Quanteda tokens() function, which takes a character vector or corpus and returns a “tokens” object:\n\ntokens &lt;- tokens(\"20,000 is not so many words\")\ntokens\n\n#&gt; Tokens consisting of 1 document.\n#&gt; text1 :\n#&gt; [1] \"20,000\" \"is\"     \"not\"    \"so\"     \"many\"   \"words\"\n\n\nWe can likewise perform word-based tokenization for the entire corpus that we created in Chapter 12. To remove punctuation before doing so, we can add remove_punct = TRUE.\n\nhippocorpus_tokens &lt;- tokens(hippocorpus_corp,\n                             remove_punct = TRUE)\nprint(hippocorpus_tokens, max_ndoc = 3, max_ntok = 6)\n\n#&gt; Tokens consisting of 6,854 documents and 6 docvars.\n#&gt; 32RIADZISTQWI5XIVG5BN0VMYFRS4U :\n#&gt; [1] \"Concerts\" \"are\"      \"my\"       \"most\"     \"favorite\" \"thing\"   \n#&gt; [ ... and 197 more ]\n#&gt; \n#&gt; 3018Q3ZVOJCZJFDMPSFXATCQ4DARA2 :\n#&gt; [1] \"The\"       \"day\"       \"started\"   \"perfectly\" \"with\"      \"a\"        \n#&gt; [ ... and 177 more ]\n#&gt; \n#&gt; 3IRIK4HM3B6UQBC0HI8Q5TBJZLEC61 :\n#&gt; [1] \"It\"        \"seems\"     \"just\"      \"like\"      \"yesterday\" \"but\"      \n#&gt; [ ... and 260 more ]\n#&gt; \n#&gt; [ reached max_ndoc ... 6,851 more documents ]\n\n\n\n13.1.2 Simplified Words: Stems and Lemmas\nAre “quantify” and “quantified” the same word? What about “quantification”? Treating every new spelling as a separate word can add unnecessary complexity to analysis. It is often useful to have some way to cut away the grammatical aspects of words, leaving only their core, canonical form. Stemming and lemmatization are two ways to do this.\nIn stemming, we chop off prefixes and suffixes from either side of each word, leaving only the “stem”. For example, “quantify” and “quantified” would both be chopped down to “quantif.”\nWe can stem our Hippocorpus tokens with Quanteda’s tokens_wordstem() function. This function uses an updated version of the classic algorithm developed by Porter (1980). The algorithm was carefully designed to remove common suffixes (e.g. “-ic”, “-ate”, “-s”) without cutting off too much (e.g. “generic” and “generate” should register as different stems).\n\nhippocorpus_stems &lt;- hippocorpus_tokens |&gt; \n  tokens_wordstem()\n\nprint(hippocorpus_stems, max_ndoc = 3, max_ntok = 6)\n\n#&gt; Tokens consisting of 6,854 documents and 6 docvars.\n#&gt; 32RIADZISTQWI5XIVG5BN0VMYFRS4U :\n#&gt; [1] \"Concert\" \"are\"     \"my\"      \"most\"    \"favorit\" \"thing\"  \n#&gt; [ ... and 197 more ]\n#&gt; \n#&gt; 3018Q3ZVOJCZJFDMPSFXATCQ4DARA2 :\n#&gt; [1] \"The\"     \"day\"     \"start\"   \"perfect\" \"with\"    \"a\"      \n#&gt; [ ... and 177 more ]\n#&gt; \n#&gt; 3IRIK4HM3B6UQBC0HI8Q5TBJZLEC61 :\n#&gt; [1] \"It\"        \"seem\"      \"just\"      \"like\"      \"yesterday\" \"but\"      \n#&gt; [ ... and 260 more ]\n#&gt; \n#&gt; [ reached max_ndoc ... 6,851 more documents ]\n\n\nNotice that “Concerts” has changed to “Concert”, “favorite” has changed to “favorit”, and “That’s” has changed to “That”.\nLemmatization takes a slightly different approach. Rather than chopping off prefixes and suffixes, lemmatization uses a lookup table to replace variants of words with the version that you might find in a dictionary. For example, “quantified” would become “quantify”. This canonical form is called a lemma.\nWe can lemmatize our Hippocorpus tokens using the lexicon package’s hash_lemmas lemmatization list and Quanteda’s tokens_replace() function.\n\nhippocorpus_lemmas &lt;- hippocorpus_tokens |&gt; \n  tokens_replace(pattern = lexicon::hash_lemmas$token, \n                 replacement = lexicon::hash_lemmas$lemma)\n\nprint(hippocorpus_lemmas, max_ndoc = 3, max_ntok = 6)\n\n#&gt; Tokens consisting of 6,854 documents and 6 docvars.\n#&gt; 32RIADZISTQWI5XIVG5BN0VMYFRS4U :\n#&gt; [1] \"concert\"  \"be\"       \"my\"       \"much\"     \"favorite\" \"thing\"   \n#&gt; [ ... and 197 more ]\n#&gt; \n#&gt; 3018Q3ZVOJCZJFDMPSFXATCQ4DARA2 :\n#&gt; [1] \"The\"       \"day\"       \"start\"     \"perfectly\" \"with\"      \"a\"        \n#&gt; [ ... and 177 more ]\n#&gt; \n#&gt; 3IRIK4HM3B6UQBC0HI8Q5TBJZLEC61 :\n#&gt; [1] \"It\"        \"seem\"      \"just\"      \"like\"      \"yesterday\" \"but\"      \n#&gt; [ ... and 260 more ]\n#&gt; \n#&gt; [ reached max_ndoc ... 6,851 more documents ]\n\n\nNotice that “are” has changed to “be” and “started” has changed to “start”. Notice also that “That’s” has not changed at all. This is because it does not appear in the lookup table we are using.\n\n\n\n\n\n\n\n\nThe Porter algorithm’s choice to stem “quantifications” to “quantif” rather than “quantifi” is a strange one. No stemming or lemmatization algorithm is perfect. Language is complex, and simple rule-based systems are bound to leave things out or make mistakes. Even so, stemming and lemmatization can be good ways to simplify the job when working with words.\n\n13.1.3 N-grams\nWords, stems, and lemmas are great, but they do not stand alone. The sentence “20,000 is not so many words” contains the word “many.” So we can conclude that the sentence is referring to a lot of something, right? Not so fast! Word-based tokenization misses the more representative unit: “not so many.” Because order creates meaning in language, it is often worthwhile to break texts into groups of a few words at a time, called n-grams. Groups of two words are called two-grams (or bigrams), groups of three words like “not so many” are called three-grams (or trigrams), and so on.\nTo get all of the two-grams and three-grams from our example text, we feed the “tokens” object we created already into the tokens_ngrams() function, and specify that we want groups of twos and threes with n = c(2L, 3L).\n\nngrams &lt;- tokens_ngrams(tokens, n = c(2L, 3L))\nngrams\n\n#&gt; Tokens consisting of 1 document.\n#&gt; text1 :\n#&gt; [1] \"20,000_is\"     \"is_not\"        \"not_so\"        \"so_many\"      \n#&gt; [5] \"many_words\"    \"20,000_is_not\" \"is_not_so\"     \"not_so_many\"  \n#&gt; [9] \"so_many_words\"\n\n\n\n13.1.4 Skipgrams\nSometimes the most important combinations of words do not appear right next to each other. For example, it might be more useful to count occurrences of “not many” than it is to count occurrences of “not so many”. We can do this by using skipgrams—n-grams of non-adjacent words. In Quanteda, we can do this again with the tokens_ngrams(), now adding the argument skip = c(0L, 1L) to specify that we want both tokens of immediately adjacent words (0L), and tokens that skip one word in between (1L).\n\nskipgrams &lt;- tokens_ngrams(tokens, n = c(2L, 3L), skip = c(0L, 1L))\nskipgrams\n\n#&gt; Tokens consisting of 1 document.\n#&gt; text1 :\n#&gt;  [1] \"20,000_is\"     \"20,000_not\"    \"is_not\"        \"is_so\"        \n#&gt;  [5] \"not_so\"        \"not_many\"      \"so_many\"       \"so_words\"     \n#&gt;  [9] \"many_words\"    \"20,000_is_not\" \"20,000_is_so\"  \"20,000_not_so\"\n#&gt; [ ... and 9 more ]\n\n\n\n13.1.5 Shingles\nSometimes even individual words are too coarse. For example, the fact that our sentence contains “20,000” might be too specific to be informative, but it might be useful to know that the sentence contains “,000”, which more generally signifies big, round numbers. To get tokens like this, we need to start back at the beginning, this time separating our text into individual characters.\n\nchar_tokens &lt;- tokens(\"20,000 is not so many words\", \n                      what = \"character\")\nchar_tokens\n\n#&gt; Tokens consisting of 1 document.\n#&gt; text1 :\n#&gt;  [1] \"2\" \"0\" \",\" \"0\" \"0\" \"0\" \"i\" \"s\" \"n\" \"o\" \"t\" \"s\"\n#&gt; [ ... and 10 more ]\n\n\nNow we can create groupings of letters, called “shingles,” in the same way that we made n-grams, with the tokens_ngrams() function.\n\nshingles &lt;- tokens_ngrams(char_tokens, n = c(4L))\nshingles\n\n#&gt; Tokens consisting of 1 document.\n#&gt; text1 :\n#&gt;  [1] \"2_0_,_0\" \"0_,_0_0\" \",_0_0_0\" \"0_0_0_i\" \"0_0_i_s\" \"0_i_s_n\" \"i_s_n_o\"\n#&gt;  [8] \"s_n_o_t\" \"n_o_t_s\" \"o_t_s_o\" \"t_s_o_m\" \"s_o_m_a\"\n#&gt; [ ... and 7 more ]",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Tokenization</span>"
    ]
  },
  {
    "objectID": "tokenization.html#sec-custom-preprocessing",
    "href": "tokenization.html#sec-custom-preprocessing",
    "title": "13  Tokenization",
    "section": "\n13.2 Custom Preprocessing",
    "text": "13.2 Custom Preprocessing\nWe have seen above that the tokens() function includes some basic preprocessing capabilities, for example remove_punct to remove punctuation. It also offers remove_numbers, remove_symbols (e.g. ▵, ⏱, ↤, $, ÷, ∬), and remove_url to remove URLs. Nevertheless, certain texts require more specific preprocessing steps. This can be achieved with the stringr package from the tidyverse, by modifying the raw text before turning it into a corpus object. For example, when working with texts from social media we might want to remove “RT” (a conventional indicator that the text was written by another person) with text = str_replace(text, \"RT \", \"\") or user handles (used to refer to specific users) with text = str_replace(text, '@\\\\w+:|@\\\\w+', \" \"), since neither of these are tokens are relevant to the psychological quality of the text. In certain cases, we might be interested in counting user handles without regard for the specific user. In these cases, we might use text = str_replace(text, '@\\\\w+:|@\\\\w+', \"USER_TAG \") so that all user tags get tokenized as “USER_TAG.”",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Tokenization</span>"
    ]
  },
  {
    "objectID": "tokenization.html#sec-quanteda-dfms",
    "href": "tokenization.html#sec-quanteda-dfms",
    "title": "13  Tokenization",
    "section": "\n13.3 Document-Feature Matrices (DFMs)",
    "text": "13.3 Document-Feature Matrices (DFMs)\nOnce we have a suitable tokens object, we can count how many times each token appears in each text. This will provide a basis for comparing those texts later on. Thinking about a text this way—as a collection of tokens with different frequencies—is often referred to as the bag of words approach. When using the bag of words approach, we ignore the order of the words. We imagine that each topic has its characteristic bag of words, and speaking or writing is just a matter of pulling them out of the bag one at a time at random. This assumption is obviously wrong, but it makes it possible to analyze text with straightforward quantitative methods. Often, this is a worthwhile trade-off to make. As we advance to more and more complex methods in the coming chapters, we will find that many methods were designed to solve problems created by the overly simplistic bag of words approach.\nFor now though, let’s start with the basics. The way to represent the number of times each token appears in each document is with a document-feature matrix (DFM). This is a table where each row is a document (i.e. a text in our corpus) and each column is a feature (usually a token). The cells can then contain counts of how many times a particular feature appears in a particular document.\nCreating DFMs in Quanteda is done with the dfm() function. By default, dfm() will also convert all text to lower case. In this default setting, “The” and “the” will register as two instances of the same token.\n\nhippocorpus_dfm &lt;- dfm(hippocorpus_tokens)\nprint(hippocorpus_dfm, max_ndoc = 6, max_nfeat = 6)\n\n#&gt; Document-feature matrix of: 6,854 documents, 27,668 features (99.50% sparse) and 6 docvars.\n#&gt;                                 features\n#&gt; docs                             concerts are my most favorite thing\n#&gt;   32RIADZISTQWI5XIVG5BN0VMYFRS4U        1   1  5    1        2     1\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQ4DARA2        0   0  5    0        0     0\n#&gt;   3IRIK4HM3B6UQBC0HI8Q5TBJZLEC61        0   0 10    3        0     1\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG04RAI        0   1  4    0        0     0\n#&gt;   3MTMREQS4W44RBU8OMP3XSK8NMJAWZ        0   0  5    0        0     0\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG06AR3        0   0  4    0        0     0\n#&gt; [ reached max_ndoc ... 6,848 more documents, reached max_nfeat ... 27,662 more features ]\n\n\nCongratulations! You have turned texts into numbers! Now how do we use these numbers to measure psychological constructs?\n\n\n\n\n\nHellman, A. B. (2011). Vocabulary size and depth of word knowledge in adult-onset second language acquisition. International Journal of Applied Linguistics, 21(2), 162–182.\n\n\nPorter, M. F. (1980). An algorithm for suffix stripping. Program, 14(3), 130–137.",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Tokenization</span>"
    ]
  },
  {
    "objectID": "word-counting.html",
    "href": "word-counting.html",
    "title": "14  Dictionary-Based Word Counts",
    "section": "",
    "text": "14.1 Dictionaries\nA dictionary is a list of words (or other tokens) associated with a given psychological or other construct. For example, a dictionary for depression might include words like “sleepy” and “down.” We can use the dictionary to count construct-related words in each text—texts that use more construct-related words are then assumed to be more construct-related overall.\nLet’s give a more concrete example: Recall that in the Hippocorpus data, the memType variable indicates whether the participant was told to tell a story that happened to them recently (“recalled”), a story that they had already told a few months earlier (“retold”), or an entirely fictional story (“imagined”).\nSap et al. (2022) hypothesized that true autobiographical stories would include more surprising events than imagined stories. To test this hypothesis, we could use a dictionary of surprise-related words. Where could we find such a dictionary? Perhaps we could try making one up?\nsurprise_dict &lt;- dictionary(\n    list(\n      surprise = c(\"surprise\", \"wow\", \"suddenly\", \"bang\")\n    )\n  )\nsurprise_dict\n\n#&gt; Dictionary object with 1 key entry.\n#&gt; - [surprise]:\n#&gt;   - surprise, wow, suddenly, bang\nGenerating a sentiment dictionary is not easy. Luckily, other researchers have done the work for us: The NRC Word-Emotion Association Lexicon (S. M. Mohammad & Turney, 2013; S. Mohammad & Turney, 2010), included in the quanteda.sentiment package, has a list of 534 surprise words.\nsurprise_dict &lt;- quanteda.sentiment::data_dictionary_NRC[\"surprise\"]\nsurprise_dict\n\n#&gt; Dictionary object with 1 key entry.\n#&gt; Polarities: pos = \"positive\"; neg = \"negative\" \n#&gt; - [surprise]:\n#&gt;   - abandonment, abduction, abrupt, accident, accidental, accidentally, accolade, advance, affront, aghast, alarm, alarming, alertness, alerts, allure, amaze, amazingly, ambush, angel, anomaly [ ... and 514 more ]\nThe NRC Word-Emotion Association Lexicon is a crowdsourced dictionary; S. M. Mohammad & Turney (2013) generated it by presenting individual words to thousands of online participants and asking them to rate how much each word is “associated with the emotion surprise.” The final dictionary includes all the words that were consistently reported to be at least moderately associated with surprise.",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Dictionary-Based Word Counts</span>"
    ]
  },
  {
    "objectID": "word-counting.html#sec-understand-your-dictionary",
    "href": "word-counting.html#sec-understand-your-dictionary",
    "title": "14  Dictionary-Based Word Counts",
    "section": "\n14.2 Understand Your Dictionary",
    "text": "14.2 Understand Your Dictionary\nIn Chapter 11, we emphasized the importance of reading through your data before conducting any analyses. The same is true for dictionaries: Before using any dictionary-based methods, always look through your dictionary and ask yourself two questions:\n\nHow was my dictionary constructed?\nHow context-dependent are the words in my dictionary?\n\nLet’s expand on each of these questions.\n\n14.2.1 How Was Your Dictionary Constructed?\nThe surprise dictionary we are using was generated by asking participants how much each word was “associated with the emotion surprise” (S. M. Mohammad & Turney, 2013). A word can be “associated with” surprise because it reflects surprise (e.g. “suddenly”), but it can also be “associated with” surprise because it reflects the exact opposite of surprise. Indeed, if we look through the dictionary, we find words like “leisure” and “lovely”.\n\nset.seed(8)\nsample(surprise_dict$surprise, 20)\n\n#&gt;  [1] \"outburst\"    \"godsend\"     \"alarming\"    \"intense\"     \"lawsuit\"    \n#&gt;  [6] \"leisure\"     \"scrimmage\"   \"curiosity\"   \"reappear\"    \"placard\"    \n#&gt; [11] \"diversion\"   \"receiving\"   \"thirst\"      \"lovely\"      \"frenetic\"   \n#&gt; [16] \"perfection\"  \"playground\"  \"fearfully\"   \"guess\"       \"unfulfilled\"\n\n\nThis means that we are not, in fact, measuring how surprising each story is. At best, we are measuring how much each story deals with surprise (or lack thereof) one way or another.\nAs you look through your dictionary, make sure you are aware of the process used to construct the dictionary. If it was generated by asking participants about individual words, how was the question formulated? How might that question have been interpreted by the participants?\n\n14.2.2 How Context-Dependent are the Words in Your Dictionary?\nThe participants generating our dictionary were asked about one word at a time. People presented words out of context often fail to consider how words are actually used in natural discourse. For example, imagine that you are an online participant, and you are asked about your associations with the word “guess”. Seeing “guess” by itself might sound like an imperative, calling to mind a situation in which someone is asking you to guess something about which you are unsure—perhaps a game show. Since this sort of situation generally results in a surprise when the truth is revealed, you report that “guess” is associated with surprise. In fact, though, “guess” is much more frequently used in the phrase “I guess”, which signifies reluctance and has very little to do with surprise. We can check how “guess” is used our corpus by using Quanteda’s kwic() function, which gives a dataframe of Key Words In Context (KIWC).\n\nhippocorpus_tokens |&gt; \n  kwic(\"guess\") |&gt; \n  mutate(text = paste(pre, keyword, post)) |&gt; \n  pull(text)\n\n#&gt; [1] \"his 30th birthday and I guess that's why he decided to\"            \n#&gt; [2] \"healthier after a month I guess it was the stress of\"              \n#&gt; [3] \"already made cake So i guess it wasn't that bad\"                   \n#&gt; [4] \"wrong Was she serious I guess so When I finished packing\"          \n#&gt; [5] \"up our unit And I guess that's it I never saw\"                     \n#&gt; [6] \"I'm not sure yet I guess I will see how the\"                       \n#&gt; [7] \"FINALLY got admitted D I guess all those crazy contractions worked\"\n#&gt; [8] \"we made it safely I guess even the car got tired\"\n\n\nWith the possible exception of #6, none of these examples give the impression of an impending surprise. Nevertheless, “guess” does appear in the NRC surprise dictionary.\nAs you look through your dictionary, think about how each word might really be used in context. Are there ways to use the word that do not have to do with your construct?",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Dictionary-Based Word Counts</span>"
    ]
  },
  {
    "objectID": "word-counting.html#raw-word-counts",
    "href": "word-counting.html#raw-word-counts",
    "title": "14  Dictionary-Based Word Counts",
    "section": "\n14.3 Raw Word Counts",
    "text": "14.3 Raw Word Counts\nAt this point, you might be pretty skeptical about using the NRC surprise dictionary to measure surprise. Even so, let’s try it out. To count how many times surprise words appear in each of our texts, we use the dfm_lookup() function.\n\nhippocorpus_surprise &lt;- hippocorpus_dfm |&gt; \n  dfm_lookup(surprise_dict)\n\nhippocorpus_surprise\n\n#&gt; Document-feature matrix of: 6,854 documents, 1 feature (5.09% sparse) and 6 docvars.\n#&gt;                                 features\n#&gt; docs                             surprise\n#&gt;   32RIADZISTQWI5XIVG5BN0VMYFRS4U        2\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQ4DARA2        0\n#&gt;   3IRIK4HM3B6UQBC0HI8Q5TBJZLEC61        4\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG04RAI        3\n#&gt;   3MTMREQS4W44RBU8OMP3XSK8NMJAWZ        4\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG06AR3        6\n#&gt; [ reached max_ndoc ... 6,848 more documents ]\n\n\n\n14.3.1 Modeling Raw Word Counts\nRecall that we wanted to test whether true autobiographical stories include more surprise than imagined stories. Now that we have counted the number of surprise words in each document, how do we test our hypothesis?\nA good first step is to reattach the word counts to our original corpus. As we do this, we convert both to dataframes.\n\nhippocorpus_surprise_df &lt;- hippocorpus_surprise |&gt; \n  convert(\"data.frame\") |&gt; # convert to dataframe\n  right_join(\n    hippocorpus_corp |&gt; \n      convert(\"data.frame\") # convert to dataframe\n    )\n\nIt makes sense to control for the total number of words in each text, since longer texts have more opportunities to use surprise words1. To count the total number of tokens in each text, we can use the ntoken() function on our DFM and add the result directly to the new dataframe.\n\nhippocorpus_surprise_df &lt;- hippocorpus_surprise_df |&gt; \n  mutate(wc = ntoken(hippocorpus_dfm))\n\nWe are now ready for modeling! When your dependent variable is a count of words, we recommend using negative binomial regression, available in R with the MASS package2. For extra sensitivity to the variable rates at which word frequencies grow with text length (see Baayen, 2001), we include wc as a both a predictor and an offset offset(log(wc)) in the regression (an offset is just a predictor with its parameter at 1). We use log() to account for the fact that negative binomial regression links the predictors with the outcome variable through a log link. This means that including offset(log(wc)) is equivalent to modeling the ratio of surprise words to total words (for a more detailed explanation of this dynamic, see the discussion here).\n\nsurprise_mod &lt;- MASS::glm.nb(surprise ~ memType + wc + offset(log(wc)),\n                             data = hippocorpus_surprise_df)\nsummary(surprise_mod)\n\n#&gt; \n#&gt; Call:\n#&gt; MASS::glm.nb(formula = surprise ~ memType + wc + offset(log(wc)), \n#&gt;     data = hippocorpus_surprise_df, init.theta = 6.070929358, \n#&gt;     link = log)\n#&gt; \n#&gt; Coefficients:\n#&gt;                   Estimate Std. Error  z value Pr(&gt;|z|)    \n#&gt; (Intercept)     -3.9065113  0.0258623 -151.050  &lt; 2e-16 ***\n#&gt; memTyperecalled -0.0324360  0.0176595   -1.837  0.06625 .  \n#&gt; memTyperetold   -0.0614152  0.0219399   -2.799  0.00512 ** \n#&gt; wc              -0.0008833  0.0000876  -10.082  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for Negative Binomial(6.0709) family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 7490.2  on 6853  degrees of freedom\n#&gt; Residual deviance: 7370.5  on 6850  degrees of freedom\n#&gt; AIC: 30997\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 1\n#&gt; \n#&gt; \n#&gt;               Theta:  6.071 \n#&gt;           Std. Err.:  0.270 \n#&gt; \n#&gt;  2 x log-likelihood:  -30987.333\n\n\nLooking at the p-values for the coefficients, we see that there was no significant difference between recalled and imagined stories (p = 0.066). There was, however, a significant difference between retold and imagined stories, such that retold stories used fewer surprise words (p = 0.005).\nAn example of using raw word counts in research: Simchon et al. (2023) collected Twitter activity over a three month period from over 2.7 million users. Using a dictionary, they then counted the number of passive auxiliary verbs (e.g. “they were analyzed”; “my homework will be completed”) in each user’s activity. They found that users with more followers (indicating higher social status) used much fewer passive auxiliary verbs, controlling for total word count.",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Dictionary-Based Word Counts</span>"
    ]
  },
  {
    "objectID": "word-counting.html#sec-polarity",
    "href": "word-counting.html#sec-polarity",
    "title": "14  Dictionary-Based Word Counts",
    "section": "\n14.4 Polarity",
    "text": "14.4 Polarity\nHow can we improve our measurement of surprise? As we saw above, one problem with the dictionary approach is that a word might be associated with a construct because it reflects the opposite of that construct. One solution to this problem is to measure the ratio between the target dictionary and its opposite. In sentiment analysis, this approach is called polarity. Polarity is most commonly used to analyze the overall valence of a text by comparing positive words (e.g. “happy”, “great”) with negative words (e.g. “disappointed”, “terrible”). In principle though, we can use it to compare any sort of opposites.\nWhat is the opposite of surprise? Plutchik (1962) argues that the opposite of surprise is anticipation. Luckily, the NRC Word-Emotion Association Lexicon also includes a dictionary of anticipation-associated words. Using this dictionary, we can measure how much a text is associated with surprise as opposed to anticipation.\nQuanteda’s built-in function for polarity is textstat_polarity(). To use this function, we first have to set the “positive” and “negative” polarities of the dictionary, and then call textstat_polarity() on our DFM. By default, this outputs the log ratio of positive to negative counts for each document:\n\nlibrary(quanteda.sentiment)\n\n#&gt; \n#&gt; Attaching package: 'quanteda.sentiment'\n\n\n#&gt; The following object is masked from 'package:quanteda':\n#&gt; \n#&gt;     data_dictionary_LSD2015\n\n# subset dictionary\nsurprise_anticipation_dict &lt;- data_dictionary_NRC[c(\"surprise\", \"anticipation\")]\n\n# set surprise and anticipation as polarity\npolarity(surprise_anticipation_dict) &lt;- list(pos = \"surprise\", neg = \"anticipation\")\n\n# get polarity\nhippocorpus_surprise_polarity &lt;- \n  textstat_polarity(hippocorpus_dfm, surprise_anticipation_dict) |&gt; \n  rename(surprise_vs_anticipation = sentiment)\n\nWhile textstat_polarity() can sometimes be useful for visualizations or downstream analyses, it is not helpful for modeling polarity as an outcome variable.\n\n14.4.1 Modeling Polarity\nTo test whether true autobiographical stories include more surprise relative to anticipation than imagined stories, we first count the surprise and anticipation words in each document, and rejoin the results to the full dataset.\n\n# count surprise/anticipation words\nhippocorpus_surprise_anticipation &lt;- hippocorpus_dfm |&gt; \n  dfm_lookup(surprise_anticipation_dict)\n\n# convert to dataframe and join to full data\nhippocorpus_surprise_anticipation_df &lt;- \n  hippocorpus_surprise_anticipation |&gt; \n  convert(\"data.frame\") |&gt; \n  right_join(\n    hippocorpus_corp |&gt; \n      convert(\"data.frame\") # convert to dataframe\n    ) |&gt; \n  mutate(wc = ntoken(hippocorpus_dfm))\n\n#&gt; Joining with `by = join_by(doc_id)`\n\n\nSince we are still modelling word counts as an output, we again use negative binomial regression. Rather than controlling for the total word count, however, we can control for the total number of surprise words plus the number of anticipation words. Because of the log link function (along with the endlessly useful properties of logarithms) entering this sum as a log offset (offset(log(surprise + anticipation))) is equivalent to modeling the ratio of surprise-related to anticipation-related words.\n\n# remove zeros to prevent divide by zero errors\nhippocorpus_surprise_anticipation_df &lt;- \n  hippocorpus_surprise_anticipation_df |&gt; \n    filter(surprise + anticipation &gt; 0)\n\nset.seed(2024)\nsurprise_anticipation_mod &lt;- MASS::glm.nb(\n  surprise ~ memType + wc + offset(log(surprise + anticipation)),\n  data = hippocorpus_surprise_anticipation_df,\n  # increase iterations to ensure model converges\n  control = glm.control(maxit = 10000) \n  )\n\nsummary(surprise_anticipation_mod)\n\n#&gt; \n#&gt; Call:\n#&gt; MASS::glm.nb(formula = surprise ~ memType + wc + offset(log(surprise + \n#&gt;     anticipation)), data = hippocorpus_surprise_anticipation_df, \n#&gt;     control = glm.control(maxit = 10000), init.theta = 2.949221746e+17, \n#&gt;     link = log)\n#&gt; \n#&gt; Coefficients:\n#&gt;                   Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)     -1.107e+00  1.990e-02 -55.659   &lt;2e-16 ***\n#&gt; memTyperecalled -1.128e-02  1.356e-02  -0.831    0.406    \n#&gt; memTyperetold   -1.966e-02  1.697e-02  -1.158    0.247    \n#&gt; wc              -5.675e-05  6.462e-05  -0.878    0.380    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for Negative Binomial(2.949222e+17) family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 4884.6  on 6843  degrees of freedom\n#&gt; Residual deviance: 4882.1  on 6840  degrees of freedom\n#&gt; AIC: 10\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 1\n#&gt; \n#&gt; \n#&gt;               Theta:  2.949222e+17 \n#&gt;           Std. Err.:  6.158994e+14 \n#&gt; \n#&gt;  2 x log-likelihood:  0\n\n\nThere is no significant difference between true and imagined stories in the ratio of surprise to anticipation words.",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Dictionary-Based Word Counts</span>"
    ]
  },
  {
    "objectID": "word-counting.html#sec-word-scoring",
    "href": "word-counting.html#sec-word-scoring",
    "title": "14  Dictionary-Based Word Counts",
    "section": "\n14.5 Lexical Norms",
    "text": "14.5 Lexical Norms\nSo far we have covered raw word counts, which use one list of words to represent a construct, and we have covered polarities, which use two lists of words to represent a construct and its opposite. The third and final dictionary-based method takes a more nuanced approach than either of these: In lexical norms, words are allowed to represent the construct or its opposite to continuously varying degrees, represented by numbers on a scale. In quanteda.sentiment, this scale is called “valence”, though elsewhere it can be called “lexical affinity” or “lexical association”.\nThe same group that created the NRC Word-Emotion Association Lexicon also created a parallel dictionary with continuous scores: the NRC Hashtag Emotion Lexicon (S. M. Mohammad & Kiritchenko, 2015). Whereas the NRC Word-Emotion Association Lexicon was crowdsourced, the NRC Hashtag Emotion Lexicon was generated algorithmically from a corpus of Twitter posts which contained hashtags like “#anger” and “#surprise”. The dictionary includes the words that were most predictive of each hashtag, with scores indicating the strength of their statistical connection with the category (higher score indicates more representative). We can access the NRC Hashtag surprise dictionary from Github:\n\npath &lt;- \"https://raw.githubusercontent.com/bwang482/emotionannotate/master/lexicons/NRC-Hashtag-Emotion-Lexicon-v0.2.txt\"\n\nhashtag &lt;- read_tsv(path, col_names = c(\"emotion\", \"token\", \"score\"))\n\n#&gt; Rows: 32389 Columns: 3\n#&gt; ── Column specification ────────────────────────────────────────────────────────\n#&gt; Delimiter: \"\\t\"\n#&gt; chr (2): emotion, token\n#&gt; dbl (1): score\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhashtag |&gt; \n  filter(emotion == \"surprise\") |&gt; \n  head()\n\n#&gt; # A tibble: 6 × 3\n#&gt;   emotion  token         score\n#&gt;   &lt;chr&gt;    &lt;chr&gt;         &lt;dbl&gt;\n#&gt; 1 surprise yada           1.49\n#&gt; 2 surprise #preoccupied   1.49\n#&gt; 3 surprise jaden          1.49\n#&gt; 4 surprise #easilyamused  1.49\n#&gt; 5 surprise #needtofocus   1.49\n#&gt; 6 surprise #amazement     1.49\n\n# Create dictionary\nsurprise_dict_hashtag &lt;- dictionary(\n  list(surprise = hashtag$token[hashtag$emotion == \"surprise\"])\n)\n\n# Set dictionary valence\nvalence(surprise_dict_hashtag) &lt;- list(\n  surprise = hashtag$score[hashtag$emotion == \"surprise\"]\n  )\n\nTo measure suprise in the Hippocorpus data, we find the suprise score of each token and compute the average score for the tokens of each document. With quanteda.sentiment, we can do this by calling the textstat_valence() function on our DFM. Since a score of zero in the NRC Hashtag Emotion Lexicon represents zero surprise, we will add normalization = \"all\" to code non-dictionary words as zero by default.\n\n# compute valence\nhippocorpus_valence &lt;- textstat_valence(\n  hippocorpus_dfm, # data\n  surprise_dict_hashtag, # dictionary\n  normalization = \"all\"\n  )\n\n# rejoin to original data\nhippocorpus_valence &lt;- hippocorpus_valence |&gt; \n  rename(surprise = sentiment) |&gt; \n  right_join(\n    hippocorpus_corp |&gt; \n      convert(\"data.frame\") # convert to dataframe\n    )\n\n\n14.5.1 Modeling Norms\nNorm scores, unlike raw word counts and polarities, can be reasonably modeled using standard linear regression. Furthermore, because the score is an average rather than a sum or count, there is no need to control for total word count. Let’s test one more time whether true autobiographical stories include more surprise-related language than imagined stories:\n\nsurprise_score_mod &lt;- lm(surprise ~ memType, hippocorpus_valence)\n\nsummary(surprise_score_mod)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = surprise ~ memType, data = hippocorpus_valence)\n#&gt; \n#&gt; Residuals:\n#&gt;       Min        1Q    Median        3Q       Max \n#&gt; -0.085708 -0.015726 -0.000448  0.015093  0.104459 \n#&gt; \n#&gt; Coefficients:\n#&gt;                  Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)     0.1402018  0.0004433 316.300  &lt; 2e-16 ***\n#&gt; memTyperecalled 0.0029688  0.0006256   4.746 2.12e-06 ***\n#&gt; memTyperetold   0.0021648  0.0007791   2.779  0.00548 ** \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.02327 on 6851 degrees of freedom\n#&gt; Multiple R-squared:  0.003406,   Adjusted R-squared:  0.003116 \n#&gt; F-statistic: 11.71 on 2 and 6851 DF,  p-value: 8.388e-06\n\n\nWe found a significant difference between recalled and imagined stories (p &lt; .001), such that recalled stories have more surprise-related language! This supports Sap et al.’s hypothesis that true autobiographical stories would include more surprising events than imagined stories. The new model also indicated a significant difference between retold and imagined stories, such that retold stories used more surprise-related language—the opposite direction relative to our original finding with the crowdsourced dictionary (p = 0.005).",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Dictionary-Based Word Counts</span>"
    ]
  },
  {
    "objectID": "word-counting.html#sec-dictionary-sources",
    "href": "word-counting.html#sec-dictionary-sources",
    "title": "14  Dictionary-Based Word Counts",
    "section": "\n14.6 Sources of Dictionaries",
    "text": "14.6 Sources of Dictionaries\nSo far we have seen the NRC Word-Emotion Association Lexicon, which used a crowdsourcing approach to generate the dictionary, and the NRC Hashtag Emotion Lexicon, which used a corpus-based approach, relying on hashtags for labeling. Crowdsourcing and algorithmic corpus-based generation are far from the only ways to generate a dictionary. Here we review various types of dictionaries and where to find them.\n\n14.6.1 Crowdsourced Dictionaries\nBesides the surprise dictionary, the NRC Word-Emotion Association Lexicon includes dictionaries for anger, fear, anticipation, trust, sadness, joy, and disgust. The same group has also produced other crowdsourced emotion dictionaries:\n\n\nNRC VAD (S. M. Mohammad, 2018a) contains 20,007 words with ratings between 0 and 1 for valence, arousal and dominance.\n\nNRC Affect Intensity (S. M. Mohammad, 2018b) contains 4192 words with ratings between 0 and 1 for anger, fear, sadness and joy.\n\nPsychologists have used crowdsourcing questionnaires to create dictionaries (especially norms) for decades. As such, crowdsourced dictionaries exist for many psychologically interesting constructs:\n\n\nBrysbaert et al. (2014) used an internet questionnaire to obtain norms for concreteness (i.e. the extent to which a word refers to a perceptible entity). The result, including nearly 40,000 words and 2-grams, is available as an Excel file here.\n\nKuperman et al. (2012) asked participants at what age they learned each word, resulting in age-of-acquisition norms for 30,000 English words.\n\nWarriner et al. (2013) crowdsourced norms for valence, arousal, and dominance, expanding on the ANEW dictionary included in quanteda.sentiment. The expanded norms are available as a zip file here.\n\nStadthagen-Gonzalez & Davis (2006) collected norms for age-of-acquisition, familiarity, and imageability (the ease with which a word evokes mental images) by surveying undergraduates.\n\nDiveica et al. (2023) asked online participants to rate the social relevance of words. The resulting “socialness” norms are available here.\n\n14.6.2 Expert-Generated Dictionaries\nWords are used in many contexts, sometimes with many possible meanings. To take these into account, some groups rely on experts to generate their dictionaries. By far the most prominent collection of expert-generated dictionaries is LIWC (pronounced “Luke”), which includes word lists for grammatical patterns, emotional content, cognitive processes, and more. With its rigorous approach, LIWC has dominated the field of dictionary-based analysis in psychology for decades. The most recent version of LIWC (Boyd et al., 2022) was generated by a team of experts who went through numerous stages of brainstorming, voting, and reliability analysis before arriving at the final word lists.\n\n14.6.3 Corpus-Based Dictionaries\nHuman raters are much better at judging full texts than individual words. Corpus-based dictionaries take advantage of this by extracting their word lists from corpora of full texts that have been rated by humans. We have already seen the NRC Hashtag Emotion Lexicon (S. M. Mohammad & Kiritchenko, 2015), which used Twitter hashtags to gather a corpus of Tweets labeled with emotions by their original authors. A more classic example of corpus-based dictionary generation is Rao et al. (2014), who used a corpus of 1,246 news headlines, each rated manually for anger, disgust, fear, joy, sad and surprise on a scale from 0 to 100 (Strapparava & Mihalcea, 2007). By correlating these ratings with frequencies of words (see Chapter 15), they extracted the words that were most representative of high ratings in each category. Araque et al. (2018) used a similar technique to create DepecheMood, which includes ratings for each word on eight emotional dimensions: afraid, amused, angry, annoyed, don’t care, happy, inspired, and sad. This base dictionary was updated with additional resources by Badaro et al. (2018) to create EmoWordNet, which can be accessed through the Internet Archive.\nMany statistical techniques have been used to extract dictionaries from labeled corpora, some of which will be covered briefly in Chapter 15 and Chapter 18 of this book. For a recent review of methods, see Bandhakavi et al. (2021).\n\n14.6.4 Other Approaches to Dictionary Generation\n\nThesaurus Mining: Strapparava & Valitutti (2004) started with a short list of strongly affect-related words (e.g. “anger”, “doubt”, “cry”), and used WordNet, a database of conceptual relations between words, to find close synonyms of the original words on the list. The result was WordNet Affect. Strapparava & Mihalcea (2007) used WordNet Affect to generate short lists of words associated with anger, disgust, fear, joy, sadness, and surprise, downloadable from here.\nDecontextualized Embeddings: In Chapter 18, we will cover a family of methods for measuring the similarities between words based on how frequently they appear together in text: decontextualized embeddings. These methods can be used on their own for measuring psychological constructs, but they can also be used as a tool for building dictionaries. For example, Buechel et al. (2020) started with a small seed lexicon and used word embeddings (Section 18.3) to find other words that are likely to appear in texts of the same topic. The result—including dictionaries for valence, arousal, dominance, joy anger, sadness, fear, and disgust—is available for download online.\nCombined Methods: Vegt et al. (2021) used a combination of expert input, thesaurus data from WordNet, word embeddings (Section 18.3), and crowdsourcing from online participants to generate norms for numerous constructs associated with grievance-fueled violence (e.g. desperation, fixation, frustration, hate, weapons). The final product is available here.\n\n\n\n\n\n\n\n\nAdvantages of Dictionary-Based Word Counts\n\n\n\n\n\nEfficient Processing: Counting is a simple operation for computers. For very large datasets, this can make a big difference.\n\nEasy to Interpret: Dictionaries for sentiment analysis are usually not more than a few hundred words long. This means that they are easy to read through and understand intuitively. The intuitive appeal is also good for explaining your research to others—“we counted the number of anger-related words” is a method that any non-expert can understand.\n\n\n\n\n\n\n\n\n\nDisadvantages of Dictionary-Based Word Counts\n\n\n\n\n\nNo Context: Dictionary-based word counts treat texts as bags of words. This means they entirely ignore word order (aside from the order of any n-grams that might be included in the dictionary).\n\nMay Reflect Various Constructs: Dictionaries are often generated by asking participants to identify associations with words. These associations do not necessarily reflect the construct in which the researcher is interested.\n\nUnnuanced: Words are either in a dictionary or they are not. Raw counts carry no nuance about the varying degrees to which different words may reflect the construct of interest. Norms can fix this problem, but are not available for many psychological dimensions.\n\nUnnaturalistic Generation Process: Dictionaries are generally crowdsourced by asking participants to report their associations with individual words. People presented words out of context often fail to consider how words are actually used in natural discourse.\n\nLimited Dictionaries Available: Dictionaries are expensive and labor intensive to produce. Researchers are generally reliant on dictionaries already produced by other teams, which may not reflect the construct of interest precisely.\n\n\n\n\n\n\n\n\nAraque, O., Gatti, L., Staiano, J., & Guerini, M. (2018). DepecheMood++: A bilingual emotion lexicon built through simple yet powerful techniques. CoRR, abs/1810.03660. http://arxiv.org/abs/1810.03660\n\n\nBaayen, R. H. (2001). Word frequency distributions. Springer Netherlands. https://link.springer.com/book/10.1007/978-94-010-0844-0\n\n\nBadaro, G., Jundi, H., Hajj, H., & El-Hajj, W. (2018). EmoWordNet: Automatic expansion of emotion lexicon using English WordNet. In M. Nissim, J. Berant, & A. Lenci (Eds.), Proceedings of the seventh joint conference on lexical and computational semantics (pp. 86–93). Association for Computational Linguistics. https://doi.org/10.18653/v1/S18-2009\n\n\nBandhakavi, A., Wiratunga, N., Massie, S., & P., D. (2021). Emotion‐aware polarity lexicons for twitter sentiment analysis. Expert Systems, 38(7).\n\n\nBoyd, R., Ashokkumar, A., Seraj, S., & Pennebaker, J. (2022). The development and psychometric properties of LIWC-22. https://doi.org/10.13140/RG.2.2.23890.43205\n\n\nBrysbaert, M., Warriner, A. B., & Kuperman, V. (2014). Concreteness ratings for 40 thousand generally known english word lemmas. Behavior Research Methods, 46, 904–911.\n\n\nBuechel, S., Rücker, S., & Hahn, U. (2020). Learning and evaluating emotion lexicons for 91 languages. In D. Jurafsky, J. Chai, N. Schluter, & J. Tetreault (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics (pp. 1202–1217). Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.acl-main.112\n\n\nDiveica, V., Pexman, P. M., & Binney, R. J. (2023). Quantifying social semantics: An inclusive definition of socialness and ratings for 8388 english words. Behavior Research Methods, 55(2), 461–473.\n\n\nKuperman, V., Stadthagen-Gonzalez, H., & Brysbaert, M. (2012). Age-of-acquisition ratings for 30,000 english words. Behavior Research Methods, 44, 978–990.\n\n\nMohammad, S. M. (2018a). Obtaining reliable human ratings of valence, arousal, and dominance for 20,000 english words. Proceedings of the Annual Conference of the Association for Computational Linguistics (ACL).\n\n\nMohammad, S. M. (2018b). Word affect intensities. Proceedings of the 11th Edition of the Language Resources and Evaluation Conference (LREC-2018).\n\n\nMohammad, S. M., & Kiritchenko, S. (2015). Using hashtags to capture fine emotion categories from tweets. Computational Intelligence, 31, 301–326. https://api.semanticscholar.org/CorpusID:2498838\n\n\nMohammad, S. M., & Turney, P. D. (2013). Crowdsourcing a word-emotion association lexicon. Computational Intelligence, 29(3), 436–465.\n\n\nMohammad, S., & Turney, P. (2010). Emotions evoked by common words and phrases: Using Mechanical Turk to create an emotion lexicon. Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, 26–34. https://aclanthology.org/W10-0204\n\n\nPlutchik, R. (1962). The emotions. Random House.\n\n\nRao, Y., Lei, J., Wenyin, L., Li, Q., & Chen, M. (2014). Building emotional dictionary for sentiment analysis of online news. World Wide Web (Bussum), 17(4), 723–742.\n\n\nSap, M., Jafarpour, A., Choi, Y., Smith, N. A., Pennebaker, J. W., & Horvitz, E. (2022). Quantifying the narrative flow of imagined versus autobiographical stories. Proceedings of the National Academy of Sciences, 119(45), e2211715119. https://doi.org/10.1073/pnas.2211715119\n\n\nSimchon, A., Hadar, B., & Gilead, M. (2023). A computational text analysis investigation of the relation between personal and linguistic agency. Communications Psychology, 1–9. https://doi.org/10.1038/s44271-023-00020-1\n\n\nStadthagen-Gonzalez, H., & Davis, C. J. (2006). The bristol norms for age of acquisition, imageability, and familiarity. Behavior Research Methods, 38(4), 598–605.\n\n\nStrapparava, C., & Mihalcea, R. (2007). SemEval-2007 task 14: Affective text. In E. Agirre, L. Màrquez, & R. Wicentowski (Eds.), Proceedings of the fourth international workshop on semantic evaluations (SemEval-2007) (pp. 70–74). Association for Computational Linguistics. https://aclanthology.org/S07-1013\n\n\nStrapparava, C., & Valitutti, A. (2004). Wordnet affect: An affective extension of wordnet. Lrec, 4, 40.\n\n\nVegt, I. van der, Mozes, M., Kleinberg, B., & Gill, P. (2021). The grievance dictionary: Understanding threatening language use. Behavior Research Methods, 1–15.\n\n\nWarriner, A. B., Kuperman, V., & Brysbaert, M. (2013). Norms of valence, arousal, and dominance for 13,915 english lemmas. Behavior Research Methods, 45, 1191–1207.",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Dictionary-Based Word Counts</span>"
    ]
  },
  {
    "objectID": "word-counting.html#footnotes",
    "href": "word-counting.html#footnotes",
    "title": "14  Dictionary-Based Word Counts",
    "section": "",
    "text": "We use total word count here for the sake of the example, but total word count may not always be the appropriate measure of text length. For example, you may want to measure the amount of surprise relative to other emotional content. In this case, it would be more appropriate to control for the total number of emotion-related words, as opposed to the total word count. Similarly, if you were measuring the number of first person singular pronouns, you may want to control for the total number of pronouns rather than the total word count.↩︎\nWe use a simple count of words as the dependent variable here, but keep in mind that it may be more appropriate to apply a transformation such as Simple Good-Turing frequency estimation (Section 16.6).↩︎",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Dictionary-Based Word Counts</span>"
    ]
  },
  {
    "objectID": "dla.html",
    "href": "dla.html",
    "title": "15  Open Vocabulary Word Counting",
    "section": "",
    "text": "15.1 Frequency Ratios\nThe most intuitive way to compare texts from two groups is one we already explored in the context of data visualization in Chapter 5: frequency ratios. To get frequency ratios from a Quanteda DFM, we can use the textstat_frequency() function from the quanteda.textstats package, with the groups parameter set to the categorical variable of interest. Let’s compare true stories from fictional ones in the Hippocorpus data.\nlibrary(quanteda.textstats)\n\nimagined_vs_recalled &lt;- hippocorpus_dfm |&gt; \n  textstat_frequency(groups = memType)\n\nhead(imagined_vs_recalled)\n\n#&gt;   feature frequency rank docfreq    group\n#&gt; 1       i     32080    1    2686 imagined\n#&gt; 2     the     25826    2    2742 imagined\n#&gt; 3      to     24104    3    2746 imagined\n#&gt; 4     and     20847    4    2707 imagined\n#&gt; 5       a     16945    5    2714 imagined\n#&gt; 6     was     15405    6    2618 imagined\nIn the resulting dataframe, each row represents one feature within each category. frequency is the number of times the feature appears in the group, rank is the ordering from highest to lowest frequency within each group, and docfreq is the number of documents in the group in which the feature appears at least once. To compare imagined stories to recalled ones, we can calculate frequency ratios.\nimagined_vs_recalled &lt;- imagined_vs_recalled |&gt; \n  filter(group %in% c(\"imagined\", \"recalled\")) |&gt; \n  pivot_wider(id_cols = \"feature\", \n              names_from = \"group\", \n              values_from = \"frequency\",\n              names_prefix = \"count_\") |&gt; \n  mutate(freq_imagined = count_imagined/sum(count_imagined, na.rm = TRUE),\n         freq_recalled = count_recalled/sum(count_recalled, na.rm = TRUE),\n         imagined_freq_ratio = freq_imagined/freq_recalled)\n\nhead(imagined_vs_recalled)\n\n#&gt; # A tibble: 6 × 6\n#&gt;   feature count_imagined count_recalled freq_imagined freq_recalled\n#&gt;   &lt;chr&gt;            &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n#&gt; 1 i                32080          32906        0.0475        0.0426\n#&gt; 2 the              25826          31517        0.0383        0.0408\n#&gt; 3 to               24104          27379        0.0357        0.0354\n#&gt; 4 and              20847          26447        0.0309        0.0342\n#&gt; 5 a                16945          19850        0.0251        0.0257\n#&gt; 6 was              15405          19484        0.0228        0.0252\n#&gt; # ℹ 1 more variable: imagined_freq_ratio &lt;dbl&gt;\nWe can now plot a rotated F/F plot, as in Chapter 5.\nlibrary(ggiraph, verbose = FALSE)\nlibrary(ggrepel)\n\nset.seed(2023)\np &lt;- imagined_vs_recalled |&gt; \n  mutate(\n    # calculate total frequency\n    common = (freq_imagined + freq_recalled)/2,\n    # remove single quotes (for html)\n    feature = str_replace_all(feature, \"'\", \"`\")) |&gt; \n  ggplot(aes(imagined_freq_ratio, common, \n             label = feature,\n             color = imagined_freq_ratio,\n             tooltip = feature, \n             data_id = feature\n             )) +\n    geom_point_interactive() +\n    geom_text_repel_interactive(size = 2) +\n    scale_y_continuous(\n      trans = \"log2\", breaks = ~.x,\n      minor_breaks = ~2^(seq(0, log2(.x[2]))),\n      labels = c(\"Rare\", \"Common\")\n      ) +   \n    scale_x_continuous(\n      trans = \"log10\", limits = c(1/10,10),\n      breaks = c(1/10, 1, 10),\n      labels = c(\"10x More Common\\nin Recalled Stories\",\n                 \"Equal Proportion\",\n                 \"10x More Common\\nin Imagined Stories\")\n      ) +\n    scale_color_gradientn(\n      colors = c(\"#023903\", \n                 \"#318232\",\n                 \"#E2E2E2\", \n                 \"#9B59A7\",\n                 \"#492050\"), \n      trans = \"log2\", # log scale for ratios\n      guide = \"none\"\n      ) +\n    labs(\n      title = \"Words in Imagined and Recalled Stories\",\n      x = \"\",\n      y = \"Total Frequency\",\n      color = \"\"\n    ) +\n    # fixed coordinates since x and y use the same units\n    coord_fixed(ratio = 1/8) + \n    theme_minimal()\n\ngirafe_options(\n  girafe(ggobj = p),\n  opts_tooltip(css = \"font-family:sans-serif;font-size:1em;color:Black;\")\n  )\nThis view allows us to explore individual words that are characteristic of one or the other group. It also shows the overall shape of the distribution—the slight skew to the left suggests that recalled stories tend to have a slightly richer vocabulary than imagined ones.",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Open Vocabulary Word Counting</span>"
    ]
  },
  {
    "objectID": "dla.html#sec-keyness",
    "href": "dla.html#sec-keyness",
    "title": "15  Open Vocabulary Word Counting",
    "section": "\n15.2 Keyness",
    "text": "15.2 Keyness\nWorking with frequency ratios has an intuitive appeal that is perfect for data visualization; “10 times more common in group A than group B” is a statement that anyone can understand. Even so, frequency ratios are statistically misleading, since they do not account for random sampling error. For example, a word that appears 1000 times in group A and 100 times in group B is much more convincingly representative of group A than a word that appears 10 times in group A and 1 time in group B, even though the frequency ratio is identical. Furthermore, simple frequency ratios do not account for base rates—ratios can be skewed to one side simply because texts in one group are longer than those in another. The F/F plot in Section 15.1 solved this problem by working with relative frequencies (the count divided by total words in the group; Section 16.3), but these problems can be solved more elegantly by using statistically motivated methods for group comparisons, sometimes referred to as keyness statistics.\nQuanteda’s default keyness statistic is none other than the chi-squared value, which measures how different the two groups are relative to what might be expected by random chance1. We can compute the chi-squared statistic directly from the DFM using the textstat_keyness() function from quanteda.textstats, with the “target” parameter set to one of the two groups of documents, in this case imagined ones, or docvars(imagined_vs_recalled_dfm, \"memType\") == \"imagined\". This group is referred to as the target group, while everything else becomes the reference group. The function keeps a token’s chi-squared statistic positive if it is more common than expected in the target group, or flips it to negative if it is less common than expected in the target group. Though chi-squared is the default, we recommend using a likelihood ratio test (measure = \"lr\") for most applications. This setting produces the G-squared statistic, a close relative of chi-squared. For smaller samples, we recommend Fisher’s exact test (measure = \"exact\"), which is more reliable for hypothesis testing but takes much longer to compute.\n\n# Filtered DFM\nimagined_vs_recalled_dfm &lt;- hippocorpus_dfm |&gt; \n  # only keep features that appear in at least 30 documents\n  dfm_trim(min_docfreq = 30) |&gt;\n  # only keep imagined and recalled stories (not retold)\n  dfm_subset(memType %in% c(\"imagined\", \"recalled\"))\n\n# Calculate Keyness\nimagined_keyness &lt;- imagined_vs_recalled_dfm |&gt; \n  textstat_keyness(docvars(imagined_vs_recalled_dfm, \"memType\") == \"imagined\",\n                   measure = \"lr\")\n\nhead(imagined_keyness)\n\n#&gt;   feature        G2 p n_target n_reference\n#&gt; 1     ago 223.47819 0     1714        1109\n#&gt; 2       i 194.53728 0    32080       32906\n#&gt; 3   can't 101.36259 0      466         246\n#&gt; 4     i'm  82.91565 0     1012         747\n#&gt; 5    just  82.08088 0     2616        2307\n#&gt; 6  really  72.29989 0     1893        1622\n\n\nWe can use Quanteda to generate a simple plot of the most extreme values of our keyness statistic, using the textplot_keyness() function, from the quanteda.textplots package.\n\nimagined_keyness |&gt; \n  quanteda.textplots::textplot_keyness() +\n    labs(title = \"Words in Imagined (target) and Recalled (reference) Stories\")\n\n\n\n\n\n\n\nWe can also represent the keyness values as a word cloud, as we did for frequency ratios in Chapter 5.\n\nlibrary(ggwordcloud)\nset.seed(2)\n\nimagined_keyness |&gt; \n  # only words with significant difference to p &lt; .001\n  filter(p &lt; .001) |&gt; \n  # arrange in descending order\n  arrange(desc(abs(G2))) |&gt; \n  # plot\n  ggplot(aes(label = feature, \n             size = G2, \n             color = G2 &gt; 0,\n             angle_group = G2 &gt; 0)) +\n    geom_text_wordcloud_area(eccentricity = 1, show.legend = TRUE) + \n    scale_size_area(max_size = 30, guide = \"none\") +\n    scale_color_discrete(\n      name = \"\",\n      breaks = c(FALSE, TRUE),\n      labels = c(\"More in Recalled\", \n                 \"More in Imagined\")\n      ) +\n    labs(caption = \"Only words with a significant difference between the groups (p &lt; .001) were included.\") +\n    theme_void() # blank background\n\n\n\n\n\n\n\nKeyness plots like this are often a good second step in EDA, after looking at your data directly (see Chapter 11). For an even better chance at noticing interesting patterns, we recommend generating keyness plots for n-grams and shingles in addition to words (see Chapter 13).",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Open Vocabulary Word Counting</span>"
    ]
  },
  {
    "objectID": "dla.html#continuous-covariates",
    "href": "dla.html#continuous-covariates",
    "title": "15  Open Vocabulary Word Counting",
    "section": "\n15.3 Continuous Covariates",
    "text": "15.3 Continuous Covariates\nOpen vocabulary analysis does not require two distinct groups. For example, we may want to investigate the differences in language associated with openness to experience. A simple way to do this is to measure the correlation between word frequencies in documents and the documents’ authors’ openness score.\n\n# openness score of the author of each document\nopenness_scores &lt;- docvars(hippocorpus_corp)$openness\n\nhippocorpus_dfm\n\n#&gt; Document-feature matrix of: 6,854 documents, 27,667 features (99.50% sparse) and 6 docvars.\n#&gt;                                 features\n#&gt; docs                             concerts are my most favorite thing and\n#&gt;   32RIADZISTQWI5XIVG5BN0VMYFRS4U        1   1  5    1        2     1  11\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQ4DARA2        0   0  5    0        0     0  10\n#&gt;   3IRIK4HM3B6UQBC0HI8Q5TBJZLEC61        0   0 10    3        0     1   6\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG04RAI        0   1  4    0        0     0   6\n#&gt;   3MTMREQS4W44RBU8OMP3XSK8NMJAWZ        0   0  5    0        0     0   5\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG06AR3        0   0  4    0        0     0   6\n#&gt;                                 features\n#&gt; docs                             boyfriend knew it\n#&gt;   32RIADZISTQWI5XIVG5BN0VMYFRS4U         2    3  3\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQ4DARA2         1    0  2\n#&gt;   3IRIK4HM3B6UQBC0HI8Q5TBJZLEC61         0    0  6\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG04RAI         0    0  1\n#&gt;   3MTMREQS4W44RBU8OMP3XSK8NMJAWZ         0    0  2\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG06AR3         0    0  6\n#&gt; [ reached max_ndoc ... 6,848 more documents, reached max_nfeat ... 27,657 more features ]\n\nopenness_cor &lt;- hippocorpus_dfm |&gt; \n  convert(to = \"data.frame\") |&gt; \n  # compute rank-based correlation\n  summarise(\n    across(\n      -doc_id, \n      ~ cor(.x, openness_scores, method = \"spearman\")\n      )\n    ) |&gt; \n  pivot_longer(everything(), \n               names_to = \"feature\", \n               values_to = \"cor\")\n\n#&gt; Warning in asMethod(object): sparse-&gt;dense coercion: allocating vector of size\n#&gt; 1.4 GiB\n\nhead(openness_cor)\n\n#&gt; # A tibble: 6 × 2\n#&gt;   feature       cor\n#&gt;   &lt;chr&gt;       &lt;dbl&gt;\n#&gt; 1 concerts  0.00415\n#&gt; 2 are      -0.0583 \n#&gt; 3 my        0.00717\n#&gt; 4 most      0.00482\n#&gt; 5 favorite  0.0129 \n#&gt; 6 thing     0.0195\n\n\nNow that we have correlation coefficients, we can once again make a word cloud.\n\nset.seed(2)\n\nopenness_cor |&gt; \n  # arrange in descending order\n  arrange(desc(abs(cor))) |&gt; \n  # only top correlating words\n  slice_head(n = 150) |&gt; \n  # plot\n  ggplot(aes(label = feature, \n             size = abs(cor), \n             color = cor,\n             angle_group = cor &lt; 0)) +\n    geom_text_wordcloud_area(eccentricity = 1, show.legend = TRUE) + \n    scale_size_area(max_size = 10, guide = \"none\") +\n    scale_color_gradient2(\n      name = \"\",\n      low = \"red3\", mid = \"grey\", high = \"blue3\",\n      breaks = c(-.05, 0, .05),\n      labels = c(\"Negatively Correlated\\nwith Openness\", \"\",\n                 \"Positively Correlated\\nwith Openness\")\n      ) +\n    theme_void() # blank background",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Open Vocabulary Word Counting</span>"
    ]
  },
  {
    "objectID": "dla.html#sec-generating-dictionaries",
    "href": "dla.html#sec-generating-dictionaries",
    "title": "15  Open Vocabulary Word Counting",
    "section": "\n15.4 Generating Dictionaries With Open Vocabulary Analysis",
    "text": "15.4 Generating Dictionaries With Open Vocabulary Analysis\nIn Section 14.6 we saw that dictionaries for word counting can be generated in many ways. One of these ways is to start with a corpus and proceed with open vocabulary analysis. As an example, let’s use the Crowdflower Emotion in Text dataset to generate a new dictionary for the emotion of surprise. The Crowdflower dataset includes 40,000 Twitter posts, each labeled with one of 13 sentiments. Of these, 2187 are labeled as reflecting surprise.\n\nglimpse(crowdflower)\n\n#&gt; Rows: 40,000\n#&gt; Columns: 4\n#&gt; $ tweet_id  &lt;dbl&gt; 1956967341, 1956967666, 1956967696, 1956967789, 1956968416, …\n#&gt; $ sentiment &lt;chr&gt; \"empty\", \"sadness\", \"sadness\", \"enthusiasm\", \"neutral\", \"wor…\n#&gt; $ author    &lt;chr&gt; \"xoshayzers\", \"wannamama\", \"coolfunky\", \"czareaquino\", \"xkil…\n#&gt; $ text      &lt;chr&gt; \"@tiffanylue i know  i was listenin to bad habit earlier and…\n\n\nGenerating a dictionary is a subtly different goal than discovering the differences between groups. In Section 15.2 above, we recommended the likelihood ratio test as a way of looking for statistically significant differences. When generating a dictionary though, we care less about statistical significance and more about information—if I observe this word in a text, how much does it tell me about the text? For this kind of question, we recommend a different keyness statistic: pointwise mutual information (PMI).\nPMI can be computed just like the likelihood ratio test, using textstat_keyness(). We will need to be careful though, since PMI is sensitive to rare tokens—if a token appears only once in the corpus, in a text labelled surprise, it will be identified as giving lots of information about whether the text is labeled surprise. This is true enough within our training corpus, but with such a small sample size, the appearance of a rare token in a text is probably due to random variation. Rare tokens are therefore unlikely to generalize to other data sets when we use them in a dictionary. So we’ll want to keep tokens with a high PMI, but remove very rare ones.\n\n# convert to corpus\ncrowdflower_corp &lt;- crowdflower |&gt; \n  corpus(docid_field = \"tweet_id\")\n\n# convert to DFM\ncrowdflower_dfm &lt;- crowdflower_corp |&gt; \n  tokens(remove_punct = TRUE) |&gt; \n  tokens_ngrams(n = c(1L, 2L)) |&gt;  # 1-grams and 2-grams\n  dfm()\n\n# compute PMI\nsurprise_pmi &lt;- crowdflower_dfm |&gt; \n  textstat_keyness(\n    docvars(crowdflower_dfm, \"sentiment\") == \"surprise\",\n    measure = \"pmi\"\n    )\n\n# filter out rare words and low PMI\nsurprise_pmi &lt;- surprise_pmi |&gt; \n  filter(n_target + n_reference &gt; 10, \n         pmi &gt; 1.5)\n\nsurprise_pmi\n\n#&gt;          feature      pmi         p n_target n_reference\n#&gt; 1      read_that 2.188152 0.1390761        6           6\n#&gt; 2       surprise 2.045051 0.1527019       13          17\n#&gt; 3       the_more 2.034001 0.1538152        6           8\n#&gt; 4       wow_that 1.993996 0.1579237        7          10\n#&gt; 5   cant_believe 1.965008 0.1609787        8          12\n#&gt; 6      surprised 1.965008 0.1609787        8          12\n#&gt; 7    thought_i'd 1.925788 0.1652200        5           8\n#&gt; 8   surprisingly 1.900470 0.1680258        6          10\n#&gt; 9         dublin 1.869698 0.1715097        4           7\n#&gt; 10        i_read 1.859648 0.1726655        9          16\n#&gt; 11 just_realised 1.782687 0.1818198        4           8\n#&gt; 12     quot_what 1.782687 0.1818198        4           8\n#&gt; 13          ship 1.782687 0.1818198        4           8\n#&gt; 14   that's_very 1.782687 0.1818198        4           8\n#&gt; 15       the_air 1.782687 0.1818198        5          10\n#&gt; 16     the_swine 1.782687 0.1818198        4           8\n#&gt; 17    didn't_see 1.728619 0.1885873        6          13\n#&gt; 18        winter 1.728619 0.1885873        6          13\n#&gt; 19      and_yeah 1.702644 0.1919426        4           9\n#&gt; 20     called_me 1.657524 0.1979380        5          12\n#&gt; 21      i_looked 1.657524 0.1979380        5          12\n#&gt; 22          east 1.628536 0.2019057        6          15\n#&gt; 23      on_earth 1.628536 0.2019057        4          10\n#&gt; 24       one_and 1.628536 0.2019057        4          10\n#&gt; 25      quot_you 1.628536 0.2019057        4          10\n#&gt; 26          rich 1.628536 0.2019057        4          10\n#&gt; 27      to_dance 1.628536 0.2019057        4          10\n#&gt; 28   didn't_know 1.612788 0.2041004        9          23\n#&gt; 29    east_coast 1.582016 0.2084705        3           8\n#&gt; 30   girlfriends 1.582016 0.2084705        3           8\n#&gt; 31      hard_for 1.582016 0.2084705        3           8\n#&gt; 32         humid 1.582016 0.2084705        3           8\n#&gt; 33     it's_very 1.582016 0.2084705        3           8\n#&gt; 34      left_for 1.582016 0.2084705        3           8\n#&gt; 35           moi 1.582016 0.2084705        3           8\n#&gt; 36     money_for 1.582016 0.2084705        3           8\n#&gt; 37 my_blackberry 1.582016 0.2084705        3           8\n#&gt; 38      my_uncle 1.582016 0.2084705        3           8\n#&gt; 39    never_seen 1.582016 0.2084705        3           8\n#&gt; 40           no1 1.582016 0.2084705        3           8\n#&gt; 41         queen 1.582016 0.2084705        3           8\n#&gt; 42      quot_i'm 1.582016 0.2084705        3           8\n#&gt; 43     the_stuff 1.582016 0.2084705        3           8\n#&gt; 44        true_i 1.582016 0.2084705        3           8\n#&gt; 45       what_he 1.582016 0.2084705        3           8\n#&gt; 46     where_did 1.582016 0.2084705        6          16\n#&gt; 47    why_didn't 1.582016 0.2084705        3           8\n#&gt; 48        i_aint 1.559543 0.2117321        4          11\n#&gt; 49      it_works 1.559543 0.2117321        4          11\n#&gt; 50        to_his 1.559543 0.2117321        4          11\n#&gt; 51        amazon 1.546298 0.2136828        5          14\n#&gt; 52       up_quot 1.546298 0.2136828        5          14\n#&gt; 53      it_right 1.537564 0.2149807        6          17\n\n\nWe are left with a list of the unigrams and bigrams that most indicate that a tweet contains surprise. We can now use this list as a dictionary.\n\ntweet_surprise_dict &lt;- dictionary(\n  list(surprise = surprise_pmi$feature),\n  separator = \"_\" # in n-grams, tokens are separated by \"_\"\n)\n\nCan we use this new surprise dictionary to test our hypothesis that true autobiographical stories include more surprise than imagined stories? Maybe. There is no guarantee that surprise words in Tweets will be the same as surprise words in autobiographical stories. With this in mind, we can proceed cautiously:\n\n# count words\nhippocorpus_surprise_df &lt;- hippocorpus_dfm |&gt; \n  dfm_lookup(tweet_surprise_dict) |&gt; # count dictionary words\n  convert(\"data.frame\") |&gt; # convert to dataframe\n  right_join(\n    hippocorpus_corp |&gt; \n      convert(\"data.frame\") # convert to dataframe\n    ) |&gt; \n  mutate(wc = ntoken(hippocorpus_dfm)) # total word count\n\n#&gt; Joining with `by = join_by(doc_id)`\n\n# model\ntweet_surprise_mod &lt;- MASS::glm.nb(surprise ~ memType + log(wc),\n                                   data = hippocorpus_surprise_df)\nsummary(tweet_surprise_mod)\n\n#&gt; \n#&gt; Call:\n#&gt; MASS::glm.nb(formula = surprise ~ memType + log(wc), data = hippocorpus_surprise_df, \n#&gt;     init.theta = 0.4897427347, link = log)\n#&gt; \n#&gt; Coefficients:\n#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)     -5.24212    0.51985 -10.084  &lt; 2e-16 ***\n#&gt; memTyperecalled -0.07517    0.06931  -1.085    0.278    \n#&gt; memTyperetold    0.05558    0.08335   0.667    0.505    \n#&gt; log(wc)          0.68752    0.09417   7.301 2.86e-13 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for Negative Binomial(0.4897) family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 3812.3  on 6853  degrees of freedom\n#&gt; Residual deviance: 3757.0  on 6850  degrees of freedom\n#&gt; AIC: 8149.5\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 1\n#&gt; \n#&gt; \n#&gt;               Theta:  0.4897 \n#&gt;           Std. Err.:  0.0402 \n#&gt; \n#&gt;  2 x log-likelihood:  -8139.4810\n\n\nUsing our dictionary generated from Tweets with open vocabulary analysis, we find no significant difference between true autobiographical stories and imagined stories in the amount of surprise-related language they contain (p = 0.278).\n\n\n\n\n\nSchwartz, H. A., Eichstaedt, J. C., Kern, M. L., Dziurzynski, L., Ramones, S. M., Agrawal, M., Shah, A., Kosinski, M., Stillwell, D., Seligman, M. E. P., & Ungar, L. H. (2013). Personality, gender, and age in the language of social media: The open-vocabulary approach. PLOS ONE, 8(9), 1–16. https://doi.org/10.1371/journal.pone.0073791",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Open Vocabulary Word Counting</span>"
    ]
  },
  {
    "objectID": "dla.html#footnotes",
    "href": "dla.html#footnotes",
    "title": "15  Open Vocabulary Word Counting",
    "section": "",
    "text": "More precisely, the chi-squared statistic is the sum of the differences between observed frequencies and the frequencies that would be expected if there were no difference between the groups, divided by the expected frequency.↩︎",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Open Vocabulary Word Counting</span>"
    ]
  },
  {
    "objectID": "word-counting-improvements.html",
    "href": "word-counting-improvements.html",
    "title": "16  Transforming Word Counts",
    "section": "",
    "text": "16.1 Text Normalization\nThe simplest way to transform word counts is by transforming the text itself. We have already seen some simple examples of this in Section 13.2: removing punctuation, symbols, or URLs before tokenization. Such transformations are often called text normalization, since they get rid of the quirks in each text and ensure that everything follows a standard format.",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Transforming Word Counts</span>"
    ]
  },
  {
    "objectID": "word-counting-improvements.html#text-normalization",
    "href": "word-counting-improvements.html#text-normalization",
    "title": "16  Transforming Word Counts",
    "section": "",
    "text": "16.1.1 Occurrence Thresholds\nBesides removing or standardizing certain types of tokens, like URLs, researchers commonly enforce an occurrence threshold, removing any token that occurs less than a certain number of times in the data. Occurrence thresholds can be calculated on the full dataset (term frequency) or between documents (document frequency; e.g. removing tokens used in fewer than 1% of documents). Using a document frequency threshold is often beneficial, since sometimes a single document will use a token a lot—either because it happens to be discussing a specific topic or because of a quirk of the author’s language style—and drive up the overall frequency in a misleading way.\nOccurrence thresholds can be performed on DFMs in Quanteda using the dfm_trim() function, with either a min_termfreq, a min_docfreq, or both. Maximum frequency thresholds can also be imposed.\n\n# remove tokens used by fewer than 1% of documents\nhippocorpus_dfm_threshold &lt;- hippocorpus_dfm |&gt; \n  dfm_trim(min_docfreq = 0.01, docfreq_type = \"prop\")\n\n\n\n\n\n\n\nAdvantages of Occurrence Thresholds\n\n\n\n\n\nCleaner Results: Occurence thresholds are an easy way to remove quirks of individuals’ writing styles, or very rare terms that complicate analysis without adding much information.\n\n\n\n\n\n\n\n\n\nDisadvantages of Occurrence Thresholds\n\n\n\n\n\nArbitrary: Determining what threshold to use can be difficult, and runs the risk of excluding important information from the analysis.\n\n\n\n\n16.1.2 Removing Stop Words\nIn natural language processing, a common step in text normalization is to remove “stop words,” everyday words like “the” and “of” that do not contribute much to the meaning of the text. Indeed, Quanteda offers a built-in list of stop words:\n\nstopwords() |&gt; head()\n\n#&gt; [1] \"i\"      \"me\"     \"my\"     \"myself\" \"we\"     \"our\"\n\n\nAlthough removing stop words can be useful for analyzing the topics of texts, it is generally a bad idea when you are interested in the psychology of texts. This is because the forms in which people choose to write a word—including stop words—are often predictive of their personality. For example, neurotic people tend to use more first-person singulars (Mehl et al., 2006), and articles like “the” and “a” are highly predictive of males, being older, and openness (Schwartz et al., 2013).\nThe relationships between language and personality also extend to more subtle patterns. For example, extraverts tend to use longer words (Mehl et al., 2006), those high in openness tend to use more quotations (Sumner et al., 2011), and those high in neuroticism tend to use more acronyms (Holtgraves, 2011). So if you are looking for psychological differences, be gentle with the text normalization—you never know what strange predictors you might find.\n\n\n\n\n\n\nAdvantages of Removing Stop Words\n\n\n\n\n\nIntuitive Appeal: Removing stop words focuses an analysis on content, rather than form. When people think of differences between texts, they generally think of differences in content.\n\n\n\n\n\n\n\n\n\nDisadvantages of Removing Stop Words\n\n\n\n\n\nRemoves Important Information: While words like “the” and “a” may seem insignificant, they often carry important psychological information.",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Transforming Word Counts</span>"
    ]
  },
  {
    "objectID": "word-counting-improvements.html#binary-boolean-tokenization",
    "href": "word-counting-improvements.html#binary-boolean-tokenization",
    "title": "16  Transforming Word Counts",
    "section": "\n16.2 Binary (Boolean) Tokenization",
    "text": "16.2 Binary (Boolean) Tokenization\nIn some cases, it makes sense to stop counting at one—each text either uses a given token or it does not. While this might seem like needlessly throwing away information, binary tokenization fixes a core problem with the bag of words assumption (BOW). Recall from Section 13.3 that BOW imagines that each author or topic has its characteristic bag of words, and speaking or writing is just a matter of pulling those words out of the bag one at a time at random. A central problem with this picture is that words are not pulled out one at a time at random—the word I am writing now is intimately tied to the words immediately before it. It may be very unlikely overall that I will write “parthenon,” but if I write it once, it is very likely that I will write it again in the same paragraph. This is because I am probably writing about the Parthenon.\nThe non-independence of words in text means that the difference between zero occurrences of “parthenon” and one occurrence is much more meaningful than the difference between one and two. If a particular token sometimes occurs lots of times in text, statistical procedures like regression may be led to focus on that variance rather than on the more interesting first occurrence. Binary tokenization is the simplest way to avoid this problem.\nIn Quanteda, a DFM can be converted to binary tokenization with dfm_weight(scheme = \"boolean\").\n\nhippocorpus_dfm_binary &lt;- hippocorpus_dfm |&gt; \n  dfm_weight(scheme = \"boolean\")\n\nprint(hippocorpus_dfm_binary, max_ndoc = 6, max_nfeat = 6)\n\n#&gt; Document-feature matrix of: 6,854 documents, 27,667 features (99.50% sparse) and 6 docvars.\n#&gt;                                 features\n#&gt; docs                             concerts are my most favorite thing\n#&gt;   32RIADZISTQWI5XIVG5BN0VMYFRS4U        1   1  1    1        1     1\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQ4DARA2        0   0  1    0        0     0\n#&gt;   3IRIK4HM3B6UQBC0HI8Q5TBJZLEC61        0   0  1    1        0     1\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG04RAI        0   1  1    0        0     0\n#&gt;   3MTMREQS4W44RBU8OMP3XSK8NMJAWZ        0   0  1    0        0     0\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG06AR3        0   0  1    0        0     0\n#&gt; [ reached max_ndoc ... 6,848 more documents, reached max_nfeat ... 27,661 more features ]\n\n\nTo use binary tokenization in dictionary-based analysis, you can also perform the dictionary look-up first (Chapter 14) and then convert it to binary with mutate(surprise_binary = as.integer(surprise &gt; 0)).\nKeep in mind: Once you convert your DFM to binary tokenization, you are no longer working with a count variable. This means that the negative binomial regression models we covered in Chapter 14 are no longer appropriate. Instead, if you want to model the binary tokenization as a dependent variable, you will have to use a binary model like logistic regression.\n\n\n\n\n\n\nAdvantages of Binary Tokenization\n\n\n\n\n\nRemoves Non-Independence of Observations: Raw word counts can be misleading (to both humans and statistical models) because the observations are not independent; the more a text uses a word, the more likely it is to use that word again. Binary tokenization avoids this problem by only counting one event per text.\n\n\n\n\n\n\n\n\n\nDisadvantages of Binary Tokenization\n\n\n\n\n\nDevalues Common Tokens: With binary tokenization, you might miss differences in common tokens like “the.” Since almost every text uses “the” at least once, you won’t be able to detect that one group uses “the” more often than another.\n\nDifficult to Control for Text Length: The longer a text is, the more likely any given word is to appear in it. But when we stop counting after the first word, this relationship becomes especially difficult to characterize. When working with shorter texts, beware of mistaking differences in text length for differences in the probability of a word appearing!",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Transforming Word Counts</span>"
    ]
  },
  {
    "objectID": "word-counting-improvements.html#sec-relative-tokenization",
    "href": "word-counting-improvements.html#sec-relative-tokenization",
    "title": "16  Transforming Word Counts",
    "section": "\n16.3 Relative Tokenization",
    "text": "16.3 Relative Tokenization\nThe most common transformation that researchers apply to word counts is dividing them by the total number of words (or other tokens) in the text. The resulting ratio is referred to as a relative frequency. This strategy has intuitive appeal (e.g. what percentage of the words are surprise-related), and the value of intuitive appeal should not be discounted see 4.1. But working with ratios or percentages can cause problems with statistical analysis. This is because dividing by the total number of words is no guarantee that the total number of words will be properly controlled for—these are two separate variables. Regression analyses on relative frequencies are likely to give false positives when predictors are correlated with text length (Kronmal, 1993). This is the same reason why we added total word count as both a log offset and a regular predictor in Section 14.3.1.\nBefore you start worrying about how to control for text length, make sure to stop and ponder: Do you want to control for text length? Usually we assume that longer documents are longer because the author is more verbose. But what if longer texts are longer because they cover multiple topics, and one of those topics is what we are interested in? In this case, controlling for text length—especially using relative tokenization—will make it look like the longer texts have less of that topic, when in fact they just have more of other topics.\nIf you decide to use relative tokenization, the process is simple. For dictionary-based word counts, divide your count by the total word count to get a relative frequency. If you want to convert a full DFM to relative tokenization, you can use dfm_weight(scheme = \"prop\").\n\nhippocorpus_dfm_relative &lt;- hippocorpus_dfm |&gt; \n  dfm_weight(scheme = \"prop\")\n\nprint(hippocorpus_dfm_relative, max_ndoc = 6, max_nfeat = 4)\n\n#&gt; Document-feature matrix of: 6,854 documents, 27,667 features (99.50% sparse) and 6 docvars.\n#&gt;                                 features\n#&gt; docs                                concerts         are         my        most\n#&gt;   32RIADZISTQWI5XIVG5BN0VMYFRS4U 0.004926108 0.004926108 0.02463054 0.004926108\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQ4DARA2 0           0           0.02732240 0          \n#&gt;   3IRIK4HM3B6UQBC0HI8Q5TBJZLEC61 0           0           0.03759398 0.011278195\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG04RAI 0           0.006060606 0.02424242 0          \n#&gt;   3MTMREQS4W44RBU8OMP3XSK8NMJAWZ 0           0           0.03086420 0          \n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG06AR3 0           0           0.01257862 0          \n#&gt; [ reached max_ndoc ... 6,848 more documents, reached max_nfeat ... 27,663 more features ]\n\n\nAn example of relative tokenization in research: Golder & Macy (2011) collected messages from all Twitter user accounts (~2.4 million) created between February 2008 and April 2009, and measured positive and negative affect as proportion of in-dictionary (from LIWC) words to total word count. This calculation was done by hour of the day, day of the week, and month of the year, revealing fluctuations in mood in line with circadian rhythms and seasonal changes.\n\n\n\n\n\n\nAdvantages of Relative Tokenization\n\n\n\n\n\nIntuitive Appeal: Relative tokenization makes sense if longer documents are longer because of verbosity, or if the construct of interest does not fluctuate over the course of a longer text (e.g. personality).\n\n\n\n\n\n\n\n\n\nDisadvantages of Relative Tokenization\n\n\n\n\n\nDiscounts Longer Texts: If texts are long because they cover multiple topics (or multiple emotions), relative tokenization will dilute true occurrences of the construct of interest.\n\nDoes Not Control for Text Length: People often assume that using a percentage will control for the denominator in statistical analyses. This is wrong, which might make ratios like relative tokenization more trouble than they’re worth.\n\nNot Normally Distributed: Dividing count variables by text length does not make them normally distributed. This can cause problems for certain statistical methods. Using the Anscombe transform can partially remedy these problems.",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Transforming Word Counts</span>"
    ]
  },
  {
    "objectID": "word-counting-improvements.html#sec-anscombe",
    "href": "word-counting-improvements.html#sec-anscombe",
    "title": "16  Transforming Word Counts",
    "section": "\n16.4 The Anscombe Transform",
    "text": "16.4 The Anscombe Transform\nWord counts (whether divided by text length or not) are not normally distributed. In Chapter 14, we avoided this problem by using negative binomial regression. An alternative way to deal with this problem is to try to transform the counts to a normal distribution. Remember that complicated-looking formula from Schwartz et al. (2013) in Section 4.1? The upper part of that formula is relative tokenization. The lower part is called the Anscombe transform, and it transforms a Poisson distribution (i.e. a well-behaved count variable) into an approximately normal distribution. The transformed variable can then be used in linear regression. In R, the Anscombe transform can be written as 2*sqrt(count + 3/8).\nThe Anscombe transform can be useful for analyzing very large numbers of word count variables (as Schwartz et al. (2013) did) without having to run negative binomial regression each time. But if you are only analyzing a few variables, we recommend against it. This is because word counts do not follow a Poisson distribution and therefore will not be properly normally distributed even after the transform. The Poisson process assumes that words occur independently from each other, and with equal probability throughout a text. In other words, it makes the BOW assumption. As we’ve covered already, this assumption is problematic. In this case, the fact that words are not pulled randomly out of a bag makes word count distributions overdispersed, meaning that the variance of the distribution is higher than expected in a Poisson process. Negative binomial regression may be complicated and take longer to compute, but it tends to be robust to overdispersion.\n\n\n\n\n\n\nAdvantages of the Anscombe Transform\n\n\n\n\nComputational Efficiency\nAddresses Non-Normality of Word Frequencies\n\n\n\n\n\n\n\n\n\nDisadvantages of the Anscombe Transform\n\n\n\n\nWrongly Assumes That Word Counts Follow a Poisson Distribution",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Transforming Word Counts</span>"
    ]
  },
  {
    "objectID": "word-counting-improvements.html#sec-tfidf",
    "href": "word-counting-improvements.html#sec-tfidf",
    "title": "16  Transforming Word Counts",
    "section": "\n16.5 TF-IDF",
    "text": "16.5 TF-IDF\nTerm frequency-inverse document frequency (TF-IDF) is one of the great triumphs of NLP in the last century. It was first introduced by Sparck Jones (1972) and is still widely used half a century later, especially in search engines.\nThe idea of TF-IDF is to measure how topical a token is in a document In other words, how representative is that token of the particular features of that document as opposed to other documents? Let’s start with term frequency (TF). TF is just relative frequency - a word count divided by the total number of words in the document The problem with relative frequency is that it emphasizes common words that don’t tell us much about the meaning of the document—if a document has a high frequency of “the,” should we conclude that “the” is very important to the meaning of the document? Of course not. To fix this, we multiply TF by inverse document frequency (IDF). Document frequency is the proportion of documents that have at least one instance of our token (i.e. the average binary tokenization across documents). IDF is the log of the inverse of the document frequency. IDF answers the question: How unusual is it for this token to appear at all in a document? So “the,” which appears in almost every document, will have a very low IDF—it is not unusual at all. Even though “the” appears a lot in our document (TF is high), its TF-IDF score will be low, reflecting the fact that “the” doesn’t tell us much about the content of this particular document.\n\\[\nTF \\cdot IDF = relative\\:frequency × \\log{\\frac{total\\:documents}{documents\\:with\\:token}}\n\\]\nYou might wonder: If we want to compare how common the token is in this document to how common it is in other documents, shouldn’t we use the inverse token frequency (i.e. the average relative frequency of tokens across documents in the corpus)? Why do we use the inverse document frequency? The answer: TF-IDF does not make the BOW assumption. If all documents were just bags of words, the frequency within documents would be distributed similarly to the frequency between documents (e.g. since the bigram “verbal reasoning” occurs in very few tweets, you would expect the probability of it occurring twice in the same document to be near-impossible). In reality though, it wouldn’t be surprising to see “verbal reasoning” even 3 times in the same tweet if that tweet were discussing verbal reasoning. Multiplying relative term frequency by inverse document frequency provides a measure of exactly how wrong BOW is in this case. In other words, how topical is the token?\nIn Quanteda, we can convert a DFM to TF-IDF with the dfm_tfidf() function:\n\nhippocorpus_dfm_tfidf &lt;- hippocorpus_dfm |&gt; \n  dfm_tfidf(scheme_tf = \"prop\")\n\nprint(hippocorpus_dfm_tfidf, max_ndoc = 6, max_nfeat = 4)\n\n#&gt; Document-feature matrix of: 6,854 documents, 27,667 features (99.50% sparse) and 6 docvars.\n#&gt;                                 features\n#&gt; docs                               concerts         are           my\n#&gt;   32RIADZISTQWI5XIVG5BN0VMYFRS4U 0.01296465 0.002577006 0.0005541024\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQ4DARA2 0          0           0.0006146600\n#&gt;   3IRIK4HM3B6UQBC0HI8Q5TBJZLEC61 0          0           0.0008457352\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG04RAI 0          0.003170499 0.0005453711\n#&gt;   3MTMREQS4W44RBU8OMP3XSK8NMJAWZ 0          0           0.0006943381\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG06AR3 0          0           0.0002829755\n#&gt;                                 features\n#&gt; docs                                    most\n#&gt;   32RIADZISTQWI5XIVG5BN0VMYFRS4U 0.003125847\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQ4DARA2 0          \n#&gt;   3IRIK4HM3B6UQBC0HI8Q5TBJZLEC61 0.007156545\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG04RAI 0          \n#&gt;   3MTMREQS4W44RBU8OMP3XSK8NMJAWZ 0          \n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG06AR3 0          \n#&gt; [ reached max_ndoc ... 6,848 more documents, reached max_nfeat ... 27,663 more features ]\n\n\nOverall, TF-IDF combines the advantages of many simpler word count transformations without many of their downsides. For example, removing stop words focuses the analysis on text content, but may remove important information. TF-IDF discounts uninformative tokens like stop words without removing them outright. Likewise, we saw that binary tokenization solves part of the problem with the BOW assumption, but throws out a lot of information in the process. Once again, TF-IDF turns this problem into a feature (quantifying how topical a token is) without throwing out any information.\nDespite all of its benefits, TF-IDF is not widely used in psychology research. This is for two reasons. First, it is difficult to interpret intuitively, and researchers prize interpretability. Second, it focuses the analysis on the topic of texts rather than the subtleties of language style that are often associated with psychological differences (see Section 16.1.2). Nevertheless, if your construct is of a less subtle nature—more akin to the “topic” of the text—consider using TF-IDF.\n\n\n\n\n\n\nAdvantages of TF-IDF\n\n\n\n\nComputational Efficiency\n\nDiscounts Uninformative Tokens Without Losing Information: TF-IDF combines the advantages of stop word removal without removing tokens from the analysis.\n\nDoes Not Rely on BOW: TF-IDF leverages the discrepancy between term frequency and document frequency to discover patterns of meaning.\n\nProven to Work: TF-IDF has a long history of use in search engines and automated recommendations. It works surprisingly well for such a simple calculation.\n\n\n\n\n\n\n\n\n\nDisadvantages of TF-IDF\n\n\n\n\n\nMay Discount Psychologically Relevant Informartion: TF-IDF operates on an assumption that more frequent tokens (across documents) are less relevant to the construct in question. For semantics this is largely true, but for latent psychological constructs it may not be.",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Transforming Word Counts</span>"
    ]
  },
  {
    "objectID": "word-counting-improvements.html#sec-smoothing",
    "href": "word-counting-improvements.html#sec-smoothing",
    "title": "16  Transforming Word Counts",
    "section": "\n16.6 Smoothing",
    "text": "16.6 Smoothing\nIn Section 16.3, we mentioned that dividing by text length does not adequately control for text length. This is true of ratios in general, but for word frequencies in particular there is extra cause for concern.\nConsider this short text written by an imaginary participant, Allison:\nI was sitting at the table, and suddenly I understood.\nThe text has 10 words in total. One of these, “suddenly,” is a surprise-related word. Given only this text, you might estimate the probability of surprise words in Allison’s language at 1/10, the relative frequency. You would be wrong. To see why, imagine that we also wanted to measure the probability of anger-related words. There are no anger-related words in this text. Is the probability of anger-related words then 0/10 = 0? Of course not. If we read more of Allison’s texts, we might very well encounter an anger-related word. Relative frequency therefore underestimates the probability of unobserved words. Conversely, it must overestimate the probability of observed words. So to leave room for the probability of unobserved words, we must admit that the true probability of anger-related words is likely to be a little less than 1/10.\nThe first solution to this kind of problem was offered by Laplace (1816), who proposed to simply add one to all the frequency counts before computing the relative frequency. This is called Laplace smoothing, or sometimes simply add-one smoothing. To perform Laplace smoothing in Quanteda, use the dfm_smooth() function to add one, and then call dfm_weight() as before.\n\nhippocorpus_dfm_laplace &lt;- hippocorpus_dfm |&gt; \n  dfm_smooth(smoothing = 1) |&gt; \n  dfm_weight(scheme = \"prop\")\n\n#&gt; Warning in asMethod(object): sparse-&gt;dense coercion: allocating vector of size\n#&gt; 1.4 GiB\n\nprint(hippocorpus_dfm_laplace, max_ndoc = 6, max_nfeat = 4)\n\n#&gt; Document-feature matrix of: 6,854 documents, 27,667 features (0.00% sparse) and 6 docvars.\n#&gt;                                 features\n#&gt; docs                                 concerts          are           my\n#&gt;   32RIADZISTQWI5XIVG5BN0VMYFRS4U 7.176175e-05 7.176175e-05 0.0002152853\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQ4DARA2 3.590664e-05 3.590664e-05 0.0002154399\n#&gt;   3IRIK4HM3B6UQBC0HI8Q5TBJZLEC61 3.579995e-05 3.579995e-05 0.0003937994\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG04RAI 3.592986e-05 7.185973e-05 0.0001796493\n#&gt;   3MTMREQS4W44RBU8OMP3XSK8NMJAWZ 3.593374e-05 3.593374e-05 0.0002156024\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG06AR3 3.573343e-05 3.573343e-05 0.0001786671\n#&gt;                                 features\n#&gt; docs                                     most\n#&gt;   32RIADZISTQWI5XIVG5BN0VMYFRS4U 7.176175e-05\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQ4DARA2 3.590664e-05\n#&gt;   3IRIK4HM3B6UQBC0HI8Q5TBJZLEC61 1.431998e-04\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG04RAI 3.592986e-05\n#&gt;   3MTMREQS4W44RBU8OMP3XSK8NMJAWZ 3.593374e-05\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG06AR3 3.573343e-05\n#&gt; [ reached max_ndoc ... 6,848 more documents, reached max_nfeat ... 27,663 more features ]\n\n\nLaplace smoothing is better than nothing, but it is designed for a situation in which the number of possible token types is small and known. In the case of natural language, the number of possible tokens is extremely large and entirely unknown (Baayen, 2001). You can partially make up for this by adding a very small smoothing number instead of 1 (e.g. 1 divided by the number of token types in the corpus: smoothing = 1/ncol(hippocorpus_dfm)), but if you are willing to invest a bit more computation time, there are better ways.\nDuring World War II, the German navy encrypted and decrypted its communications using the Enigma machine, which scrambled and unscrambled messages according to an initial input setting. This input setting was updated each day, and part of the process required the operator of the machine to select a three-letter sequence from a large book “at random” (Good, 2000). Alan Turing, who was in charge of the decryption effort on the part of the British, quickly understood that the German operators’ choices from this book were not entirely random, and that patterns in their choices could be exploited to narrow down the search. The problem became how to estimate the probability of each three-letter sequence based on a relatively small sample of previously decoded input settings—a sample much smaller than the number of three-letter sequences in the book (which, like the number of possible tokens in English text, was extremely large). Turing solved this problem—essentially the same problem posed by word frequencies in text—by developing a method that used frequencies of frequencies in the sample to estimate the total probability of yet-unseen sequences and correct the observed frequencies based on this estimate. The algorithm was later refined and published by Turing’s assistant I. J. Good (1953), and has since seen many variations. Today, the most popular of these variations is the Simple Good-Turing algorithm (Gale & Sampson, 1995). Unfortunately, Simple Good-Turing smoothing is not yet implemented in Quanteda, but that is expected to change in the coming months. As soon as it does, we will add it to the textbook.\nWhile smoothing may seem unnecessarily complex, it can be an important safeguard against false positives when analyzing word counts or frequencies. This is because the bias that smoothing accounts for (due to unobserved tokens) is stronger for shorter texts, and grows in a non-linear pattern (Baayen, 2001). This bias can become a confounding factor in regression models, especially when a predictor variable is strongly correlated with text length.\n\n\n\n\n\n\nAdvantages of Smoothing\n\n\n\n\nBetter Estimates of Token Probabilities\n\nCorrects Text-Length-Dependent Bias:  This bias can cause false positive results in regression analyses.\n\n\n\n\n\n\n\n\n\nDisadvantages of Smoothing\n\n\n\n\n\nComputationally Inefficient:  Laplace smoothing is much more efficient than Simple Good-Turing, but it does not perform as well.",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Transforming Word Counts</span>"
    ]
  },
  {
    "objectID": "word-counting-improvements.html#sec-machine-learning-word-counts",
    "href": "word-counting-improvements.html#sec-machine-learning-word-counts",
    "title": "16  Transforming Word Counts",
    "section": "\n16.7 Machine Learning Approaches",
    "text": "16.7 Machine Learning Approaches\nThis chapter is dedicated to methods that can be carried out after computing word counts (Chapter 13) and before analysis (Chapter 14; Chapter 15). These methods can be thought of as transformations that we apply to word counts to make them better at measuring what we want them to measure. The last of these transformations is the most elaborate one: supervised machine learning.\nFor supervised machine learning, you need a training dataset of text that is already labeled with your construct of interest. We already saw a dataset like this for the emotion of surprise in Section 15.4: Crowdflower Emotion in Text dataset. Many other good examples can be found in Chapter 8. The labels could be generated by participants who read the texts, or by the authors of the texts themselves (e.g. in the form of questionnaires that measure their personality).\nOnce you have your training dataset, compute word counts for its texts (Chapter 13). These could be dictionary-based counts from a variety of different dictionaries, or they could be individual token counts in the form of a DFM. You can even transform these counts in some of the ways described in this chapter (or even more than one). These counts will become the predictor variables for your machine learning model.\nThis book is not the place an in-depth tutorial on machine learning1, but we will give a brief example. Recall that in Section 15.4 we used the Crowdflower Emotion in Text dataset to find the words most indicative of surprise in tweets (based on PMI). In Section 15.4 we used these words as a dictionary, but we could get more robust results by using them as predictor variables to train a regularized machine learning model, in this case ridge regression, although there are many other options.\n\n# New Hippocorpus DFM \nhippocorpus_dfm_ngrams &lt;- hippocorpus_corp |&gt; \n  tokens(remove_punct = TRUE) |&gt; \n  tokens_ngrams(n = c(1L, 2L)) |&gt; # 1-grams and 2-grams\n  dfm() |&gt; \n  dfm_weight(scheme = \"prop\") # relative tokenization\n\n# Select only high PMI tokens from Crowdflower\nhippocorpus_dfm_surprise &lt;- hippocorpus_dfm_ngrams |&gt; \n  dfm_select(tweet_surprise_dict)\n\n# Crowdflower DFM with only tokens that appear in Hippocorpus DFM\ncrowdflower_dfm &lt;- crowdflower_dfm |&gt; \n  dfm_weight(scheme = \"prop\") |&gt;  # relative tokenization\n  dfm_select(featnames(hippocorpus_dfm_surprise)) \n\n# Rejoin to Crowdflower DFM labeled data\ncrowdflower_train &lt;- crowdflower |&gt; \n  mutate(\n    doc_id = as.character(tweet_id),\n    label_surprise = as.integer(sentiment == \"surprise\")\n    ) |&gt; \n  select(doc_id, label_surprise) |&gt; \n  left_join(\n    convert(crowdflower_dfm, \"data.frame\"),\n    by = \"doc_id\"\n  ) |&gt; \n  select(-doc_id)\n\n# Balanced training dataset\n# set.seed(2)\n# crowdflower_train &lt;- crowdflower_train |&gt; \n#   group_by(label_surprise) |&gt; \n#   slice_sample(n = sum(crowdflower_train$label_surprise==1))\n\n# Ridge Regression (10-fold cross-validation)\nlibrary(caret)\n\ntg &lt;- expand.grid(\n  alpha = 0, \n  lambda = c(2 ^ seq(-1, -6, length = 20))\n  )\n\nset.seed(2)\nsurprise_ridge &lt;- train(\n  label_surprise ~ ., \n  data = crowdflower_train, \n  method = \"glmnet\",\n  tuneGrid = tg,\n  trControl = trainControl(\"cv\", number = 10)\n  )\n\n# Prepare Hippocorpus data to run model\nhippocorpus_features &lt;- hippocorpus_dfm_surprise |&gt; \n  convert(\"data.frame\") |&gt; \n  left_join(\n    convert(hippocorpus_corp, \"data.frame\"),\n    by = \"doc_id\"\n  )\n\n# Run model on Hippocorpus for surprise estimation\nsurprise_pred &lt;- predict(surprise_ridge, newdata = hippocorpus_features)\nhippocorpus_features &lt;- hippocorpus_features |&gt; \n  mutate(surprise_pred = surprise_pred)\n\nNow that we have a new machine-learning-powered estimate of surprise for the Hippocorpus data, we can retest our hypothesis that true autobiographical stories include more surprise than imagined stories.\n\nsurprise_mod_ml &lt;- lm(surprise_pred ~ memType, \n                      data = hippocorpus_features)\nsummary(surprise_mod_ml)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = surprise_pred ~ memType, data = hippocorpus_features)\n#&gt; \n#&gt; Residuals:\n#&gt;       Min        1Q    Median        3Q       Max \n#&gt; -0.004668 -0.004625 -0.004251  0.002972  0.066572 \n#&gt; \n#&gt; Coefficients:\n#&gt;                   Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)      5.735e-02  1.458e-04 393.268   &lt;2e-16 ***\n#&gt; memTyperecalled -4.174e-04  2.058e-04  -2.028   0.0426 *  \n#&gt; memTyperetold   -4.367e-05  2.563e-04  -0.170   0.8647    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.007655 on 6851 degrees of freedom\n#&gt; Multiple R-squared:  0.0006732,  Adjusted R-squared:  0.0003814 \n#&gt; F-statistic: 2.308 on 2 and 6851 DF,  p-value: 0.09958\n\n\nWe find that imagined stories have significantly less surprise-related language than autobiographical stories (p = 0.043)!\nDespite the exciting result, we should be careful with this newfangled approach. As with dictionary-based methods, beware of problems with generalization—there is no guarantee that surprise in Tweets will look similar to surprise in autobiographical accounts. Likewise, keep in mind all of the regular challenges of machine learning. Notice for example that the intercept of this model is extremely low (0.057; surprise is measured between 0 and 1). This reflects the fact that the Crowdflower dataset is not balanced; there are very few Tweets labeled with surprise relative to the size of the dataset.\nAn example of machine learning approaches in research: Zamani et al. (2018) extracted n-grams from Facebook status updates. They then computed TF-IDF scores, binary tokenization, and LDA topics (Chapter 21), subjected all of these values to a dimensionality reduction process to reduce overfitting, and used the resulting features to train a ridge regression model to predict questionnaire-based measures of trustfulness. This regression model could then be used on novel texts to estimate the trustworthiness of their authors.\n\n\n\n\n\n\nAdvantages of Machine Learning Approaches\n\n\n\n\nAccuracy\n\nRegularization: Machine learning algorithms focus on mitigating the influence of outliers. This can sometimes help generalize across datasets too.\n\nAvoid Statistical Troubles: The output of machine learning models is often continuous and more or less normally distributed. This means standard linear regression is usually sufficient for hypothesis testing.\n\n\n\n\n\n\n\n\n\nDisadvantages of Machine Learning Approaches\n\n\n\n\nRequire a Relevant Training Dataset\nDifficult to Interpret\nMay Fail to Generalize Across Datasets\n\n\n\n\n\n\n\n\nBaayen, R. H. (2001). Word frequency distributions. Springer Netherlands. https://link.springer.com/book/10.1007/978-94-010-0844-0\n\n\nGale, W. A., & Sampson, G. (1995). Good‐turing frequency estimation without tears. Journal of Quantitative Linguistics, 2(3), 217–237. https://doi.org/10.1080/09296179508590051\n\n\nGiuntini, F. T., Cazzolato, M. T., Reis, M. de J. D. dos, Campbell, A. T., Traina, A. J. M., & Ueyama, J. (2020). A review on recognizing depression in social networks: Challenges and opportunities. Journal of Ambient Intelligence and Humanized Computing, 11(11), 4713–4729.\n\n\nGood, I. J. (1953). The Population Frequencies of Species and the Estimation of Population Parameters. Biometrika, 40(3/4), 237–264. https://doi.org/10.2307/2333344\n\n\nGood, I. J. (2000). Turing’s anticipation of empirical bayes in connection with the cryptanalysis of the naval enigma. Journal of Statistical Computation and Simulation, 66(2), 101–111. https://doi.org/10.1080/00949650008812016\n\n\nHoltgraves, T. (2011). Text messaging, personality, and the social context. Journal of Research in Personality, 45(1), 92–99. https://doi.org/https://doi.org/10.1016/j.jrp.2010.11.015\n\n\nKronmal, R. A. (1993). Spurious correlation and the fallacy of the ratio standard revisited. Journal of the Royal Statistical Society Series A, 156(3), 379–392. https://www.jstor.org/stable/2983064\n\n\nLaplace, P. S. de. (1816). Essai philosophique sur les probabilités; par M. Le comte Laplace .. M.me v.e Courcier, impr.-libr. pour les mathématiques et la marine, quai des Augustins, no 57. http://archive.org/details/bub_gb_vVzdR0tuoWAC\n\n\nMehl, M. R., Gosling, S. D., & Pennebaker, J. W. (2006). Personality in its natural habitat: Manifestations and implicit folk theories of personality in daily life. Journal of Personality and Social Psychology, 90 5, 862–877. https://api.semanticscholar.org/CorpusID:2932332\n\n\nSchwartz, H. A., Eichstaedt, J. C., Kern, M. L., Dziurzynski, L., Ramones, S. M., Agrawal, M., Shah, A., Kosinski, M., Stillwell, D., Seligman, M. E. P., & Ungar, L. H. (2013). Personality, gender, and age in the language of social media: The open-vocabulary approach. PLOS ONE, 8(9), 1–16. https://doi.org/10.1371/journal.pone.0073791\n\n\nSparck Jones, K. (1972). A statistical interpretation of term specificity and its application in retrieval. Journal of Documentation, 28(1), 11–21. https://doi.org/10.1108/eb026526\n\n\nSumner, C., Byers, A., & Shearing, M. (2011). Determining personality traits & privacy concerns from facebook activity. Black Hat Brief, 11.\n\n\nZamani, M., Buffone, A., & Schwartz, H. A. (2018). Predicting human trustfulness from Facebook language. In K. Loveys, K. Niederhoffer, E. Prud’hommeaux, R. Resnik, & P. Resnik (Eds.), Proceedings of the fifth workshop on computational linguistics and clinical psychology: From keyboard to clinic (pp. 174–181). Association for Computational Linguistics. https://doi.org/10.18653/v1/W18-0619",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Transforming Word Counts</span>"
    ]
  },
  {
    "objectID": "word-counting-improvements.html#footnotes",
    "href": "word-counting-improvements.html#footnotes",
    "title": "16  Transforming Word Counts",
    "section": "",
    "text": "For a good overview of different methods, see Giuntini et al. (2020)↩︎",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Transforming Word Counts</span>"
    ]
  },
  {
    "objectID": "vectorspace-intro.html",
    "href": "vectorspace-intro.html",
    "title": "17  Introduction to Vector Space",
    "section": "",
    "text": "17.1 Distance and Similarity\nWhen we added Elizabeth to the graph above, we could tell that she was more similar to Daniel than to Amos just by looking at the graph. But how do we quantify this similarity or difference?",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Introduction to Vector Space</span>"
    ]
  },
  {
    "objectID": "vectorspace-intro.html#distance-and-similarity",
    "href": "vectorspace-intro.html#distance-and-similarity",
    "title": "17  Introduction to Vector Space",
    "section": "",
    "text": "17.1.1 Euclidean Distance\nThe most straightforward way to measure the similarity between two points in space is to measure the distance between them. Euclidean distance is the simplest sort of distance—the length of the shortest straight line between the two points. The Euclidean distance between two vectors \\(A\\) and \\(B\\) can be calculated in any number of dimensions \\(n\\) using the following formula:\n\\[\nd\\left( A,B\\right)   = \\sqrt {\\sum _{i=1}^{n}  \\left( A_{i}-B_{i}\\right)^2 }\n\\]\nA low Euclidean distance means two vectors are very similar. Let’s calculate the Euclidean distance between Daniel and Elizabeth, and between Amos and Elizabeth:\n\n# dataset\npersonality\n\n#&gt; # A tibble: 3 × 4\n#&gt;   person    extraversion openness neuroticism\n#&gt;   &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 Daniel               4        6           5\n#&gt; 2 Amos                 2        7           3\n#&gt; 3 Elizabeth            8        4           6\n\n# Elizabeth's vector\neliza_vec &lt;- personality |&gt; \n  filter(person == \"Elizabeth\") |&gt; \n  select(extraversion:neuroticism) |&gt; \n  as.numeric()\n\n# Euclidean distance function\neuc_dist &lt;- function(x, y){\n  diff &lt;- x - y\n  sqrt(sum(diff^2))\n}\n\n# distance between Elizabeth and each person\npersonality_dist &lt;- personality |&gt; \n  rowwise() |&gt; \n  mutate(\n    dist_from_eliza = euc_dist(c_across(extraversion:neuroticism), eliza_vec)\n  )\n\npersonality_dist\n\n#&gt; # A tibble: 3 × 5\n#&gt; # Rowwise: \n#&gt;   person    extraversion openness neuroticism dist_from_eliza\n#&gt;   &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;\n#&gt; 1 Daniel               4        6           5            4.58\n#&gt; 2 Amos                 2        7           3            7.35\n#&gt; 3 Elizabeth            8        4           6            0\n\n\nWe now see that the closest person to Elizabeth is… Elizabeth herself, with a distance of 0. After that, the closest is Daniel. So we can conclude that Daniel has a more Elizabeth-like personality than Amos does.\n\n17.1.2 Cosine Similarity\nBesides Euclidean distance, the most common way to measure the similarity between two vectors is with cosine similarity. This is the cosine of the angle between the two vectors. Since the cosine of 0 is 1, a high cosine similarity (close to 1) means two vectors are very similar.\n\n\n\n\n\n\n\n\nA nice thing about the cosine is that it is always between -1 and 1: When the two vectors are pointing in a similar direction, the cosine is close to 1, and when they are pointing in a near-opposite direction (180°), the cosine is close to -1.\nLooking at the above visualization, you might wonder: Why should the angle be fixed at the zero point? What does the zero point have to do with anything? If you wondered this, good job. The reason: Cosine similarity works best when your vector space is centered at zero (or close to it). In other words, it works best when zero represents a medium level of each variable. This fact is sometimes taken for granted because, in practice, most vector spaces are already centered at zero. For example, neural network embeddings (Section 18.3, Chapter 19) can usually be assumed to center at zero because neural networks are almost always trained with a penalty for high values of weights (weight decay) and because many modern neural networks internally rely on dot products between embeddings for their internal representations of data (the dot product is a close cousin of cosine similarity). The ubiquity of zero-centered vector spaces makes cosine similarity a very useful tool. Even so, not all vector spaces are zero-centered, so take a moment to consider the nature of your vector space before deciding which similarity or distance metric to use.\nThe formula for calculating cosine similarity might look a bit complicated:\n\\[\nCosine(A,B) = \\frac{A \\cdot B}{|A||B|} = \\frac{\\sum _{i=1}^{n}  A_{i}B_{i}}{\\sqrt {\\sum _{i=1}^{n} A_{i}^2} \\cdot \\sqrt {\\sum _{i=1}^{n} B_{i}^2}}\n\\] In R though, it’s pretty simple. Let’s calculate the cosine similarity between Elizabeth and each of the other people in our sample. To make sure the vector space is centered at zero, we will subtract 4 from each value (the scales all range from 1 to 7).\n\n# cosine similarity function\ncos_sim &lt;- function(x, y){\n  dot &lt;- x %*% y\n  normx &lt;- sqrt(sum(x^2))\n  normy &lt;- sqrt(sum(y^2))\n  as.vector( dot / (normx*normy) )\n}\n\n# center at 0\neliza_vec_centered &lt;- eliza_vec - 4\npersonality_sim &lt;- personality |&gt; \n  mutate(across(extraversion:neuroticism, ~.x - 4))\n\n# distance between Elizabeth and each person\npersonality_sim &lt;- personality_sim |&gt; \n  rowwise() |&gt; \n  mutate(\n    similarity_to_eliza = cos_sim(c_across(extraversion:neuroticism), eliza_vec_centered)\n  )\n\npersonality_sim\n\n#&gt; # A tibble: 3 × 5\n#&gt; # Rowwise: \n#&gt;   person    extraversion openness neuroticism similarity_to_eliza\n#&gt;   &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;               &lt;dbl&gt;\n#&gt; 1 Daniel               0        2           1               0.2  \n#&gt; 2 Amos                -2        3          -1              -0.598\n#&gt; 3 Elizabeth            4        0           2               1\n\n\nOnce again, we see that the most similar person to Elizabeth is Elizabeth herself, with a cosine similarity of 1. The next closest, as before, is Daniel.\nIf you are comfortable with cosines, you might be happy with the explanation we have given so far. Nevertheless, it might be helpful to consider the relationship between cosine similarity and a more familiar statistic that ranges between -1 and 1: the Pearson correlation coefficient (i.e. regular old correlation). Cosine similarity measures the similarity between two vectors, while the correlation coefficient measures the similarity between two variables. Now just imagine our vectors as variables, with each dimension as an observation. Since we only compare two vectors at a time with cosine similarity, let’s start with Elizabeth and Amos:\n\n\n\n\n\n\n\n\nNow imagine centering those variables at zero, like this:\n\n\n\n\n\n\n\n\nWhen seen like this, the correlation is the same as the cosine similarity. In other words, the correlation between two vectors is the same as the cosine similarity between them when the values of each vector are centered at zero.2 Seeing cosine similarity as the non-centered version of correlation might give you extra intuition for why cosine similarity works best for vector spaces that are centered at zero.",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Introduction to Vector Space</span>"
    ]
  },
  {
    "objectID": "vectorspace-intro.html#sec-word-count-vectors",
    "href": "vectorspace-intro.html#sec-word-count-vectors",
    "title": "17  Introduction to Vector Space",
    "section": "\n17.2 Word Counts as Vector Space",
    "text": "17.2 Word Counts as Vector Space\nThe advantage of thinking in vector space is that we can quantify similarities and differences even without understanding what any of the dimensions in the vector space are measuring. In the coming chapters, we will introduce methods that require this kind of relational thinking, since the dimensions of the vector space are abstract statistical contrivances. Even so, any collection of variables can be thought of as dimensions in a vector space. You might, for example, use distance or similarity metrics to analyze groups of word counts.\nAn example of word counts as relational vectors in research: Ireland & Pennebaker (2010) asked students to answer essay questions written in different styles. They then calculated dictionary-based word counts for both the questions and the answers using 9 linguistic word lists from LIWC (see Section 14.6), including personal pronouns (e.g. “I”, “you”), and articles (e.g, “a”, “the”). They treated these 9 word counts as a 9-dimensional vector for each text, and measured the similarity between questions and responses with a metric similar to Euclidean distance. They found that students automatically matched the linguistic style of the questions (i.e. answers were more similar to the question they were answering than to other questions) and that women and students with higher grades matched their answers especially closely to the style of the questions.\n\n\n\n\n\nAlammar, J. (2019). The illustrated Word2vec. In Jay Alammar – Visualizing machine learning one concept at a time. http://jalammar.github.io/illustrated-word2vec/\n\n\nIreland, M. E., & Pennebaker, J. W. (2010). Language style matching in writing: Synchrony in essays, correspondence, and poetry. Journal of Personality and Social Psychology, 99(3), 549–571.\n\n\nO’Connor, B. (2012). Cosine similarity, Pearson correlation, and OLS coefficients. In AI and Social Science. https://brenocon.com/blog/2012/03/cosine-similarity-pearson-correlation-and-ols-coefficients/",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Introduction to Vector Space</span>"
    ]
  },
  {
    "objectID": "vectorspace-intro.html#footnotes",
    "href": "vectorspace-intro.html#footnotes",
    "title": "17  Introduction to Vector Space",
    "section": "",
    "text": "This section is adapted from Alammar (2019)↩︎\nFor proof, see O’Connor (2012)↩︎",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Introduction to Vector Space</span>"
    ]
  },
  {
    "objectID": "decontextualized-embeddings.html",
    "href": "decontextualized-embeddings.html",
    "title": "18  Word Embeddings",
    "section": "",
    "text": "18.1 The Distributional Hypothesis\nThis textbook assumes that words have psychologically interesting content. For example, certain words are associated with surprise, while others may be associated with concrete thought. But what does it mean for a word to be associated with an emotion or a cognitive process? How do words come to have any meaning at all? One answer: People associate a word with surprise because they often hear it in surprising situations. Because people associate surprise-related words with surprising situations, they use those words more when they are thinking about surprising situations.\nSo teaching a computer to recognize surprise-related words should be simple, right? We’ll just tell the computer to look for words that tend to appear in surprising situations! But there’s a problem: Computers don’t get surprised, and they have no idea what a surprising situation is.\nAccording to the distributional hypothesis, our problem is actually not a problem at all. The computer might not know what surprise is, but it doesn’t need to. It doesn’t need to know what anything is—it just needs to know how everything is related to everything else. To do this, it just needs to notice what appears next to what. Similar words appear in similar contexts. For example, consider the following two sentences from the paper that introduced the distributional hypothesis, Harris (1954, emphasis added).\nEven if we have no idea what “utterances” or “meaning” are, we can learn from these sentences that they must be related somehow, since they both appear together with the word “language.” The more sentences we observe, the more sure we can be about the distributional patterns (i.e. which words tend to have similar words nearby). Words that tend to have very similar words nearby are likely to be similar in meaning, while words that have very different contexts are probably unrelated. Algorithms that learn the meanings of tokens (or at least the relations between their meanings) from these patterns of co-occurrence are called Distributional Semantic Models (DSMs).\nWhen DSMs learn how different meanings are related, they embed those meanings as vectors in a vector space, like this:\nexample_embeddings &lt;- read_csv(\"data/example_embeddings.csv\")\nlibrary(plotly)\nNow that you are comfortable with the concept of a word vector space (Chapter 17), let’s look at how different DSMs embed words and documents into them.",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Word Embeddings</span>"
    ]
  },
  {
    "objectID": "decontextualized-embeddings.html#the-distributional-hypothesis",
    "href": "decontextualized-embeddings.html#the-distributional-hypothesis",
    "title": "18  Word Embeddings",
    "section": "",
    "text": "“The formation of new utterances in the language is therefore based on the distributional relations as changeably perceived by the speakers-among the parts of the previously heard utterances.”\n\n\n“The correlation between language and meaning is much greater when we consider connected discourse.”\n\n\n\n\n\n\n\n\nA Common Misconception\n\n\n\nTwo words are NOT considered similar based on whether they appear together often. Words are similar when they tend to appear in similar contexts. For example, “fridge” and “refrigerator” almost never appear together in the same sentence, but they do tend to appear next to similar groupings of other words (e.g. “food,” “cold,” etc.). LSA, the first DSM we will cover, does not fully address this difficulty.",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Word Embeddings</span>"
    ]
  },
  {
    "objectID": "decontextualized-embeddings.html#sec-lsa",
    "href": "decontextualized-embeddings.html#sec-lsa",
    "title": "18  Word Embeddings",
    "section": "\n18.2 LSA",
    "text": "18.2 LSA\nLatent Semantic Analysis (LSA) is the simplest sort of DSM.1 You can think of it as the linear regression of the embeddings world. In its standard form, LSA is a simple dimensionality reduction technique—singular-value decomposition (SVD)—applied to the DFM. To illustrate what this means, let’s start with a DFM describing the 862 posts on Reddit’s r/relationship_advice that are featured on the cover of this book. For the sake of illustration, we’ll only consider two features of this DFM: the words “I” and “me”.\n\n\n#&gt; Document-feature matrix of: 868 documents, 2 features (3.17% sparse) and 7 docvars.\n#&gt;        features\n#&gt; docs     i me\n#&gt;   post1 38  7\n#&gt;   post2 28 18\n#&gt;   post3 17  7\n#&gt;   post4  9  5\n#&gt;   post5 17  2\n#&gt;   post6 24 13\n#&gt; [ reached max_ndoc ... 862 more documents ]\n\n\nSince we’re only considering two features, we can visualize this DFM in two dimensions:\n\n\n\n\n\n\n\n\nThe terms “I” and “me” are strongly correlated with each other. LSA will recognize this and summarize the I/me-ness of each document with a single number, a combination of the two variables. How does it find this single number?\nIt finds the line of best fit that goes through through the origin (0,0)—the line along which the variables stretch out the most. The direction of that I/me summary line becomes the first dimension of the new embedding space. So each post’s score for this new dimension (i.e. the first number in its embedding vector) represents how far each point is along the direction of the summary line. In the following visualization, each point is projected down onto the summary line. You can see how this squishes the two dimensions of “I” and “me” so that each post can be measured simply by how far it is along the summary line:\n\n\n\n\n\n\n\n\nIn this simple example, we started with two dimensions (“I” and “me”) and reduced them to one. Our new one-dimensional document embeddings measure how far the document is along the line drawn by the LSA.\nIf we wanted a second dimension for the embedding space (which would be silly in this example since we only have two dimensions overall), LSA would draw a second line perpendicular to the first—the line of best fit to the spread not accounted for by the first line. In real applications, of course, we’ll want to use tens of thousands of features, not two. And the embedding vectors we make might be hundreds of dimensions, not one. Nevertheless, the concept remains the same: Lines through the origin are found that best explain the variance between the documents (note that these lines will all be perpendicular to each other). These lines become the new dimensions: The embedding of a document describes the projection of that document point onto each line.\nAs part of the process of producing an embedding for each document, LSA also produces an embedding for each word. In the simple example above, the new one-dimensional word embeddings of “I” and “me” measure how much influence each word has on the summary line from Step 2. In other words, the embedding of “me” is the rise of the line along the y axis, and the embedding of “I” is the run of the line along the x axis. This is equivalent to asking how much I/me-ness is in “I” or how much I/me-ness is in “me.” In a real application with higher dimensional embeddings, the concept remains the same: The LSA embedding of a word describes the weights of that word count on each successive line of best fit.\nPerforming LSA on a training dataset is made easy with the textmodel_lsa() function from the quanteda.textmodels package. Let’s try it on the full DFM from the r/relationship_advice posts, with all 14,897 features instead of just “I” and “me”.\nWe’ll use nd = 100 to reduce these 14,897 dimensions into an embedding space of just 100 dimensions. Why 100 dimensions? Choosing the right dimensionality for LSA can be tricky—too many dimensions make the vector space noisy, but too few dimensions can miss important nuances in meaning. Of course, the larger the training dataset, the more dimensions you can use without overfitting. Notice that the process of finding the line of best fit, and then the next best line perpendicular to the first (and then the next best line perpendicular to both, etc.) guarantees that the first dimensions of the LSA vectors will be the most important ones, and the later ones will be more likely to reflect random noise in the data. 100 or 150 dimensions are popular choices for sufficiently large training sets.\n\nlibrary(quanteda.textmodels)\n\nra_posts_lsa &lt;- ra_posts_dfm |&gt; \n  textmodel_lsa(nd = 100, margin = \"both\")\n\nNow that the model is set up, we can access the word embeddings with ra_posts_lsa$features and the document embeddings with ra_posts_lsa$docs. For example, the embedding of the word “surprised” would be ra_posts_lsa$features[\"surprised\",] and the embedding of the first post in the dataset would be ra_posts_lsa$docs[\"post1\",]. We could also look for the words closest in meaning to the word “surprised” by measuring their cosine similarity with the “surprised” embedding (see Section 17.1.2).\n\nsurprise_embedding &lt;- ra_posts_lsa$features[\"surprised\",]\n\n# cosine similarity function\ncos_sim &lt;- function(x, y){\n  dot &lt;- x %*% y\n  normx &lt;- sqrt(sum(x^2))\n  normy &lt;- sqrt(sum(y^2))\n  as.vector( dot / (normx*normy) )\n}\n\n# measure cosine similarity of each vector to \"surprised\"\nsurprise_words &lt;- ra_posts_lsa$features |&gt; \n  as_tibble(rownames = NA) |&gt; \n  rownames_to_column(\"token\") |&gt; \n  rowwise() |&gt; \n  mutate(\n    surprise = cos_sim(c_across(V1:V100), surprise_embedding)\n  ) |&gt; \n  ungroup()\n\n# find the ten closest words to \"surprised\"\nsurprise_words |&gt; \n  arrange(desc(surprise)) |&gt; \n  slice_head(n = 10) |&gt; \n  pull(token)\n\n#&gt;  [1] \"surprised\" \"peanut\"    \"besides\"   \"child's\"   \"woah\"      \"brick\"    \n#&gt;  [7] \"cases\"     \"oil\"       \"insulin\"   \"trolling\"\n\n\nSome of these are a bit strange—probably we would get better results with a larger dataset—but “woah” and “trolling” do sound pretty surprising. It seems likely that “peanut”, “cases”, “oil”, and “insulin” were learned from posts about surprising allergy incidents.\nWe can also apply the embeddings learned on the r/relationship_advice posts to the now-familiar Hippocorpus data, and use the new surprise scores to retest the hypothesis that true autobiographical stories include more surprise than imagined stories.\n\n# reformat hippocorpus_dfm to match ra_posts_dfm\nhippocorpus_dfm &lt;- hippocorpus_dfm |&gt; \n  dfm_match(featnames(ra_posts_dfm))\n\n# apply LSA model to Hippocorpus data\nhippocorpus_lsa &lt;- predict(ra_posts_lsa, hippocorpus_dfm)\n\n# measure surprise in Hippocorpus \n# (similarity to the word \"surprised\")\nhippocorpus_surprise &lt;- hippocorpus_lsa$docs |&gt; \n  as.matrix() |&gt; \n  as_tibble(rownames = NA) |&gt; \n  rownames_to_column(\"doc_id\") |&gt; \n  rowwise() |&gt; \n  mutate(\n    surprise = cos_sim(c_across(V1:V100), surprise_embedding)\n  ) |&gt; \n  ungroup()\n\n# rejoin docvars\nhippocorpus_surprise &lt;- hippocorpus_surprise |&gt; \n  bind_cols(docvars(hippocorpus_corp))\n\nsurprise_mod_lsa &lt;- lm(surprise ~ memType, hippocorpus_surprise)\n\nsummary(surprise_mod_lsa)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = surprise ~ memType, data = hippocorpus_surprise)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -0.36575 -0.06302 -0.00042  0.06200  0.31361 \n#&gt; \n#&gt; Coefficients:\n#&gt;                 Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)     0.039456   0.001738  22.698  &lt; 2e-16 ***\n#&gt; memTyperecalled 0.011608   0.002453   4.732 2.27e-06 ***\n#&gt; memTyperetold   0.015252   0.003055   4.992 6.13e-07 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.09126 on 6851 degrees of freedom\n#&gt; Multiple R-squared:  0.004902,   Adjusted R-squared:  0.004611 \n#&gt; F-statistic: 16.87 on 2 and 6851 DF,  p-value: 4.895e-08\n\n\nWe found a significant difference between recalled and imagined stories (p &lt; .001), such that recalled stories have more surprise-related language! We also found a significant difference between retold and imagined stories (p &lt; .001), such that retold stories have more surprise-related language. Both of these findings replicate the results of Sap et al. (2022).\n\n18.2.1 Variations on LSA\nEven in the simplified example shown above using word counts for only “I” and “me”, it is easy to see some problems with the standard LSA procedure. First, there is no guarantee that the line of best fit for describing the relationships between word counts will go through the origin. How can we fix this?\nStandard LSA is singular-value decomposition (SVD) applied to a DFM. If you are familiar with principle components analysis (PCA), the explanation of this process above may have sounded familiar. Indeed, PCA is almost the same as SVD, but with one added step at the beginning: centering all the variables at zero. This centering can make a big difference when the line of best fit to your data does not go through the origin. To center a DFM before performing LSA, you can use this function:\n\ndfm_center &lt;- function(dfm) {\n    new(\"dfmSparse\", as((t(apply(dfm, 1, function(x) (x - mean(x))))), \"dgCMatrix\"))\n}\n\nAnother potential problem is that standard LSA gives more weight to common tokens, since common tokens tend to have more variance in their counts (remember that the line of best fit is the one along which the variables spread out the most). This can be remedied by normalizing the DFM before performing the LSA (i.e. transforming all of the counts to z-scores). To do this, you can use this function on your DFM:\n\ndfm_scale &lt;- function(dfm) {\n    new(\"dfmSparse\", as((t(apply(dfm, 1, function(x) (x - mean(x)) / sd(x)))), \"dgCMatrix\"))\n}\n\nTo gain an appreciation for these variations, let’s see what LSA looks like on our “I” and “me” features with centering and normalization:\n\n\n\n\n\n\n\n\nOther problems with LSA are familiar from Chapter 16: For example, LSA can only model linear relationships, but the relationships between word counts are not necessarily linear. In fact, the scatterplots above make it fairly clear that the relationship between the number of “I”s and the number of “we”s in a Reddit post is curved. Similarly, LSA (and SVD in general) works best with normally distributed data (see Rosario, 2001), and word counts are anything but normally distributed. Also, standard LSA is sensitive to text length and may not generalize well to a dataset with texts that are much shorter or much longer than the training set. All of these problems can be remedied using the methods discussed in Chapter 16. For example, one might calculate TF-IDF scores (Section 16.5) before performing LSA to emphasize topical content. Alternatively, one might perform smoothing (Section 16.6) followed by relative tokenization (Section 16.3) and the Anscombe transform (Section 16.4) to standardize word counts across text length and get them closer to a normal distribution.\nAnother difficulty with LSA is that it relies on documents to define the context of words. This works well if each document only deals with one topic (or emotion), but not so well with documents that include multiple topics. One solution to this (if you have reasonably long texts) is to use a moving context window: Extract all segments of, say, 10 words, and use each one as a separate document for training the LSA. This can be accomplished in R by applying the following code to your texts before tokenization:\n\nexample_texts &lt;- c(\n  \"One solution to this is to use a moving context window\",\n  \"extracting all segments of, say, 10 words, and using each one as a separate document for training the LSA.\"\n)\n\n# function to split text with moving window\nstr_split_window &lt;- function(string, window_size){\n  nwords &lt;- str_count(string, \" \") + 1L\n  out &lt;- lapply(1:length(string), function(s) {\n    sapply((window_size + 1L):nwords[s], function(i) word(string[s], i-window_size, i))\n  })\n  unlist(out)\n}\n\nstr_split_window(example_texts, 10)\n\n#&gt;  [1] \"One solution to this is to use a moving context window\"       \n#&gt;  [2] \"extracting all segments of, say, 10 words, and using each one\"\n#&gt;  [3] \"all segments of, say, 10 words, and using each one as\"        \n#&gt;  [4] \"segments of, say, 10 words, and using each one as a\"          \n#&gt;  [5] \"of, say, 10 words, and using each one as a separate\"          \n#&gt;  [6] \"say, 10 words, and using each one as a separate document\"     \n#&gt;  [7] \"10 words, and using each one as a separate document for\"      \n#&gt;  [8] \"words, and using each one as a separate document for training\"\n#&gt;  [9] \"and using each one as a separate document for training the\"   \n#&gt; [10] \"using each one as a separate document for training the LSA.\"\n\n\nAn example of LSA in research: Moss et al. (2006) asked mechanical engineering students to write brief descriptions of devices that were presented in diagrams. They then performed LSA on these descriptions, reducing them to a 100 dimensional embedding space. They then found the embeddings of an existing dictionary of function-related words (e.g. “actuate”, “adjust”, “control”), and averaged them to produce a vector representing the function of devices. Finally, they computed cosine similarity between this vector and that of each document. They found that fourth-year engineering students used more functional language than first-year students.\n\n\n\n\n\n\nAdvantages of LSA\n\n\n\n\n\nContext-Based Model: LSA captures the correlations between tokens in texts. This is an improvement on simple word counting methods that can miss subtle patterns in language use.\n\nSimplicity: Since many psychology researchers are familiar with PCA, LSA may feel like less of a black box than more modern methods.\n\nEasy Integration With Transformations: Since LSA is so straightforward, it is easy to integrate with methods from Chapter 16.\n\n\n\n\n\n\n\n\n\nDisadvantages of LSA\n\n\n\n\nAssumes Linearity\nWorks Best With Normal Distributions\n\nRelies on Documents as Context: LSA works best when documents have only one topic each.\n\nPrioritizes Common Words: This can be fixed by adding a normalization step.",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Word Embeddings</span>"
    ]
  },
  {
    "objectID": "decontextualized-embeddings.html#sec-word-embeddings",
    "href": "decontextualized-embeddings.html#sec-word-embeddings",
    "title": "18  Word Embeddings",
    "section": "\n18.3 Advanced Word Embeddings",
    "text": "18.3 Advanced Word Embeddings\nLSA is a good baseline for word embeddings, but as we have seen, it suffers from many of the familiar problems associated with word counts: difficulties with nonlinear relationships, non-normal distributions, etc.\nLSA also suffers from an even more fundamental problem. Recall the warning from the beginning of this chapter: Two words are NOT considered similar based on whether they appear together often. Words are similar when they tend to appear in similar contexts. LSA is fundamentally based on global patterns of covariance in the DFM. Because synonyms rarely appear together in the same document (i.e. their counts are likely to be negatively correlated), their embeddings will be further apart in the vector space than they really should be. More modern techniques for embedding words fix this problem as well as the others with model architectures that are carefully tailored for capturing meaning.\n\n18.3.1 Word2vec\nWord2vec was first introduced by Mikolov, Chen, et al. (2013) and was refined by Mikolov, Sutskever, et al. (2013). They proposed a few variations on a simple neural network2 that learns the relationships between words and contexts. Here we describe the most commonly used variation—continuous Skip-gram with negative sampling.3 Imagine training the model on the following sentence:\n\nCoding can be frustrating.\n\nOur Skip-gram training dataset would have one column for the input word, and another column for words from its immediate context. It is called “continuous” because it slides a context window along the training text (Section 18.2.1), considering each word as input, and the words immediately around it (e.g. 10 before and 10 after) as context, like this:\n\n\n#&gt; # A tibble: 12 × 2\n#&gt;    word        context    \n#&gt;    &lt;chr&gt;       &lt;chr&gt;      \n#&gt;  1 coding      can        \n#&gt;  2 coding      be         \n#&gt;  3 coding      frustrating\n#&gt;  4 can         coding     \n#&gt;  5 can         be         \n#&gt;  6 can         frustrating\n#&gt;  7 be          coding     \n#&gt;  8 be          can        \n#&gt;  9 be          frustrating\n#&gt; 10 frustrating coding     \n#&gt; 11 frustrating can        \n#&gt; 12 frustrating be\n\n\nThe negative sampling method adds more rows to the training set, this time from words and contexts that do not go together, drawn at random from other parts of the corpus. A third column indicates whether the pair of words are really neighbors or not:\n\n\n#&gt; # A tibble: 12 × 3\n#&gt;    word   context     neighbors\n#&gt;    &lt;chr&gt;  &lt;chr&gt;           &lt;dbl&gt;\n#&gt;  1 coding can                 1\n#&gt;  2 coding be                  1\n#&gt;  3 coding frustrating         1\n#&gt;  4 can    coding              1\n#&gt;  5 can    be                  1\n#&gt;  6 can    frustrating         1\n#&gt;  7 coding happy               0\n#&gt;  8 coding olive               0\n#&gt;  9 coding jump                0\n#&gt; 10 can    happy               0\n#&gt; 11 can    olive               0\n#&gt; 12 can    jump                0\n\n\nThe word2vec model takes the first two columns as input and tries to predict whether the two words are neighbors or not. It does this by learning two separate sets of embeddings: word embeddings and context embeddings.\n\n\n\n\n\n\n\n\nFor each row of the training set, the model looks up the embedding for the target word and the embedding for the context word, and computes the dot product between the two vectors. The dot product is closely related to the cosine similarity, which we discussed in Section 17.1.2—it measures how similar the two embeddings are. If the dot product is large (i.e. the word embedding and the context embedding are very similar), the model predicts that the two words are likely to be real neighbors. If the dot product is small, the model predicts that the two words were probably sampled at random.4 During training, the model learns which word embeddings and context embeddings will do best at this binary prediction task.\nNotice that word2vec (and fastText and GloVe) give each word two embeddings: one for when the word is the target and another for when it is the context (Goldberg & Levy, 2014). This may seem strange, but it actually solves two important problems with LSA:\n\n\nA Nuance of the Distributional Hypothesis. Recall the case of “fridge” and “refrigerator”, which almost never appear together in the same sentence, but do tend to appear next to similar groupings of other words. Because LSA is based directly on broad patterns of covariance in word frequencies, it will pick up on the fact that “fridge” and “refrigerator” are negatively correlated and push them further apart than they should be. Word2vec, on the other hand, can learn a context embedding for “refrigerator” that is not so close to the word embedding for “fridge”, even when the word embeddings of the two words are very close. This allows word2vec to recognize that “refrigerator” and “fridge” tend to appear in similar contexts, but are unlikely to appear together. In this way, word2vec is truer to the distributional hypothesis than LSA.\n\nAssociative Asymmetry. The cosine similarity between two word embeddings gives the best estimate of conceptual similarity (Torabi Asr et al., 2018). This is because conceptual similarity is not the same as association in language (or in the mind). In fact, psycholinguists have long known that human associations between two words are asymmetric. For example, people prompted with “leopard” are much more likely to think of “tiger” than people prompted with “tiger” are to think of “leopard” (Tversky & Gati, 1982). These sorts of associative connections are closely tied to probabilities of co-occurrence in language and are therefore much better represented by the cosine similarity (or even the dot product) between a word embedding and a context embedding (Torabi Asr et al., 2018). Thus the association between “leopard” and “tiger” would be represented by the similarity between the word embedding of “leopard” and the context embedding of “tiger”, allowing for the asymmetry observed in mental associations.5 Since LSA only produces one embedding per word, it cannot capture this asymmetry.\n\nWord2vec was revolutionary when it came out. The main reason for this is the efficiency of the training process. This efficiency means that the model can be trained on massive datasets. Larger and more diverse datasets mean more reliable embeddings. A few pretrained models can be easily downloaded from the Internet (e.g. from here or here). Because these models are trained on very large datasets and are already known to perform well, it almost never makes sense to train your own word2vec from scratch.\nOnce you’ve downloaded a pretrained model (generally as a .bin file), you can open it in R with the word2vec package. Here we’ll be using a model trained on the entirety of Google news, downloaded from here, which uses 300-dimensional embeddings.\n\nlibrary(word2vec)\n\n# model file path\nword2vec_mod &lt;- \"data/GoogleNews-vectors-negative300.bin\"\n\n# open model\nword2vec_mod &lt;- read.word2vec(file = word2vec_mod, normalize = TRUE)\n\nTo find embeddings of specific words, use predict(word2vec_mod, c(\"word1\", \"word2\"), type = \"embedding\"). To get embeddings for full documents, average the embeddings of the words in the document. Here we provide a function to compute document embeddings directly from a DFM.\n\ntextstat_embedding &lt;- function(dfm, model){\n  feats &lt;- featnames(dfm)\n  # find word embeddings\n  feat_embeddings &lt;- predict(model, feats, type = \"embedding\")\n  feat_embeddings[is.na(feat_embeddings)] &lt;- 0\n  # average word embeddings of each document\n  out_mat &lt;- (dfm %*% feat_embeddings)/ntoken(dfm)\n  as_tibble(as.matrix(out_mat), rownames = \"doc_id\")\n}\n\nLet’s use word2vec embeddings and cosine similarity to reanalyze the Hippocorpus data.\n\n# embedding of the word \"surprised\"\nsurprise_embedding &lt;- predict(word2vec_mod, \"surprised\", type = \"embedding\") |&gt; \n  as.vector()\n\n# document embeddings\nhippocorpus_word2vec &lt;- hippocorpus_dfm |&gt; \n  textstat_embedding(word2vec_mod)\n\n# score documents by surprise\nhippocorpus_surprise_word2vec &lt;- hippocorpus_word2vec |&gt; \n  rowwise() |&gt; \n  mutate(\n    surprise = cos_sim(c_across(V1:V300), surprise_embedding)\n  ) |&gt; \n  ungroup() |&gt; \n  select(-c(V1:V300))\n\n# rejoin docvars\nhippocorpus_surprise_word2vec &lt;- hippocorpus_surprise_word2vec |&gt; \n  bind_cols(docvars(hippocorpus_corp))\n\nsurprise_mod_word2vec &lt;- lm(surprise ~ memType, hippocorpus_surprise_word2vec)\n\nsummary(surprise_mod_word2vec)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = surprise ~ memType, data = hippocorpus_surprise_word2vec)\n#&gt; \n#&gt; Residuals:\n#&gt;       Min        1Q    Median        3Q       Max \n#&gt; -0.129976 -0.014663  0.001045  0.015047  0.086923 \n#&gt; \n#&gt; Coefficients:\n#&gt;                   Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)      0.3154892  0.0004298 733.972  &lt; 2e-16 ***\n#&gt; memTyperecalled -0.0046029  0.0006066  -7.588 3.69e-14 ***\n#&gt; memTyperetold   -0.0034789  0.0007555  -4.605 4.21e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.02257 on 6851 degrees of freedom\n#&gt; Multiple R-squared:  0.008743,   Adjusted R-squared:  0.008454 \n#&gt; F-statistic: 30.21 on 2 and 6851 DF,  p-value: 8.629e-14\n\n\nOnce again we found a significant difference between recalled and imagined stories (p &lt; .001), but this time in the opposite direction—recalled stories have less surprise-related language! We also found a significant difference between retold and imagined stories (p &lt; .001) such that retold stories have less surprise-related language. These findings seem to go against those of Sap et al. (2022), but stick around for Chapter 20 to learn why they might be misleading.\nAn example of word2vec in research: Chatterjee et al. (2023) used word2vec to study the phenomenon of nominative determinism—the purported tendency to chose a profession or city with a first letter that matches the first letter of one’s name (e.g. someone named Louis might choose to be a language researcher). They first used a word2vec model trained on Google News to obtain embeddings for 3,410 first names, 508 professions, and 14,856 US cities. They then averaged the embeddings of all names/professions/cities that begin with the same letter to obtain a vector representing names that begin with the letter “A”, a vector representing professions that begin with the letter “A”, etc. Using cosine similarity, they found that same-letter names and professions (e.g. Albert and Actuary) tend to be more similar than different-letter names and professions (e.g. Albert and Dentist), even when controlling for gender, ethnicity, and frequency. They found a similar pattern for names and cities.\n\n\n\n\n\n\nAdvantages of Word2vec\n\n\n\n\n\nAccurately Represents Meaning: By distinguishing between target and context words, word2vec stays true to the distributional hypothesis. Since it is not based on counts, it also avoids problems with non-linear relationships.\n\nEfficient for Large Datasets: This means that models can be trained on enormous amounts of text. Some such models are available for download on the Internet.\n\n\n\n\n\n\n\n\n\nDisadvantages of Word2vec\n\n\n\n\n\nRelies on Word-Level Meaning: Word2vec assumes that each word has only one meaning. This means that it has trouble with words that can mean more than one thing (e.g. deep learning model vs. fashion model). Word2vec will learn the average of these meanings.\n\nWorks Best in English: English words are generally spelled the same no matter where they are in a sentence. Word2vec doesn’t work as well for languages that have more prefixes, suffixes, conjugations, etc., since it has to relearn the meaning for each form of the word.\nNot Many Pretrained Models Available\n\n\n\n\n18.3.2 GloVe\nWord2vec produces spectacularly rich and reliable vector embeddings, but their reliance on randomly sampled pairs of words and contexts makes them somewhat noisy and overly sensitive to frequent tokens. The developers of word2vec managed to fix these problems by strategically filtering the training dataset, but Pennington et al. (2014) came up with a more elegant solution: Global Vectors (GloVe) is designed on the same principles of word2vec, but it is computed from global patterns of co-occurrence rather than individual examples.6\nEven though GloVe uses a different method of training, the embeddings it generates are very similar to those generated by word2vec. Because GloVe embeddings are so similar to word2vec embeddings, we will not go into detail here about the way the GloVe algorithm works. Nevertheless, GloVe does have one very important advantage over word2vec: Better pretrained models are available online. Whereas the most easily available word2vec model is trained on news, the GloVe website offers models trained on social media (glove.twitter.27B.zip) and on large portions of the Internet (Common Crawl). These models generalize better to social media texts (since they were trained on similar texts) and are likely to have richer representations of emotional or social content, since more examples of that content appear on social media than in the news or on Wikipedia.\nSince the pretrained GloVe models are available in .txt format, you don’t need a wrapper package to use them in R. Simply download the pretrained model, input the path to the file as path_to_glove, and run the following code:\n\npath_to_glove &lt;- \"data/glove/glove.twitter.27B.100d.txt\"\ndimensions &lt;- as.numeric(str_extract(path_to_glove, \"[:digit:]+(?=d\\\\.txt)\"))\n\n# matrix with token embeddings\nglove_pretrained &lt;- read_delim(\n  path_to_glove, \n  delim = \" \",\n  quote = \"\",\n  escape_double = FALSE,\n  col_names = c(\"token\", paste0(\"dim_\", 1:dimensions))\n) |&gt; column_to_rownames(\"token\") |&gt; as.matrix()\n\n# update class to \"embeddings\" (required for `predict.embeddings` function)\nclass(glove_pretrained) &lt;- \"embeddings\"\n\n# function to retrieve embeddings\n#   `object`: an \"embeddings\" object (matrix with character rownames)\n#   `newdata`: a character vector of tokens\n#   `type`: 'embedding' gives the embeddings of newdata. \n#           'nearest' gives nearest embeddings by cosine similarity \n#           (requires the cos_sim function)\n#   `top_n`: for `type = 'nearest'`, how many nearest neighbors to output?\npredict.embeddings &lt;- function(object, newdata, \n                               type = c(\"embedding\", \"nearest\"), \n                               top_n = 10L){\n  embeddings &lt;- as.matrix(object)\n  if (type == \"embedding\") {\n    embeddings[newdata,]\n  }else{\n    if(length(newdata) &gt; 1){\n      target &lt;- as.vector(apply(embeddings[newdata,], 2, mean))\n    }else{\n      target &lt;- as.vector(embeddings[newdata,])\n    }\n    sims &lt;- apply(object, 1, cos_sim, target)\n    embeddings &lt;- embeddings[rev(order(sims)),]\n    head(embeddings, top_n)\n  }\n}\n\nYou can then proceed just as we did for word2vec, using the textstat_embedding() function provided in that section to compute document embeddings directly from a DFM.\n\n18.3.2.1 Training a Custom GloVe Model\nSince excellent pretrained GloVe embeddings are available online, it rarely makes sense to train your own model. Nevertheless, GloVe’s elegant training procedure makes for easy integration with Quanteda. A tutorial on training a custom GloVe model in Quanteda can be found here.\nWhy might you want to train a custom word embeddings model? Maybe you are interested in quantifying differences in individual word use between multiple large groups of text. For example, you might train a GloVe model on texts written by conservatives and another on texts written by liberals, and demonstrate that the word “skirt” is closer to the word “woman” in conservative language than it is in liberal language.\n\n\n\n\n\n\nAdvantages of GloVe\n\n\n\n\nElegant Training Procedure\nPsychologically Sensitive Pretrained Models\n\n\n\n\n\n\n\n\n\nDisadvantages of GloVe\n\n\n\n\nRequires Large Training Sets\nRelies on Word-Level Meaning\nWorks Best in English\n\n\n\n\n18.3.3 FastText\nFastText (Bojanowski et al., 2017) is a specialized version of word2vec, designed to work with languages in which words take different forms depending on their grammatical place. Rather than learning a word embedding and a context embedding for each full word (e.g. “quantify” and “quantification” each get their own embedding), fastText learns a vector for each shingle within a word (see Section 13.1.5). For example, “quantify” might be broken up into “quant”, “uanti”, “antif”, and “ntify”. But it doesn’t treat each shingle as its own word. Rather, it trains on words just like word2vec and GloVe, but makes sure that the embedding of a word is equal to the sum of all of the shingle vectors inside it.\nThis approach is mostly unnecessary for English, where words are generally spelled the same wherever they appear. But for more morphologically rich languages like Hebrew, Arabic, French, or Finnish, fastText works much better than word2vec and GloVe. This is because there might not be enough data for word2vec and GloVe to learn reliable representations of every form of every word, especially rare forms. FastText, on the other hand, can focus on the important subcomponents of the words that stay the same across different forms. This way it can learn rich representations even of rare forms of a word that don’t appear in the training dataset (e.g. it could quantify the meaning of מחשבותייך even if it were only trained on מחשבה, מחשבות, חבר, and חברייך).\nAfter downloading a pretrained model from this page (Grave et al., 2018), you can use fastText in R through the fastTextR package. Conveniently, fastTextR includes a dedicated function for obtaining full text embeddings, ft_sentence_vectors().\n\nlibrary(fastTextR)\n\n# example texts\nheb_words &lt;- c(\"מחשבותייך\", \"מחשבה\")\nheb_texts &lt;- c(\"אל תחשוב שנקרא לשוננו לשון הקדש לגאותינו או לטעותינו, אבל הוא בדין, כי זה הלשון קדוש לא ימצאו בו שמות לאבר הבעילה בזכר או בנקבה\", \"הטעם שהזכיר על דעתי איננו אמת, כי מה שיכנו ישגלנה, ישכבנה, יורה כי משגל שם עצם לבעילה\")\n\n# load pretrained model from file\nheb_model &lt;- ft_load(\"data/cc.he.300.bin\")\n\n# get word embeddings\nword_vecs &lt;- ft_word_vectors(heb_model, heb_words)\n\n# get text embeddings\ntext_vecs &lt;- ft_sentence_vectors(heb_model, heb_texts)\n\n\n\n\n\n\n\nAdvantages of FastText\n\n\n\n\nBetter for Morphologically Rich Languages\nBetter for Rare Words\nCan Infer Embeddings for Words That Were Not in Training Data\n\n\n\n\n\n\n\n\n\nDisadvantages of FastText\n\n\n\n\n\nMore Complex: This means larger files to download when using pretrained models. It also increases the risk of overfitting.\n\n\n\n\n\n\n\n\nAlammar, J. (2019). The illustrated Word2vec. In Jay Alammar – Visualizing machine learning one concept at a time. http://jalammar.github.io/illustrated-word2vec/\n\n\nBojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). Enriching word vectors with subword information. https://arxiv.org/abs/1607.04606\n\n\nChatterjee, P., Mishra, H., & Mishra, A. (2023). Does the first letter of one’s name affect life decisions? A natural language processing examination of nominative determinism. Journal of Personality and Social Psychology, 125. https://doi.org/10.1037/pspa0000347\n\n\nDeerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R. A. (1990). Indexing by latent semantic analysis. Journal of the Association for Information Science and Technology, 41(6), 391–407. https://doi.org/10.1002/(SICI)1097-4571(199009)41:6&lt;391::AID-ASI1&gt;3.0.CO;2-9\n\n\nGoldberg, Y., & Levy, O. (2014). word2vec explained: Deriving mikolov et al.’s negative-sampling word-embedding method. https://arxiv.org/abs/1402.3722\n\n\nGrave, E., Bojanowski, P., Gupta, P., Joulin, A., & Mikolov, T. (2018). Learning word vectors for 157 languages. Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018).\n\n\nHarris, Z. S. (1954). Distributional Structure. WORD, 10(2-3), 146–162. https://doi.org/10.1080/00437956.1954.11659520\n\n\nMikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. https://arxiv.org/abs/1301.3781\n\n\nMikolov, T., Sutskever, I., Chen, K., Corrado, G., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality. https://arxiv.org/abs/1310.4546\n\n\nMoss, J., Kotovsky, K., & Cagan, J. (2006). The Role of Functionality in the Mental Representations of Engineering Students: Some Differences in the Early Stages of Expertise. Cognitive Science, 30(1), 65–93. https://doi.org/10.1207/s15516709cog0000_45\n\n\nOyama, M., Yokoi, S., & Shimodaira, H. (2023). Norm of word embedding encodes information gain. https://arxiv.org/abs/2212.09663\n\n\nPennington, J., Socher, R., & Manning, C. D. (2014). GloVe: Global vectors for word representation. Empirical Methods in Natural Language Processing (EMNLP), 1532–1543. http://www.aclweb.org/anthology/D14-1162\n\n\nRosario, B. (2001). Latent Semantic Indexing : An Overview 1 Latent Semantic Indexing : An overview INFOSYS 240 Spring 2000 Final Paper. https://www.semanticscholar.org/paper/Latent-Semantic-Indexing-%3A-An-Overview-1-Latent-%3A-Rosario/95981f057cb76a24329fcf2b572f75d8c2b1613e#citing-papers\n\n\nSap, M., Jafarpour, A., Choi, Y., Smith, N. A., Pennebaker, J. W., & Horvitz, E. (2022). Quantifying the narrative flow of imagined versus autobiographical stories. Proceedings of the National Academy of Sciences, 119(45), e2211715119. https://doi.org/10.1073/pnas.2211715119\n\n\nSchakel, A. M. J., & Wilson, B. J. (2015). Measuring word significance using distributed representations of words. https://arxiv.org/abs/1508.02297\n\n\nTorabi Asr, F., Zinkov, R., & Jones, M. (2018). Querying word embeddings for similarity and relatedness. In M. Walker, H. Ji, & A. Stent (Eds.), Proceedings of the 2018 conference of the north American chapter of the association for computational linguistics: Human language technologies, volume 1 (long papers) (pp. 675–684). Association for Computational Linguistics. https://doi.org/10.18653/v1/N18-1062\n\n\nTversky, A., & Gati, I. (1982). Similarity, separability, and the triangle inequality. Psychological Review, 89, 123–154. https://doi.org/10.1037/0033-295X.89.2.123",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Word Embeddings</span>"
    ]
  },
  {
    "objectID": "decontextualized-embeddings.html#footnotes",
    "href": "decontextualized-embeddings.html#footnotes",
    "title": "18  Word Embeddings",
    "section": "",
    "text": "LSA was first introduced by Deerwester et al. (1990).↩︎\nSome people think word2vec is too simple to be called a neural network. If you are one of these people, you are welcome to think of word2vec as a fancy sort of logistic regression instead.↩︎\nThis section is partially adapted from Alammar (2019)↩︎\nCosine similarity is the dot product of two normalized vectors. In other words, the dot product is the same as cosine similarity, except that it gets larger as the vectors get farther away from the origin. In dot-product-based models like word2vec, the distance of a word vector from the origin (also called the norm or magnitude) is proportional to the informativeness of the word (Oyama et al., 2023; Schakel & Wilson, 2015). Therefore, the norm of the embedding measures how representative it is of certain contexts as opposed to others, similar to averaging the TF-IDF of a word across the corpus (Section 16.5). This makes intuitive sense, because vectors with a larger norm will result in very positive or very negative dot products, making the model more confident in the pair of words either being neighbors or not. Since the informativeness of words is often irrelevant to analyses of similarity, cosine similarity is used to discount it. Ways to incorporate it in analyses will be discussed in Chapter 20 and Chapter 22.↩︎\nTo the best of our knowledge, pretrained context embeddings are not available online. So if you are interested in associative (rather than conceptual) relationships between words, we recommend training your own model (see Section 18.3.2.1).↩︎\nGloVe is built on the same metric that we used in Chapter 15: relative frequency ratios. Rather than comparing two word frequencies in two groups of texts as we did in that chapter, it instead compares co-occurrence with one word to co-occurrence with another.↩︎",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Word Embeddings</span>"
    ]
  },
  {
    "objectID": "contextualized-embeddings.html",
    "href": "contextualized-embeddings.html",
    "title": "19  Contextualization and Large Language Models",
    "section": "",
    "text": "19.1 Hugging Face and the text Package\nO. N. E. Kjell et al. (2021)\nO. Kjell et al. (2022)",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Contextualization and Large Language Models</span>"
    ]
  },
  {
    "objectID": "contextualized-embeddings.html#distributed-dictionary-representation-ddr",
    "href": "contextualized-embeddings.html#distributed-dictionary-representation-ddr",
    "title": "19  Contextualization and Large Language Models",
    "section": "\n19.2 Distributed Dictionary Representation (DDR)",
    "text": "19.2 Distributed Dictionary Representation (DDR)",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Contextualization and Large Language Models</span>"
    ]
  },
  {
    "objectID": "contextualized-embeddings.html#contextualized-construct-representation-ccr",
    "href": "contextualized-embeddings.html#contextualized-construct-representation-ccr",
    "title": "19  Contextualization and Large Language Models",
    "section": "\n19.3 Contextualized Construct Representation (CCR)",
    "text": "19.3 Contextualized Construct Representation (CCR)\nAtari et al. (2023)",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Contextualization and Large Language Models</span>"
    ]
  },
  {
    "objectID": "contextualized-embeddings.html#correlational-methods",
    "href": "contextualized-embeddings.html#correlational-methods",
    "title": "19  Contextualization and Large Language Models",
    "section": "\n19.4 Correlational Methods",
    "text": "19.4 Correlational Methods\n\n\n\n\nAtari, M., Omrani, A., & Dehghani, M. (2023). Contextualized construct representation: Leveraging psychometric scales to advance theory-driven text analysis. PsyArXiv. https://doi.org/10.31234/osf.io/m93pd\n\n\nKjell, O. N. E., Giorgi, S., & Schwartz, H. A. (2021). The text-package: An r-package for analyzing and visualizing human language using natural language processing and deep learning. PsyArXiv. https://doi.org/10.31234/osf.io/293kt\n\n\nKjell, O., Sikström, S., Kjell, K., & Schwartz, H. (2022). Natural language analyzed with AI-based transformers predict traditional subjective well-being measures approaching the theoretical upper limits in accuracy. Scientific Reports, 12, 3918. https://doi.org/10.1038/s41598-022-07520-w",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Contextualization and Large Language Models</span>"
    ]
  },
  {
    "objectID": "navigating-vectorspace.html",
    "href": "navigating-vectorspace.html",
    "title": "20  Navigating Vector Space",
    "section": "",
    "text": "20.1 Parallelograms\nIntroduced with word2vec by Mikolov et al. (2013)\nGlove (Pennington et al., 2014) is designed with this property in mind. Transformer models are not.\nIn CCR, for example, there’s a difference between agreeing with statements in a questionnaire and actually writing like a questionnaire. How do I know that I’m measuring how anxious/depressed the comments are and not how questionnaire-y they are? Potential workaround: reverse all of the statements in the questionnaire (as for reverse-coded items), add them to the original questionnaire, and get the average embedding of the construct-neutral questionnaire. Then subtract this embedding from the original questionnaire average embedding.",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Navigating Vector Space</span>"
    ]
  },
  {
    "objectID": "navigating-vectorspace.html#sec-advanced-similarity",
    "href": "navigating-vectorspace.html#sec-advanced-similarity",
    "title": "20  Navigating Vector Space",
    "section": "\n20.2 Advanced Similarity Measures",
    "text": "20.2 Advanced Similarity Measures\n\n20.2.1 Dot Product\n\n20.2.2 Jaccard similarity\n\n20.2.3 Mutual Information\n\n20.2.4 Jensen–Shannon divergence",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Navigating Vector Space</span>"
    ]
  },
  {
    "objectID": "navigating-vectorspace.html#sec-dimension-projection",
    "href": "navigating-vectorspace.html#sec-dimension-projection",
    "title": "20  Navigating Vector Space",
    "section": "\n20.3 Dimension Projection",
    "text": "20.3 Dimension Projection\nGrand et al. (2022)\nAn example of using dimension projection in research: Simchon et al. (2023) collected 10,000 messages from the r/depression subreddit, along with a control group of 100 messages each from 100 randomly selected subreddits. They then used a variant of the SBERT model called all-MiniLM-L6-v2 to compute CCR embeddings (Chapter 19) of a psychological questionnaire measuring “locus of control,” the feeling that you have control over your own life. The questionnaire included items measuring an internal locus of control (“I have control”), and items measuring an external locus of control (“External forces have control”). Simchon et al. constructed an anchored vector to capture the direction between internal and external locus of control, and projected embeddings of the Reddit posts onto that vector to measure how much each post reflected an internal vs. an external locus of control. They found that posts in r/depression exhibited a more external locus of control than posts in the control group.",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Navigating Vector Space</span>"
    ]
  },
  {
    "objectID": "navigating-vectorspace.html#sec-machine-learning-methods",
    "href": "navigating-vectorspace.html#sec-machine-learning-methods",
    "title": "20  Navigating Vector Space",
    "section": "\n20.4 Machine Learning Methods",
    "text": "20.4 Machine Learning Methods\nChersoni et al. (2021) used partial least squares regression to map distributional embeddings to human-rated semantic features derived from research in cognitive psychology.\n\n\n\n\nChersoni, E., Santus, E., Huang, C.-R., & Lenci, A. (2021). Decoding word embeddings with brain-based semantic features. Computational Linguistics, 47(3), 663–698. https://doi.org/10.1162/coli_a_00412\n\n\nGrand, G., Blank, I. A., Pereira, F., & Fedorenko, E. (2022). Semantic projection recovers rich human knowledge of multiple object features from word embeddings. Nature Human Behaviour, 6(7), 975–987. https://doi.org/10.1038/s41562-022-01316-8\n\n\nGünther, F., Rinaldi, L., & Marelli, M. (2019). Vector-space models of semantic representation from a cognitive perspective: A discussion of common misconceptions. Perspectives on Psychological Science, 14(6), 1006–1033. https://doi.org/10.1177/1745691619861372\n\n\nMikolov, T., Yih, W., & Zweig, G. (2013). Linguistic regularities in continuous space word representations. In L. Vanderwende, H. Daumé III, & K. Kirchhoff (Eds.), Proceedings of the 2013 conference of the north American chapter of the association for computational linguistics: Human language technologies (pp. 746–751). Association for Computational Linguistics. https://aclanthology.org/N13-1090\n\n\nPennington, J., Socher, R., & Manning, C. D. (2014). GloVe: Global vectors for word representation. Empirical Methods in Natural Language Processing (EMNLP), 1532–1543. http://www.aclweb.org/anthology/D14-1162\n\n\nSimchon, A., Hadar, B., & Gilead, M. (2023). A computational text analysis investigation of the relation between personal and linguistic agency. Communications Psychology, 1–9. https://doi.org/10.1038/s44271-023-00020-1",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Navigating Vector Space</span>"
    ]
  },
  {
    "objectID": "lda.html",
    "href": "lda.html",
    "title": "21  Topic Modeling",
    "section": "",
    "text": "This page is still under construction. Come back soon!\n\n\n\n\n\n\nPoldrack et al. (2012)\n\n21.0.1 Supervised LDA\nsLDA in R\n\n21.0.2 Semi-Supervised LDA\nseededLDA in R\n\n\n\n\n\n\nAdvantages of Topic Modeling\n\n\n\n\n:\n\n\n\n\n\n\n\n\n\nDisadvantages of Topic Modeling\n\n\n\n\n:\n\n\n\n\n\n\n\nPoldrack, R. A., Mumford, J. A., Schonberg, T., Kalar, D., Barman, B., & Yarkoni, T. (2012). Discovering Relations Between Mind, Brain, and Mental Disorders Using Topic Mapping. PLOS Computational Biology, 8(10), e1002707. https://doi.org/10.1371/journal.pcbi.1002707",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Topic Modeling</span>"
    ]
  },
  {
    "objectID": "linguistic-complexity.html",
    "href": "linguistic-complexity.html",
    "title": "22  Measures of Linguistic Complexity",
    "section": "",
    "text": "This section has not yet been written. Check again in a few months!",
    "crumbs": [
      "Quantifying Psychological Properties of Text",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Measures of Linguistic Complexity</span>"
    ]
  },
  {
    "objectID": "audio-video-image.html",
    "href": "audio-video-image.html",
    "title": "Audio, Video, and Image Data",
    "section": "",
    "text": "This section has not yet been written. Check again in a few months!",
    "crumbs": [
      "Audio, Video, and Image Data"
    ]
  },
  {
    "objectID": "transcription.html",
    "href": "transcription.html",
    "title": "23  Transcribing Audio",
    "section": "",
    "text": "This section has not yet been written. Check again in a few months!",
    "crumbs": [
      "Audio, Video, and Image Data",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Transcribing Audio</span>"
    ]
  }
]