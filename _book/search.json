[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Psychology: Natural Language",
    "section": "",
    "text": "Welcome\nThis is the website for Data Science for Psychology: Natural Language.\nThis book will teach you state-of-the-art methods for analyzing psychological properties of text, and will cover the fundamentals of data visualization, data collection, and scientific methodology necessary to produce meaningful work in the field.\nAt every step of the way, we will give examples in R, using the tools and rules of the tidyverse. This book will also teach you the basics of the quanteda and text packages for natural language processing (NLP), and the vosonSML package for collecting data from popular social media sites.\nUnless otherwise noted, all figures in this book were generated by us using ggplot2. The full, reproducible code for their generation can be viewed by clicking the “View Source” button at the bottom of each page.\n\nOn the Cover: Posts on Reddit’s r/relationship_advice, distributed by their emotional content according to the Pleasure-Arousal-Dominance model. The x axis represents pleasure, the y axis represents dominance, and the color scale represents arousal. The size of the points represents the number of comments responding to each post. For more details, see the source code."
  },
  {
    "objectID": "intro.html#the-minds-messenger",
    "href": "intro.html#the-minds-messenger",
    "title": "1  Introduction: Why Does Psychology Need Natural Language?",
    "section": "1.1 “The Mind’s Messenger”",
    "text": "1.1 “The Mind’s Messenger”\nPsychology is often referred to as a “behavioral science”. Humans engage in many behaviors: eating, sleeping, pressing buttons, coming in five minutes early to work… All sorts of behaviors can reveal secrets of the mind, but one type of behavior in particular has always struck people as greater than all others in its ability to express the richness of thought: language. The 11th century philosopher Bahaye ibn Paquda wrote:\n\nSpeech and the orderly arrangement of words [are the ways in which a human] gives expression to what is in his mind and soul and understands the conditions of others. The tongue is the heart’s pen and the mind’s messenger. Without speech, there would be no social relations between one person and another.1\n\nSomewhat more recently, Tausczik & Pennebaker (2010) expressed the same sentiment:\n\nLanguage is the most common and reliable way for people to translate their internal thoughts and emotions into a form that others can understand. Words and language, then, are the very stuff of psychology and communication.\n\nWhat exactly is the relationship between psychology and language? The 14th century philosopher William of Ockham proposed that thought itself has essentially linguistic structure, with subjects, objects, and verbs. According to Ockham then, spoken or written language is a sort of rough reflection of inner language. Theories like Ockham’s, in which language is a straightforward representation of inner psychological life, have been common over the history of philosophy. Nowadays, studies of neurological patients have made it clear that linguistic abilities are not necessary for complex thought (Fedorenko & Varley, 2016). Likewise, now-classic research has made it clear that people cannot be trusted to accurately report their own thought processes (Nisbett & Wilson, 1977). Nevertheless, language is without doubt a centrally important mode of human behavior. Humans are constantly talking to each other or listening to each other talk. Even if this talk cannot be construed as reliable reporting about internal states, it must reflect those states in some way, in the same way that any other behavior must do so."
  },
  {
    "objectID": "intro.html#problems-with-language-data-and-why-data-science-solves-them",
    "href": "intro.html#problems-with-language-data-and-why-data-science-solves-them",
    "title": "1  Introduction: Why Does Psychology Need Natural Language?",
    "section": "1.2 Problems With Language Data, and Why Data Science Solves Them",
    "text": "1.2 Problems With Language Data, and Why Data Science Solves Them\nPsychologists have long insisted that talk therapy can heal, and that questionnaires can accurately measure psychological phenomena. These are language-based techniques, which rely on the assumption that language processing is linked to more fundamental internal states. Even so, psychological research has generally been unable to study naturalistic language, i.e. the sort of language that people produce in their day-to-day lives. There are three good reasons for this. Let’s go through each one and explain how data science solves the problem.\n\n1.2.1 Language is Hard to Record\nBefore the invention of audio recording technology, using language in scientific research was nearly impossible. Early efforts by linguists to record language samples from representative populations were heroic; starting in 1896, Edmond Edmont spent four years traveling around France on a bicycle conducting specially designed interviews to collect data for the Atlas linguistique de la France. He collected data from 700 participants in total (Crystal, 1997). Since then, microphones have made it easier to record speech, but even simple quantitative measurements like word counts have still required painstaking hours of listening to recordings and marking each occurrence of the word.\nThe advent of transformer neural networks has made working with audio data much easier than it once was, but the largest boon to our language recording abilities has come through a different medium: text.\nOnly a few decades ago, public access to text was limited to highly edited long-form productions like books, magazines, and newspapers. Psychologists tend to be more interested in accessing people’s thoughts and feelings as they happen, so these texts held little interest for them. Some psychologists studied diaries or personal letters (e.g. Allport, 1942; Creegan, 1944), but personal documents like these are hard to collect at scale. This all changed with the advent of the Internet. Now more than ever before, people communicate through text—not just in long-distance correspondence, but for day-to-day socializing with friends and family. Moreover, much of this textual communication is synchronous and shares many of the same features as face-to-face spoken conversation (Placiński & Żywiczyński, 2023). Most importantly, much of this textual communication is freely available to researchers, through social media platforms like Twitter, Reddit, and YouTube. Data science techniques allow researchers to access these texts and transform them into manageable datasets with APIs (Chapter 9) and web scraping (Chapter 10).\n\n\n1.2.2 Language is Hard to Quantify\nEven when interesting texts were available to psychologists of the past, they were rarely able to make use of them in quantitative analysis. Language is complex, with near-infinite ways to describe the same thing. There are no clear rules for measuring the extent to which a text reflects depression, anxiety, mania, introspection, or any other psychological construct. The few researchers who tried to extract quantitative psychological dimensions of text were nearly as heroic as Edmond Edmont on his four year journey around France. For example, Peterson & Seligman (1984) administered a questionnaire that prompted participants to write short explanations of various hypothetical scenarios. They then carefully read each response, noted each time a phrase like “because of” or “as a result of” was mentioned, and marked the accompanying explanation. These explanations were then typed by hand and shown one at a time to four trained judges who rated them on various 7-point scales. Finally, the agreement between the judges was assessed and their ratings were aggregated into the final variable used in their analysis of risk factors for depression. Today, this sort of analysis could be performed in a matter of seconds using dictionary-based word counts (Chapter 14), neural embeddings (Chapter 19), or other methods covered in this book.\n\n\n1.2.3 Language is Hard to Control\nLanguage is a social phenomenon. People do not write or speak in a vacuum, they participate in conversations or group discussions, considering their audiences as they form their words. For the researcher, this means that language is full of uncontrolled, confounding variables: Is the speaker responding to another speaker? Who is the other speaker? How many participants are there in the conversation? Researchers in the field of psycholinguistics have tried to solve these problems by isolating speakers in a laboratory setting, contriving situations in which participants process and produce speech without the uncontrolled variability of conversational partners (O’Connell & Kowal, 2003). Nevertheless, the inherently social nature of language has made it difficult to analyze language behavior in even remotely naturalistic settings.\nToday, the highly structured nature of interaction on social media has made the social context of utterances easily measurable. For example, comments on Reddit are always associated with a well-defined community, are responding to a known original post, and are directly responding to previous comments in a tree-like structure (Section 9.2.3). Researchers can leverage this structure to provide robust statistical control by using it in tandem with new methods for quantifying the relationships between utterances. A few decades ago the question “How similar are Daniel’s utterances to Amos’s utterances?” would have seemed hopelessly ill-defined. Similar in what way? Today, answering this question is simple with vector-based semantic embeddings (Chapters 17—19). Methods like these can now make sense of nuanced features of language use in dialogue (see Duran et al., 2019 for an example in psycholinguistics).\nThis book focuses primarily on how to extract psychological dimensions from text. The statistical analysis of these dimensions will not be covered in depth here. Even so, the methods presented in the book can be used in tandem with modern methods of statistical analysis (e.g. Kenny et al., 2006) to draw inferences from complex social interactions between pairs or even larger groups of people."
  },
  {
    "objectID": "intro.html#what-this-textbook-is-useful-for",
    "href": "intro.html#what-this-textbook-is-useful-for",
    "title": "1  Introduction: Why Does Psychology Need Natural Language?",
    "section": "1.3 What This Textbook is Useful For",
    "text": "1.3 What This Textbook is Useful For\nThe tools presented in this book are useful for many fields of psychology, cognitive science, and neuroscience. Most of these fields are related to language, but not all. The following are a few examples of ways to apply these tools in practice:\n\nEnhance experimental control by matching word stimuli according to semantic similarity (e.g. Gagné et al., 2005). See Chapter 18.\nFind similarities between large language models and neural processing in the brain (e.g. Millet et al., 2022). See Chapter 19.\nMeasure the degree to which a therapist and patient build mutual understanding over the course of a session (cf. Duran et al., 2019). See Chapter 17.\nAnalyze emotional responses to current events on social media (e.g. Simchon et al., 2020). See Chapter 9 and Section 20.4.\nFind links between group members’ language and their probability of leaving the group (Ashokkumar & Pennebaker, 2022). See Chapter 8 and Chapter 14.\nValidate personality assessments with individuals’ behavior on social media (Schwartz et al., 2013). See Chapter 16.\n\n\n\n\n\n\nAllport, G. W. (1942). The use of personal documents in psychological science. Social Science Research Council Bulletin, 49, xix + 210–xix + 210.\n\n\nAshokkumar, A., & Pennebaker, J. W. (2022). Tracking group identity through natural language within groups. PNAS Nexus, 1(2), pgac022. https://doi.org/10.1093/pnasnexus/pgac022\n\n\nCreegan, R. F. (1944). The phenomenological analysis of personal documents. The Journal of Abnormal and Social Psychology, 39(2), 244–266. https://doi.org/10.1037/h0062816\n\n\nCrystal, D. (1997). The cambridge encyclopedia of language (Second edition.). Cambridge University Press.\n\n\nDuran, N. D., Paxton, A., & Fusaroli, R. (2019). ALIGN: Analyzing linguistic interactions with generalizable techNiques-a python library. Psychological Methods, 24(4), 419–438.\n\n\nFedorenko, E., & Varley, R. (2016). Language and thought are not the same thing: Evidence from neuroimaging and neurological patients. Annals of the New York Academy of Sciences, 1369. https://doi.org/10.1111/nyas.13046\n\n\nGagné, C., Spalding, T., & Ji, H. (2005). Re-examining evidence for the use of independent relational representations during conceptual combination. Journal of Memory and Language, 53, 445–455. https://doi.org/10.1016/j.jml.2005.03.006\n\n\nKenny, D. A., Kashy, D. A., & Cook, W. L. (2006). Dyadic data analysis. (pp. xix, 458–xix, 458). Guilford Press.\n\n\nMillet, J., Caucheteux, C., Orhan, P., Boubenec, Y., Gramfort, A., Dunbar, E., Pallier, C., & King, J.-R. (2022). Toward a realistic model of speech processing in the brain with self-supervised learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, & A. Oh (Eds.), Advances in neural information processing systems (Vol. 35, pp. 33428–33443). Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2022/file/d81ecfc8fb18e833a3fa0a35d92532b8-Paper-Conference.pdf\n\n\nNisbett, R., & Wilson, T. (1977). Telling more than we can know: Verbal reports on mental processes. Psychological Review, 84, 231–259. https://doi.org/10.1037/0033-295X.84.3.231\n\n\nO’Connell, D., & Kowal, S. (2003). Psycholinguistics: A half century of monologism. The American Journal of Psychology, 116, 191–212. https://doi.org/10.2307/1423577\n\n\nPeterson, C., & Seligman, M. E. (1984). Causal explanations as a risk factor for depression: Theory and evidence. Psychological Review, 91(3), 347–374.\n\n\nPlaciński, M., & Żywiczyński, P. (2023). Modality effect in interactive alignment: Differences between spoken and text-based conversation. Lingua, 293, 103592. https://doi.org/https://doi.org/10.1016/j.lingua.2023.103592\n\n\nSchwartz, H. A., Eichstaedt, J. C., Kern, M. L., Dziurzynski, L., Ramones, S. M., Agrawal, M., Shah, A., Kosinski, M., Stillwell, D., Seligman, M. E. P., & Ungar, L. H. (2013). Personality, gender, and age in the language of social media: The open-vocabulary approach. PLOS ONE, 8(9), 1–16. https://doi.org/10.1371/journal.pone.0073791\n\n\nSimchon, A., Guntuku, S. C., Simhon, R., Ungar, L. H., Hassin, R. R., & Gilead, M. (2020). Political depression? A big-data, multimethod investigation of americans’ emotional response to the trump presidency. Journal of Experimental Psychology. General. https://doi.org/10.1037/xge0000767\n\n\nTausczik, Y. R., & Pennebaker, J. W. (2010). The psychological meaning of words: LIWC and computerized text analysis methods. Journal of Language and Social Psychology, 29(1), 24–54. https://doi.org/10.1177/0261927X09351676"
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "1  Introduction: Why Does Psychology Need Natural Language?",
    "section": "",
    "text": "Duties of the Heart, Second Treatise on Examination (Chapter 5), trans. Rabbi Moses Hyamson, New York, 1925↩︎"
  },
  {
    "objectID": "ethics.html#anonymization-is-hard",
    "href": "ethics.html#anonymization-is-hard",
    "title": "2  The Ethics of Data Science in Psychology",
    "section": "\n2.1 Anonymization is Hard",
    "text": "2.1 Anonymization is Hard\nSharing data is an important way for researchers to stay accountable to their colleagues and to promote further research. Nevertheless, data sharing can become problematic when individual subjects can be identified. This is especially true in psychology, which often deals with sensitive personal information. As such, it is important to anonymize data before sharing it. You might think that removing personal names would be enough to accomplish this. It is not.\nIn August 2006, the online service provider AOL released the search queries of 657,000 users over a 3-month period. The dataset was anonymized by replacing personal names with a numeric user ID. Within days, New York Times reporters were able to identify user No. 4417749 as a 62-year-old widow from Lilburn, Georgia by putting together searches involving place names, family names, and ages. AOL quickly took the dataset down, but it was too late. The data are still widely available on the internet, and many more users have been identified based on their search histories.\nAs technology improves, data that previously seemed innocuous can be leveraged to reveal personal information. For example, Facebook users’ “likes” were once public information. Kosinski et al. (2013) then showed that likes alone could be used to predict a user’s age, gender, sexual orientation, ethnicity, religion, intelligence, drug use, and more. Facebook now makes page likes accessible only to friends by default.\nKosinski et al. (2013) did their work without the aid of deep neural networks. With more advanced language processing algorithms emerging every day, text data in particular are becoming increasingly difficult to anonymize. The text that people write (and read) is a window into their soul. This is why NLP is so useful for psychology, but it is also a reason to be vigilant."
  },
  {
    "objectID": "ethics.html#text-based-psychology-is-powerful",
    "href": "ethics.html#text-based-psychology-is-powerful",
    "title": "2  The Ethics of Data Science in Psychology",
    "section": "\n2.2 Text-Based Psychology is Powerful",
    "text": "2.2 Text-Based Psychology is Powerful\nCambridge Analytica is the prime example of the power of data science in psychology. In the 2010s, Cambridge Analytica used an app to collect demographic and psychological data from tens of millions of Facebook users, and paired this with users’ behavior on Facebook. They then used the resulting psychological measures (based on the well-known Big Five personality traits) to create tailored advertisements for political campaigns. The revelation of this privacy breach created an international scandal for both Facebook and Cambridge Analytica.\nCambridge Analytica used methods not unlike many of those described in this book—methods for extracting psychological characteristics from naturalistic online behavior. In fact, due to developments in the field over the last decade, many of the methods described in this book can be quite a bit more powerful than those employed by Cambridge Analytica. Be careful—the research you conduct can be used for the kind of things that create international scandals."
  },
  {
    "objectID": "ethics.html#what-to-do-about-it",
    "href": "ethics.html#what-to-do-about-it",
    "title": "2  The Ethics of Data Science in Psychology",
    "section": "\n2.3 What to do About it",
    "text": "2.3 What to do About it\nThere are no universally accepted rules for ethical text data usage. Many countries have developed data protection laws, for example those of the European Data Protection Supervisor (EDPS) or Israel’s Privacy Protection Authority. Nevertheless, as with any ethical problem, the best policy is to think for yourself, weighing risks against benefits.\nIf you want to share your data widely, but are worried about sensitive private information contained in it, consider using one of many advanced anonymization techniques, such as those that leverage generative AI models to create synthetic data while maintaining statistical properties of the original. These techniques are sometimes costly or labor-intensive, but can be worthwhile for high-impact studies.\nThis chapter is far from a thorough treatment of ethical problems and possible solutions for data collection on the internet. For further reading, we suggest the Association of Internet Researchers Ethical Guidelines.\n\n\n\n\n\nKosinski, M., Stillwell, D., & Graepel, T. (2013). Private traits and attributes are predictable from digital records of human behavior. Proceedings of the National Academy of Sciences, 110(15), 5802–5805. https://doi.org/10.1073/pnas.1218772110"
  },
  {
    "objectID": "data-viz.html",
    "href": "data-viz.html",
    "title": "Data Visualization",
    "section": "",
    "text": "There are no objective rules for how to make a good data visualization. This is because not all data visualizations have the same purpose.\nHere are three common purposes for data visualization:\n\nGetting a quick, intuitive understanding of the data you are working with\nLooking for aspects of your data that common statistics might miss (like outliers or nonlinear relationships)\nCommunicating something to people who don’t have a deep understanding of your data\n\nThe first two purposes are an important aspects of exploratory data analysis (EDA). They will be discussed in Chapter 16. The third one - communication - is the topic of this unit.\nCommunication is hard. It is especially hard for scientists, who often need to balance the needs of multiple target audiences. While insiders in a particular field may want detailed, objective analysis of results and the uncertainty surrounding them, everyone else wants a story.\nA story is clear, oversimplified and sensational. Stories are what grab people’s attention and hold it. Even for experts, stories are what make one study (or news article, or social media post) stand out among many. If you tell a good story, people will want to learn more about the details. If you don’t tell a good story, nobody will want to read your supplementary materials. In this way, all scientists are journalists."
  },
  {
    "objectID": "aesthetics.html",
    "href": "aesthetics.html",
    "title": "3  Why Aesthetic Choices are Important",
    "section": "",
    "text": "Take a moment to appreciate this comic from xkcd.com:\n\n\n\n\nFor most people, aesthetics is the art of making things pleasant to look at. To the data visualizer though, “aesthetics” means something much more precise: Aesthetics are the visual representation of variables.\nJust as journalists need to decide which words to use to express their ideas, data visualizers need to decide which aesthetics to use to express their variables.\nThere are many options. Below are six different ways to represent the numbers “1”, “2”, and “3”, each mapping them to a different aesthetic.\n\n\n\n\n\nLet’s take a concrete example. Eichstaedt et al. (2015) collected Twitters posts from 935 U.S. counties, and counted the number of words related to positive emotions. This emotional measure could then be connected with known demographic measures of each county, such as race and average income.\n\nhead(twitter_counties)\n\n#&gt; # A tibble: 6 × 5\n#&gt;   county   state income posEmotions maj                        \n#&gt;   &lt;chr&gt;    &lt;chr&gt;  &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;                      \n#&gt; 1 Autauga  AL     55165     0.00549 Majority Non-Black/Hispanic\n#&gt; 2 Baldwin  AL     50006     0.00670 Majority Non-Black/Hispanic\n#&gt; 3 Blount   AL     43450     0.00336 Majority Non-Black/Hispanic\n#&gt; 4 Butler   AL     29769     0.00407 Majority Non-Black/Hispanic\n#&gt; 5 Calhoun  AL     38473     0.00448 Majority Non-Black/Hispanic\n#&gt; 6 Chambers AL     30546     0.00523 Majority Non-Black/Hispanic\n\n\nThere are many ways to present this information graphically. Each choice emphasizes a different aspect of the data, and tells a different story.\nIn the following visualization,\n\n\nincome is mapped to the “x” position aesthetic\n\nposEmotions (positive emotions) is mapped to the “y” position aesthetic\n\nmaj (racial majority) is mapped to the color aesthetic\n\n\nlibrary(ggborderline) # for making the lines pop\n\ntwitter_counties |&gt; \n  ggplot(\n    aes(income, posEmotions, \n        # rearrange the categorical variable so that the order \n        # in the legend matches the order in the plot\n        color = factor(\n          maj, \n          levels = c(\"Majority Non-Black/Hispanic\", \n                     \"Majority Hispanic\", \n                     \"Majority Black\")\n          )\n        )\n    ) +\n    # scatterplot\n    geom_point(alpha = .5, \n               # draw sample such that \"Majority Non-Black/Hispanic\" \n               # points don't overpower the others\n               data = twitter_counties |&gt; \n                 group_by(maj) |&gt; \n                 slice_sample(n = 100)) +\n    # loess regression\n    stat_smooth(\n      # borders that match the lines, but are slightly darker\n      aes(bordercolor = after_scale(colorspace::darken(color))), \n      se = FALSE, geom = \"borderline\", \n      linewidth = 1, lineend = \"square\"\n      ) +\n    theme_bw() +\n    # nicer color palette\n    scale_color_brewer(\n      palette = \"Paired\", \n      direction = -1\n      ) +\n    # proper formatting for income\n    scale_x_continuous(labels=scales::dollar_format()) +\n    labs(\n      x = \"County Average Income\",\n      y = \"Positive Emotional Words in Twitter Posts\\n(proportion of total words)\",\n      color = \"\"\n      )\n\n\n\n\nThis is the most intuitive way to organize the three variables. By mapping income to the x axis, we lightly suggest that it is the cause of whatever is happening on the y axis—in this case positive emotion. The idea that higher income causes positive emotion is intuitive—any people believe they would be happier with a higher income. People accustomed to languages that are written left-to-right, like English, will tend to think about what happens as they move left to right on the graph. Three LOESS regression lines encourage the viewer to compare the slopes of the three color groups, which they will go through from top to bottom:\n\nIn counties without a Black or Hispanic majority, greater income means more positive emotion, up to about $60,000 a year, when the line starts flattening out.\nIn counties with a majority Hispanic population, greater income means dramatically more positive emotion, on average.\nIn counties with a majority Black population, greater income doesn’t seem to make much of a difference.\n\nBut just because this scheme is the most intuitive does not mean it is the best one.\nThe next visualization shows the same data but tells a different story. This one also has x, y, and color, but they are mapped to the variables differently:\n\n\nincome is binned and mapped to the “x” position aesthetic\n\nmaj (racial majority) is mapped to the “y” position aesthetic\n\nposEmotions (positive emotions) is mapped to the “fill” color aesthetic\n\n\ntwitter_counties |&gt; \n  # hand-made bins (note the inconsistent bin width to \n  # give more space to the center of the distribution) \n  mutate(\n    income = factor( \n      case_when(\n        income &gt; 100000 ~ \"$100,000+\",\n        income &gt; 80000 ~ \"$80,000-$100,000\",\n        income &gt; 60000 ~ \"$60,000-$80,000\",\n        income &gt; 50000 ~ \"$50,000-$60,00\",\n        income &gt; 40000 ~ \"$40,000-$50,00\",\n        .default = \"$20,000-$40,00\"),\n      levels = c(\"$20,000-$40,00\", \n                 \"$40,000-$50,00\",\n                 \"$50,000-$60,00\", \n                 \"$60,000-$80,000\",\n                 \"$80,000-$100,000\",\n                 \"$100,000+\")\n      )\n    ) |&gt; \n  # aggregate by the new bins\n  group_by(income, maj) |&gt; \n  summarise(\n    posEmotions = mean(posEmotions, \n                       na.rm = TRUE)\n    ) |&gt; \n  # plot\n  ggplot(aes(income, maj, fill = posEmotions)) +\n    # tiles with a little bit of space in between\n    geom_tile(width = .95, height = .95) +\n    # minimal theme\n    theme_minimal() +\n    # color scale to emphasize differences between extremes\n    scale_fill_gradient2(low = \"blue\", \n                         mid = \"green\", \n                         high = \"yellow\", \n                         midpoint = .0055) +\n    labs(\n      x = \"County Average Income\",\n      y = \"\",\n      fill = \"Positive Emotional Words\\nin Twitter Posts\\n(proportion of total words)\"\n      ) +\n    theme(\n      # angles x axis text to fit it all in\n      axis.text.x = element_text(angle = 30, hjust = 1), \n      axis.title = element_text(size = 8),\n      legend.title = element_text(size = 8)\n      ) +\n    # constrain the tiles to be perfectly square\n    coord_equal()\n\n\n\n\nUsing fill makes it harder to see the slope within each racial group, and easier to see the differences between them. The vertical ordering from most positive emotion (in majority non-Black/Hispanic counties) to least positive emotion (majority Black counties) emphasizes this even more. The blank squares on the grid also make the point that there are no majority Black or Hispanic counties with average incomes above $80,000.\nLet’s try one more way to present these data. In this visualization,\n\n\nincome is mapped to the “y” position aesthetic\n\nposEmotions (positive emotions) is mapped to the “color” aesthetic\n\nmaj (racial majority) is mapped to the “x” position aesthetic\n\n\nset.seed(2023)\ntwitter_counties |&gt; \n  ggplot(aes(maj, income, fill = posEmotions)) +\n    # sina plot\n    ggbeeswarm::geom_quasirandom(\n      aes(color = after_scale(colorspace::darken(fill, .3))), \n      alpha = .5, method = \"pseudorandom\", \n      shape = 21, varwidth = TRUE\n      ) +\n    # color scheme which maximizes the visibility of different \n    # values among the crowd (this is a losing battle)\n    scale_fill_gradient2(low = \"red\", \n                         mid = \"white\", \n                         high = \"blue\", \n                         midpoint = .005) +\n    # unintrusive theme\n    theme_bw() +\n    # log scale, and proper formatting for income\n    scale_y_continuous(\n      labels=scales::dollar_format(), \n      trans = \"log10\"\n      ) +\n    labs(\n      x = \" \",\n      y = \"County Average Income\",\n      fill = \"Positive Emotional Words\\nin Twitter Posts\\n(proportion of total words)\"\n      ) +\n    theme(axis.text.x = element_text(angle = 30, hjust = 1))\n\n\n\n\nThis is a sina plot (also known as a beeswarm plot), in which point clouds are arranged by a continuous variable on one axis, a categorical variable on the other axis, and spread out in proportion to their density along the spaces in between the categories.\nSina plots are a good way to compare distributions of different groups (they are almost always more informative than box plots or violin plots); this plot emphasizes that counties with a majority Black population tend to have relatively low average incomes. The other story that this plot tells is the uneven sizes of the three groups—by separating out the points in each group, this visualization emphasizes the fact that there are very few US counties with majority Black or Hispanic population.\nThis plot makes it very difficult to learn anything about positive emotion in Twitter posts. The colors themselves give some guidance: the white in the middle of the scale suggests that it represents some sort of zero point—a “normal” amount of positive emotion. Nevertheless, the viewer will have to squint in order to notice the trend for higher income counties to be happier. The emotional variable is not adding much to this visualization.\nIn this chapter we have seen how choices about which aesthetics to map to variables make a big difference in the way a data visualization is interpreted. Three visualization of the same data can emphasize tell very different stories.\n\nPress the “View Source” button below to see the hidden code blocks in this chapter.\n\n\n\n\n\nEichstaedt, J. C., Schwartz, H. A., Kern, M. L., Park, G., Labarthe, D. R., Merchant, R. M., Jha, S., Agrawal, M., Dziurzynski, L. A., Sap, M., Weeg, C., Larson, E. E., Ungar, L. H., & Seligman, M. E. P. (2015). Psychological language on twitter predicts county-level heart disease mortality. Psychological Science, 26(2), 159–169. https://doi.org/10.1177/0956797614557867"
  },
  {
    "objectID": "telling-a-story.html#sec-simplify-the-story",
    "href": "telling-a-story.html#sec-simplify-the-story",
    "title": "4  Don’t Distract From the Story",
    "section": "\n4.1 A Small, Compelling Story is Better Than a Big, Confusing One",
    "text": "4.1 A Small, Compelling Story is Better Than a Big, Confusing One\nSchwartz et al. (2013) collected 15.4 million Facebook status updates from participants who had filled out a variety of questionnaires on the My Personality application (discussed in Chapter 2). They analyzed the frequencies of millions of words, phrases, and topics as they correlate with gender, age, and personality traits of the author. The resulting paper focused on methodology, but Schwartz et al. (2013) nevertheless understood the importance of telling a good story. Here is their figure 5B:\n\n\nFigure from Schwartz et al. (2013)\n\nThis is a beautiful data visualization. The story it tells is so clear and simple that it doesn’t need a caption: Older people use “I” less and “we” more. The unstated implication is either that people get less individualistic with age, or that the young people of today are self-centered. Both are excellent stories.\nHow did Schwartz et al. achieve such a clear story? Let’s take a closer look at some of their choices:\nFirst, out of millions of words, phrases, and topics in their analysis, they chose to focus this visualization on only two. This is the first step of story-telling with data: remove distractions. A small, compelling story is better than a big, confusing one.\nSecond, they chose not to show the data points themselves, but to represent the overall trends with regression lines. This is a major sacrifice, since it makes the graph much less informative—any good scientist will wonder about the distributions surrounding these lines: How rare are community-oriented 20-year-olds? What about self-centered 60-year-olds? Nevertheless, Schwartz et al. decided that including a scatter plot behind the lines would make the graph too confusing to look at, and distract from the main story.\nThird, they chose to use bendy LOESS regression lines, even though the main analysis of the paper was conducted with linear regression. This was a great choice because it makes the story more convincing. The fact that even LOESS lines show near-linear trends is impressive. Even though there are no data points to be seen, those steady lines give the impression that the underlying data are reliable. Also, the the LOESS lines give the viewer the opportunity to notice nuances in the story without distracting from the big picture (it is fascinating that “we” reaches it’s all-time low around the time most people move out of their parents’ house, and not before).\nLastly, let’s take a look at the y axis: What is “Standardized Frequency”? We have an intuitive idea that higher means using the word more and lower means using it less. But this intuitive simplicity did not come easily—it had to be carefully constructed by the authors of the paper. Actually, “Standardized Frequency” is calculated using this formula:\n\n\nFigure from Schwartz et al. (2013)\n\nDon’t understand any of this? That’s OK. We’ll cover methods of standardizing word frequencies in Chapter 15. For now, the point is this: Sometimes you have to do something complicated to make something simple. If Schwartz et al. had not performed them, “I” would likely be much higher frequency than “we” at all ages, and the story, which requires the viewer to focus on the slopes of the lines, would be much harder to appreciate."
  },
  {
    "objectID": "telling-a-story.html#engineer-your-aesthetics",
    "href": "telling-a-story.html#engineer-your-aesthetics",
    "title": "4  Don’t Distract From the Story",
    "section": "\n4.2 Engineer Your Aesthetics",
    "text": "4.2 Engineer Your Aesthetics\nWe have just seen in Schwartz et al.’s beautiful data visualization (Section 4.1) that choosing to map the frequency variable to the y position aesthetic was not enough. In order to make the story clear, they carefully engineered the scale on which they measured frequency. In their case, this required some complicated standardization tailored to the particular statistics underlying their data. Often though, the solution is much more straightforward.\nThe remainder of this chapter outlines some common ways to engineer aesthetics that can help make a story clear and intuitive.\n\n4.2.1 Nonlinear Axes\nOften a simple log scale is enough to reveal a much clearer presentation of data. The following graph uses data from Buechel et al. (2018), in which participants read news stories, rated their own empathy and distress after reading them, and then described their thoughts in their own words.\nThis visualization tells a story about the most and least common words in participant’s responses.\n\nlibrary(ggrepel)\nlibrary(patchwork)\n\ndistressed_texts_binary_ordered &lt;- distressed_texts_binary |&gt; \n  # ratio of distressed frequency to non-distressed frequency\n  mutate(distressed_freq_ratio = distressed_count/nondistressed_count) |&gt; \n  # refactor in descending order\n  arrange(distressed_freq_ratio) |&gt; \n  mutate(word = factor(word, levels = word))\n\nset.seed(2023)\nbadplot1 &lt;- distressed_texts_binary_ordered |&gt; \n  ggplot(aes(word, distressed_freq_ratio, label = word)) +\n    geom_point(color = \"blue3\", \n               size = 1, \n               alpha = .2) +\n    geom_text_repel(\n      size = 3, \n      data = distressed_texts_binary_ordered |&gt; \n        filter(distressed_freq_ratio &gt; 1.75)) +\n    labs(title = \"Linear Scale\",\n         x = \"Words, from least to most distressed\",\n         y = \"Distressed frequency / non-distressed frequency\") +\n    scale_y_continuous(breaks = seq(-.5, 2, .5)) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(color = \"red3\", hjust = 1, size = 20),\n      panel.grid.major.x = element_blank(),\n      axis.text.x = element_blank(),\n      axis.ticks.x = element_blank(),\n      axis.title.x = element_text(size = 15)\n      )\n\nset.seed(2023)\ngoodplot1 &lt;- distressed_texts_binary_ordered |&gt; \n  ggplot(aes(word, distressed_freq_ratio, \n             label = word, \n             color = distressed_freq_ratio &lt; 1)\n         ) +\n    geom_point(size = 1, alpha = .2) +\n    geom_text_repel(\n      size = 3, \n      data = distressed_texts_binary_ordered |&gt; \n        filter(distressed_freq_ratio &gt; 1.75 | distressed_freq_ratio &lt; 1/2),\n      max.overlaps = 20) +\n    geom_hline(linetype = 2, yintercept = 1) +\n    labs(title = \"Log Scale\",\n         x = \"Words, from least to most distressed\",\n         y = \"Distressed frequency / non-distressed frequency\") +\n    scale_y_continuous(\n      breaks = c(2^(-6:6)), \n      trans = \"log2\", \n      labels = ~MASS::fractions(.x)\n      ) +\n    guides(color = \"none\") +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(color = \"green3\", hjust = 1, size = 20),\n      panel.grid.major.x = element_blank(),\n      axis.text.x = element_blank(),\n      axis.ticks.x = element_blank(),\n      axis.title.x = element_text(size = 15)\n      )\n\ngoodplot1 + badplot1\n\n\n\n\nWhen plotting ratios, it is almost always a good idea to use a log scale (left). This way, the viewer can compare the largest and the smallest relative values. Without the log scale (right), the smallest values are squished into oblivion.\n\n4.2.2 Ordering Categorical Variables\nTake another look at the graph labeled “Log Scale” above, and notice the ordering along the x axis. Words, on their own, are an unordered categorical variable. Nevertheless, in the context of a story, even unordered variables have an order. Ordering the categorical variable along the continuous variable of interest calls attention to the distribution and removes confusion.\n\nlibrary(ggrepel)\nlibrary(patchwork)\n\nset.seed(2023)\nbadplot2 &lt;- distressed_texts_binary |&gt; \n  # ratio of distressed frequency to non-distressed frequency\n  mutate(distressed_freq_ratio = distressed_count/nondistressed_count) |&gt; \n  ggplot(\n    aes(\n      word, distressed_freq_ratio, \n      label = word, \n      color = distressed_freq_ratio &lt; 1\n      )\n    ) +\n    geom_point(size = 1, alpha = .7) +\n    geom_text_repel(\n      size = 3, \n      data = distressed_texts_binary_ordered |&gt; \n        filter(distressed_freq_ratio &gt; 1.75 | distressed_freq_ratio &lt; .5),\n      max.overlaps = 20\n      ) +\n    geom_hline(linetype = 2, yintercept = 1) +\n    labs(\n      title = \"Unordered\",\n      x = \"Words\",\n      y = \"Distressed frequency / non-distressed frequency\"\n      ) +\n    scale_y_continuous(\n      breaks = c(2^(-6:6)), \n      trans = \"log2\", \n      labels = ~MASS::fractions(.x)\n      ) +\n    guides(color = \"none\") +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(color = \"red3\", hjust = 1, size = 20),\n      panel.grid.major.x = element_blank(),\n      axis.text.x = element_blank(),\n      axis.ticks.x = element_blank(),\n      axis.title.x = element_text(size = 15)\n      )\n\ngoodplot2 &lt;- goodplot1 + labs(title = \"Ordered\")\n\ngoodplot2 + badplot2\n\n\n\n\n\n4.2.3 Color Scales\nWe have already seen how a data visualizer can clarify a story by spatially ordering a categorical variable. A carefully tailored color scale can be an even more powerful communicator than an x or y axis. This is because color, even though it is usually treated as a single aesthetic, actually has many dimensions: luminosity, saturation, redness, blueness, etc.\nThe first step in choosing a color scale for any variable is to consider whether the thing being measured is diverging, sequential, or qualitative.\nDiverging scales measure something with a neutral center. This center is often represented by zero, but beware! Sometimes a neutral center is 4 on a seven point Likert scale (see “Agreement” in the figure below). When dealing with fractions, the neutral center is 1 (see “Frequency Ratio” in the figure below).\nWhen applying diverging scales, keep in mind any associations people might have with the colors involved. For example, red should always be bad and blue/green good (see the plot in Section 4.2.2, in which red = distress = bad).\n\n\n\n\n\nSequential scales measure something that has an order, but no neutral center. Often, one side of the scale is at zero, so that the scale goes from nothing to something. In these cases, the appropriate color scale will represent amount with luminosity, where zero is the lightest (see “Frequency” and “Anxiety” in the figure below). This way, the lower amounts have lower contrast against the white background of the plot (if using a non-white plot background, make sure the low end of the scale matches).\nSometimes sequential scales do not measure amount, as in “Weekdays” in the figure below. Weekdays have an order—rom the beginning of the week to the end—ut it would be a mistake to use a scale with one side blending in to the background and the other intensely dark, since that would suggest that Thursday is somehow ‘more’ than Wednesday. Likewise, there is no neutral center (there’s nothing neutral about Wednesday). In such case, the scale should go from one noticeable color to another. The chart below uses a palette reminiscent of sunset to give the impression of time passing.\n\n\n\n\n\nTo emphasize the point about weekdays, consider the following two versions of the same graph (data taken from Reddit users associated with subreddits for 9 US cities):\n\n\n\n\n\nBoth versions are confusing to look at (these data might be better represented as a heat map), but the one with the sequential color scale is much better. Whereas the qualitative scale requires the viewer to look constantly back and forth between the legend and the plot, the sequential scale maps to an intuitive understanding of beginning-of-week vs. end-of-week.\nWe have seen that many seemingly unordered variables should be ordered in the context of a story. Nevertheless, some variables are truly qualitative. In these cases, the color scale should maximize contrast between neighboring values without accidentally suggesting an order. For example, “Parts of Speech” in the figure below are all soft pastel colors. If some were darker or more saturated, it might suggest that there is an important difference between the groups.\nAgain, keep in mind any associations people might have with colors involved. For example, countries should be represented by colors that appear in their flags. This is of course sometimes difficult—all of the countries in the figure below have red in their flag, and all but China have blue and white. Nevertheless, try your best. The Wikipedia page for national colors was helpful in making the chart below.\n\n\n\n\n\n\n4.2.4 Accent colors\nBecause color has many dimensions, it can sometimes be used to represent two scales at the same time. One common tactic is to use luminosity or saturation to emphasize certain values and de-emphasize others. Below, we have redrawn the frequency ratio plot from earlier in this chapter (Section 4.2.1) to tell a story about two words in particular. By using accent colors to emphasize the two words of interest, we remove distractors while maintaining the broader context of the story.1\n\nlibrary(ggnewscale)\n\nset.seed(2023)\ndistressed_texts_binary_ordered |&gt; \n  ggplot(aes(word, distressed_freq_ratio, \n             label = word, \n             color = distressed_freq_ratio &lt; 1)) +\n    geom_point() +\n    geom_hline(linetype = 2, yintercept = 1) +\n    scale_color_discrete(\n      type = colorspace::lighten(c(\"#F8766D\", \"#00BFC4\"), .7)) +\n    geom_text_repel(\n      size = 3, \n      data = distressed_texts_binary_ordered |&gt; \n        filter(distressed_freq_ratio &gt; 3.6 \n               | distressed_freq_ratio &lt; .2),\n      max.overlaps = 20) +\n    guides(color = \"none\") +\n    new_scale_color() +\n    geom_point(\n      aes(color = distressed_freq_ratio &lt; 1), \n      size = 3,\n      data = distressed_texts_binary_ordered |&gt; \n        filter(word %in% c(\"i\", \"we\"))) +\n    geom_text_repel(\n      size = 4, \n      data = distressed_texts_binary_ordered |&gt; \n        filter(word %in% c(\"i\", \"we\")),\n      max.overlaps = 20) +\n    labs(title = '\"We\" is a Sign of Distress',\n         x = \"Words, from least to most distressed\",\n         y = \"Distressed frequency / non-distressed frequency\") +\n    scale_y_continuous(breaks = c(2^(-6:6)), \n                       trans = \"log2\", \n                       labels = ~MASS::fractions(.x)) +\n    guides(color = \"none\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = .5, size = 18),\n          panel.grid.major.x = element_blank(),\n          axis.text.x = element_blank(),\n          axis.ticks.x = element_blank())\n\n\n\n\n\n4.2.5 Aspect Ratios\nSimchon et al. (2021) investigated whether COVID-19 concern among New Yorkers resulted in higher or lower levels of certainty, as expressed in language on Twitter. Their story: Higher concern leads to greater expressions of certainty, since people use certainty as a coping mechanism. Here is their Figure 3, reproduced in three different aspect ratios:\n\nlibrary(patchwork)\n\nplot &lt;- covid_concern |&gt; \n  mutate(z_cert = as.numeric(scale(cert)),\n         z_concern = as.numeric(scale(ny_net_concern))) |&gt; \n  pivot_longer(cols = c(z_cert, z_concern)) |&gt; \n  mutate(name = if_else(name==\"z_cert\", \"Certainty\", \"NY Net Concern\")) |&gt; \n  ggplot() +\n    geom_smooth(aes(date, value, \n                    linetype = name), \n                se = FALSE, method = \"loess\", \n                color = \"black\", span = 1/3, \n                method.args = list(degree=1)) +\n    ylab(\"Z-score\") + \n    xlab(\"Date\") + \n    scale_colour_grey() +\n    cowplot::theme_cowplot() + \n    labs(linetype =c(\"\"))\n\nplot_squished &lt;- plot + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n(plot_squished + plot_squished +  \n    plot_layout(widths = c(1, 2))) / \n  plot + plot_layout(heights = c(2, 1)) +\n  plot_annotation(tag_levels = 'A')\n\n\n\n\nWhich aspect ratio is the right one? A good aspect ratio is one that communicates the meaning of the variables in question. Since months are spread out over time (by definition), it makes sense to make the x-axis longer so that viewers have the feeling of time passing as they scan it. But it shouldn’t be too wide, since the aspect ratio should also emphasize important differences in position (here, the positive slope of both lines). Something in between B and C seems appropriate. Indeed, this is the figure printed in the final paper:\n\n\nAs always, press the “View Source” button below to see the hidden code blocks in this chapter.\n\n\n\n\n\nBuechel, S., Buffone, A., Slaff, B., Ungar, L. H., & Sedoc, J. (2018). Modeling empathy and distress in reaction to news stories. CoRR, abs/1808.10399. http://arxiv.org/abs/1808.10399\n\n\nSchwartz, H. A., Eichstaedt, J. C., Kern, M. L., Dziurzynski, L., Ramones, S. M., Agrawal, M., Shah, A., Kosinski, M., Stillwell, D., Seligman, M. E. P., & Ungar, L. H. (2013). Personality, gender, and age in the language of social media: The open-vocabulary approach. PLOS ONE, 8(9), 1–16. https://doi.org/10.1371/journal.pone.0073791\n\n\nSimchon, A., Turkin, C., Svoray, T., Kloog, I., Dorman, M., & Gilead, M. (2021). Beyond doubt in a dangerous world: The effect of existential threats on the certitude of societal discourse. Journal of Experimental Social Psychology, 97, 104221. https://doi.org/https://doi.org/10.1016/j.jesp.2021.104221"
  },
  {
    "objectID": "telling-a-story.html#footnotes",
    "href": "telling-a-story.html#footnotes",
    "title": "4  Don’t Distract From the Story",
    "section": "",
    "text": "In the example here, we used ggnewscale to control the accented an non-accented color scales separately. If you’d like a simpler method for accenting values without using layered geoms, we recommend the gghighlight package.↩︎"
  },
  {
    "objectID": "word-viz.html#sec-freq-freq",
    "href": "word-viz.html#sec-freq-freq",
    "title": "5  Visualizing Distributions of Words",
    "section": "\n5.1 Frequency/Frequency Plots",
    "text": "5.1 Frequency/Frequency Plots\nA scatterplot is the most obvious choice for visualizing the relationship between two variables. For text data, this approach is commonly associated with the scattertext Python library (Kessler, 2017), but the same effect is easily accomplished in ggplot2.\nSince we are comparing frequency in one group to frequency in another, we can put each frequency variable on an axis. We will call this a frequency/frequency plot, or F/F plot. To emphasize words that are more frequent in one group than in the other, we represent the ratio between the two frequencies with a diverging color scale.\n\nlibrary(ggrepel)\n\nset.seed(2023)\ndistressed_texts_binary |&gt; \n  ggplot(aes(nondistressed_count, distressed_count, \n             label = word,\n             color = distressed_freq_ratio)) +\n    geom_point() +\n    geom_text_repel(max.overlaps = 20) +\n    scale_x_continuous(trans = \"log10\", n.breaks = 5) +\n    scale_y_continuous(trans = \"log10\", n.breaks = 6) +\n    scale_color_gradient2(\n      low = \"blue4\", \n      mid = \"#E2E2E2\", \n      high = \"red4\", \n      trans = \"log2\", # log scale for ratios\n      limits = c(.25, 4), \n      breaks = c(.25, 1, 4),\n      labels = c(\"Characteristically\\nNon-Distressed\",\n                 \"Equal Proportion\",\n                 \"Characteristically\\nDistressed\")\n      ) +\n    labs(\n      title = \"Stop Words in Distressed and Non-Distressed Texts\",\n      x = \"Occurrences in Non-Distressed Texts\",\n      y = \"Occurrences in Distressed Texts\",\n      color = \"\"\n      ) +\n    coord_fixed() +\n    theme_minimal()\n\n\n\n\nThis plot has the advantage of showing not just which words are characteristic of one group or the other, but also which are more common in both.\nTo allow viewers to explore these patterns in greater detail, we can make the plot interactive using the ggiraph package. Hover over the points to show the words they represent!\n\nlibrary(ggiraph, verbose = FALSE)\nlibrary(ggrepel)\n\nset.seed(2023)\np &lt;- distressed_texts_binary |&gt; \n  ggplot(aes(nondistressed_count, distressed_count, \n             label = word,\n             color = distressed_freq_ratio,\n             tooltip = word, \n             data_id = word # aesthetics for interactivity\n             )) +\n    geom_point_interactive() +\n    geom_text_repel_interactive() +\n    scale_x_continuous(trans = \"log10\", n.breaks = 5) +\n    scale_y_continuous(trans = \"log10\", n.breaks = 6) +\n    scale_color_gradient2(\n      low = \"blue4\", \n      mid = \"#E2E2E2\", \n      high = \"red4\", \n      trans = \"log2\", # log scale for ratios\n      limits = c(.25, 4), \n      breaks = c(.25, 1, 4),\n      labels = c(\"Characteristically\\nNon-Distressed\",\n                 \"Equal Proportion\",\n                 \"Characteristically\\nDistressed\")\n    ) +\n    labs(\n      title = \"Stop Words in Distressed and Non-Distressed Texts\",\n      x = \"Occurrences in Non-Distressed Texts\",\n      y = \"Occurrences in Distressed Texts\",\n      color = \"\"\n    ) +\n    # fixed coordinates since x and y use the same units\n    coord_fixed() + \n    theme_minimal()\n\ngirafe_options(\n  girafe(ggobj = p),\n  opts_tooltip(css = \"font-family:sans-serif;font-size:1em;color:Black;\")\n  ) \n\n\n\n\n\n\n5.1.1 Rotated Frequency/Frequency Plots\nA disadvantage of simple F/F plots: When people see a scatterplot, they think, “Aha! A correlation!” Any two samples of text in the same language will have highly correlated word frequencies. This boring story about the correlation is distracting from the more interesting stories about words that are especially characteristic of one group or another. This distraction can be removed by “rotating” the axes. Mathematically, we achieve this by plotting the average of the two frequencies (nondistressed_count + distressed_count)/2 on the y axis, and the ratio between the two frequencies on the x axis. The result is a much more intuitive plot with a clear binary comparison. Remember, sometimes you have to do something complicated to make something simple (Section 4.1).\n\nlibrary(ggiraph, verbose = FALSE)\nlibrary(ggrepel)\n\nset.seed(2023)\np1 &lt;- distressed_texts_binary |&gt; \n  mutate(common = (nondistressed_count + distressed_count)/2) |&gt; \n  ggplot(aes(distressed_freq_ratio, common, \n             label = word,\n             color = distressed_freq_ratio,\n             tooltip = word, data_id = word # aesthetics for interactivity\n             )) +\n    geom_point_interactive() +\n    geom_text_repel_interactive() +\n    scale_y_continuous(trans = \"log2\", breaks = ~.x,\n                       minor_breaks = ~2^(seq(0,log2(.x[2]))),\n                       labels = c(\"Rare\", \"Common\")) +   \n    scale_x_continuous(trans = \"log2\", limits = c(1/6,6),\n                       breaks = c(.25, 1, 4),\n                       labels = c(\"Characteristically\\nNon-Distressed\",\n                                  \"Equal Proportion\",\n                                  \"Characteristically\\nDistressed\")) +\n    scale_color_gradient2(low = \"blue4\", \n                          mid = \"#E2E2E2\", \n                          high = \"red4\", \n                          trans = \"log2\", # log scale for ratios\n                          guide = \"none\") +\n    labs(title = \"Stop Words in Distressed and Non-Distressed Texts\",\n         x = \"\",\n         y = \"Average Frequency\",\n         color = \"\") +\n    theme_minimal()\n\ngirafe_options(\n  girafe(ggobj = p1),\n  opts_tooltip(css = \"font-family:sans-serif;font-size:1em;color:Black;\")\n  )\n\n\n\n\n\nBecause we love these rotated F/F plots so much, we couldn’t help showing off one more example, this time with data from the Corpus of Contemporary American English (Davies, 2009):\n\n# get frequency data\nhttr::GET(\"https://www.wordfrequency.info/files/genres_sample.xls\",\n          httr::write_disk(tf &lt;- tempfile(fileext = \".xls\")))\nword_freqs &lt;- readxl::read_excel(tf) |&gt; \n  select(lemma, ACADEMIC, SPOKEN)\n\n\np2 &lt;- word_freqs |&gt; \n  filter(ACADEMIC != 0, SPOKEN != 0) |&gt; \n  # generate tooltip text\n  mutate(rep = if_else(ACADEMIC/SPOKEN &gt; 1, \n                       \"more common in academic texts\",\n                       \"more common in spoken texts\"),\n         mult = if_else(ACADEMIC/SPOKEN &gt; 1, \n                        as.character(round(ACADEMIC/SPOKEN, 2)),\n                        as.character(round(SPOKEN/ACADEMIC, 2))),\n         tooltip = paste0(\"&lt;b&gt;\",lemma, \"&lt;/b&gt;\", \"&lt;br/&gt;\", \n                          mult, \"x \", rep)) |&gt; \n  ggplot(aes(ACADEMIC/SPOKEN, (ACADEMIC + SPOKEN)/2, \n             label = lemma,\n             color = ACADEMIC/SPOKEN,\n             tooltip = tooltip, \n             data_id = lemma # aesthetics for interactivity\n             )) +\n    geom_point_interactive() +\n    scale_x_continuous(trans = \"log2\", \n                       breaks = c(1/100, 1, 100),\n                          labels = c(\"Characteristically\\nSpoken\",\n                                     \"Equal Proportion\",\n                                     \"Characteristically\\nAcademic\")) +\n    scale_y_continuous(trans = \"log2\", \n                       breaks = ~.x, \n                       minor_breaks = ~2^(seq(0, log2(.x[2]))),\n                       labels = c(\"Rare\", \"Common\")) +\n    scale_color_gradientn(limits = c(1/740, 740),\n                          colors = c(\"#023903\", \n                                     \"#318232\",\n                                     \"#E2E2E2\", \n                                     \"#9B59A7\",\n                                     \"#492050\"), \n                          trans = \"log2\", # log scale for ratios\n                          guide = \"none\") +\n    labs(title = \"Academic vs. Spoken English\",\n         x = \"\", y = \"\",\n         color = \"\") +\n    theme_minimal()\n\ngirafe_options(\n  girafe(ggobj = p2),\n  opts_tooltip(css = \"font-family:sans-serif;font-size:1em;color:Black;\")\n  )   \n\n\n\n\n\n\nThis is a particularly good visualization, because it tells an interesting story. The story it tells is: “Academic English is very different from spoken English.” The wide aspect ratio and the tooltip text with “#x more common in spoken/academic texts” especially emphasize this point. The F/F plot is an appropriate visualization method because this story pertains to the full distribution of words—look at how many are more than 10 times more common in one or the other!\n\n5.1.2 Advanced Frequency/Frequency Plots\nFor more information about frequency/frequency plots and other related plot types incorporating various statistics, see the scattertext tutorial (Kessler, 2017). While the tutorial is intended for the scattertext library in Python, almost all examples can be produced in R with ggplot2 and ggiraph. Search bars and other types of responsive interactivity can be accomplished with shiny.\n\n\n\n\n\n\n\nAdvantages of Frequency/Frequency Plots\n\n\n\n\n\nSpatial Mapping: F/F plots use axes, which make it easy to compare values of different words.\n\nReadability: The layout of F/F plots is easy to interpret, especially when rotated and properly labeled.\n\nFull Picture: F/F plots convey the full shape of the frequency distribution, rather than singling out words most characteristic of one side or the other. This can be useful when the distribution itself is interesting.\n\n\n\n\n\n\n\n\n\nDisadvantages of Frequency/Frequency Plots\n\n\n\n\n\nInteractivity Required: F/F plots require interactivity to be maximally informative.\n\nMessy: Without interactivity, labels can be messy and confusing.\n\nImplies that Correlation is Interesting: F/F plots may imply that the point of the graph is to show the correlation between frequencies in the two texts. Rotating the axes mostly solves this problem.\n\nVague: By showing many words at the same time, F/F plots make it difficult to focus in on particular stories (unless the story is about the distribution itself)."
  },
  {
    "objectID": "word-viz.html#sec-word-clouds",
    "href": "word-viz.html#sec-word-clouds",
    "title": "5  Visualizing Distributions of Words",
    "section": "\n5.2 Word Clouds",
    "text": "5.2 Word Clouds\nWord clouds are commonly used for purposes like:\n\nSummarizing text using word frequencies\nDecorating placemats at cheap restaurants\nComparing word usage in two groups of texts\nCorrelating word usage with a construct of interest\n\nWord clouds are not a good tool for summarizing text. They are a perfectly fine tool for kitschy placemats, but those are beyond the scope of this textbook. In the world of data science, there are only two legitimate uses for word clouds: comparing words across two groups of texts, and correlating word frequencies with a construct of interest. Even these legitimate uses break a fundamental rule of data visualization, since by showing many words at the same time they are telling many stories at the same time, each distracting from the others. Nevertheless, analyses often include so many words (or other units of text) that producing a visualization for each one is unfeasible, and a summary graphic is necessary.\n\n5.2.1 Word Clouds for Comparing Two Groups\nWord clouds generally have three aesthetics: label, color, and size:\n\n\nlabel will always be the text of the words.\n\ncolor is appropriate for representing relative frequency, since it has a neutral center (where the frequencies in both groups are the same and distressed_freq_ratio = 1. Such a neutral center calls for a diverging color scale (Section 4.2.3). Because we are representing the ratio of two frequencies, it is appropriate to use a log scale (see Section 4.2.1). This will make the scale symmetrical for values above and below the neutral center.\n\nsize is technically unnecessary, since the diverging color scale already represents both the valence and the magnitude of the relative frequency. In practice though, we are generally most interested in the largest differences. Size is therefore used to emphasize words with greater a discrepancy between the groups. This magnitude value is calculated as abs(log2(distressed_freq_ratio)).\n\n\ndistressed_texts_binary &lt;- distressed_texts_binary |&gt; \n  mutate(freq_ratio_log_magnitude = abs(log2(distressed_freq_ratio)))\n\nhead(distressed_texts_binary)\n\n#&gt; # A tibble: 6 × 5\n#&gt;   word  distressed_count nondistressed_count distressed_freq_ratio\n#&gt;   &lt;chr&gt;            &lt;int&gt;               &lt;int&gt;                 &lt;dbl&gt;\n#&gt; 1 the               3297                2985                 1.10 \n#&gt; 2 to                2556                2415                 1.06 \n#&gt; 3 and               2125                1856                 1.14 \n#&gt; 4 of                1592                1416                 1.12 \n#&gt; 5 i                 1587                1874                 0.847\n#&gt; 6 a                 1547                1603                 0.965\n#&gt; # ℹ 1 more variable: freq_ratio_log_magnitude &lt;dbl&gt;\n\n\nFor creating word clouds in R we use the ggwordcloud package, with its geom_text_wordcloud geom:\n\nlibrary(ggwordcloud)\nset.seed(2)\n\ndistressed_texts_binary |&gt; \n  # top 100 highest discrepancy words\n  arrange(desc(freq_ratio_log_magnitude)) |&gt; \n  slice_head(n = 150) |&gt; \n  # plot\n  ggplot(aes(label = word, \n             size = freq_ratio_log_magnitude, \n             color = distressed_freq_ratio)) +\n    geom_text_wordcloud(show.legend = TRUE) + # wordcloud geom\n    scale_radius(range = c(2, 18), guide = \"none\") + # control text size\n    scale_color_gradient2(\n      name = \"Distressed /\\nNon-distressed\\nFrequency\",\n      labels = ~ MASS::fractions(.x), # show legend labels as fractions\n      low = \"blue3\", mid = \"white\", high = \"red3\", # set diverging color scale\n      trans = \"log2\" # log scale\n      ) +\n    theme_void() # blank background\n\n\n\n\nWe can now easily see that the word most representative of non-distressed texts is “interesting”, which is far more representative of one group than any other word in the analysis.\nThe angle_group aesthetic can be used to separate out the words more frequent in distressed texts from those more frequent in non-distressed texts:\n\nset.seed(2)\n\ndistressed_texts_binary |&gt; \n  # top 150 highest discrepancy words\n  arrange(desc(freq_ratio_log_magnitude)) |&gt; \n  slice_head(n = 150) |&gt; \n  # plot\n  ggplot(aes(label = word, \n             size = freq_ratio_log_magnitude, \n             color = distressed_freq_ratio,\n             angle_group = distressed_freq_ratio &gt; 1)) +\n    geom_text_wordcloud(show.legend = TRUE) + # wordcloud geom\n    scale_radius(range = c(2, 18), guide = \"none\") + # control text size\n    scale_color_gradient2(\n      name = \"Distressed /\\nNon-distressed\\nFrequency\",\n      labels = ~ MASS::fractions(.x), # show legend labels as fractions\n      low = \"blue3\", mid = \"grey\", high = \"red3\", # set diverging color scale\n      trans = \"log2\" # log scale\n      ) +\n    theme_void() # blank background\n\n\n\n\nAlternatively, we can specify an original position for each label (as x and y aesthetics) to create multiple clouds:\n\nset.seed(2)\n\ndistressed_texts_binary |&gt; \n  # top 100 highest discrepancy words\n  arrange(desc(freq_ratio_log_magnitude)) |&gt; \n  slice_head(n = 150) |&gt; \n  # plot\n  ggplot(aes(label = word, \n             size = freq_ratio_log_magnitude, \n             color = distressed_freq_ratio,\n             x = distressed_freq_ratio &lt; 1,\n             y = distressed_freq_ratio &gt; 1)) +\n    # wordcloud geom\n    geom_text_wordcloud(show.legend = TRUE) + \n    # control text size\n    scale_radius(range = c(2, 18), guide = \"none\") + \n    scale_color_gradient2(\n      name = \"Distressed /\\nNon-distressed\\nFrequency\",\n      # show legend labels as fractions\n      labels = ~ MASS::fractions(.x), \n      # set diverging color scale\n      low = \"blue3\", mid = \"grey\", high = \"red3\", \n      # log scale\n      trans = \"log2\" \n      ) +\n    theme_void() # blank background\n\n\n\n\n\n5.2.2 Word Clouds for Continuous Variables of Interest\nRecently, some have advocated using correlation coefficients instead of frequency ratios in word clouds. This approach has three advantages:\n\nCorrelation coefficients take variance into account.\nSince correlation coefficients are more commonly used, it is easier to perform significance testing on them. This way we can include only significant results in the visualization.\nUnlike frequency ratios, which always compare two groups, correlation coefficients can be applied to continuous variables of interest.\n\nTo apply this method to the data from Buechel et al. (2018), we can use participants’ continuous distress ratings for each text. We count the occurrences of each word in each text, and measure the correlation between these frequency variables and the corresponding distress ratings. Since the association may be non-linear, we use the Kendall rank correlation. You can see the full calculations by pressing the “View Source” button at the bottom of this page.\nWe can now map the strength of the correlation (i.e. abs(cor)) to size, and use color to show the direction of the correlation.\n\ndistress_cor &lt;- read_csv(\"data/distress_cor.csv\", show_col_types = FALSE)\nhead(distress_cor)\n\n#&gt; # A tibble: 6 × 2\n#&gt;   word      cor\n#&gt;   &lt;chr&gt;   &lt;dbl&gt;\n#&gt; 1 and    0.0983\n#&gt; 2 from   0.0680\n#&gt; 3 how    0.0448\n#&gt; 4 is     0.0422\n#&gt; 5 it    -0.0354\n#&gt; 6 me     0.0473\n\nset.seed(2)\n\ndistress_cor |&gt; \n  arrange(desc(abs(cor))) |&gt; \n  ggplot(aes(label = word, \n             color = cor, \n             size = abs(cor),\n             angle_group = cor &lt; 0)) +\n    geom_text_wordcloud(eccentricity = 1.2, show.legend = TRUE) +\n    scale_radius(range = c(4, 15), guide = \"none\") +\n    labs(caption = \"All correlations passed significance testing at p &lt; .05\") + \n    scale_color_gradient2(\n        name = \"Correlation\\nwith distress\\n(Kendall's τ)\",\n        low = \"blue3\", mid = \"white\", high = \"red3\", # set diverging color scale\n        ) +\n    theme_void()\n\n\n\n\n“Interesting” is still the most highly correlated word, indicating lack of distress, but now we can see that “and” and “we” are highly indicative of distress. The assurance that all correlations passed significance testing makes for a particularly convincing graphic.\n\n5.2.3 Advanced Word Clouds\nFor more information about how word clouds are generated and how to customize them, see Pennec (2023). Be careful though - any customization of your word clouds should be in the service of communicating information effectively.\n\n\n\n\n\n\n\nAdvantages of Word Clouds\n\n\n\n\n\nTo the Point: Word clouds emphasize words most characteristic of the variable of interest.\n\nNo Interactivity Required: Word clouds show many words at once without requiring interactivity.\nLooks Fancy\n\n\n\n\n\n\n\n\n\nDisadvantages of Word Clouds\n\n\n\n\n\nHard to Interpret Proportions: Size and color aesthetics make it extremely difficult to compare values of different words (e.g. Is x twice as blue as y?).\n\nVague: By showing many words at the same time, word clouds make it difficult to focus in on particular stories.\n\n\n\n\nPress the “View Source” button below to see the hidden code blocks in this chapter.\n\n\n\n\n\nBuechel, S., Buffone, A., Slaff, B., Ungar, L. H., & Sedoc, J. (2018). Modeling empathy and distress in reaction to news stories. CoRR, abs/1808.10399. http://arxiv.org/abs/1808.10399\n\n\nDavies, M. (2009). The 385+ million word corpus of contemporary american english (1990―2008+): Design, architecture, and linguistic insights. International Journal of Corpus Linguistics, 14, 159–190. https://www.english-corpora.org//coca/\n\n\nHellman, A. B. (2011). Vocabulary size and depth of word knowledge in adult-onset second language acquisition. International Journal of Applied Linguistics, 21(2), 162–182.\n\n\nKessler, J. S. (2017). Scattertext: A browser-based tool for visualizing how corpora differ. https://github.com/JasonKessler/scattertext\n\n\nPennec, E. le. (2023). Ggwordcloud: A word cloud geom for ggplot2. In lepennec.github.io/ggwordcloud/. https://lepennec.github.io/ggwordcloud/articles/ggwordcloud.html"
  },
  {
    "objectID": "data-viz-resources.html#general-data-visualization-materials",
    "href": "data-viz-resources.html#general-data-visualization-materials",
    "title": "6  Additional Resources for Data Visualization",
    "section": "\n6.1 General Data Visualization Materials",
    "text": "6.1 General Data Visualization Materials\n\n\nFundamentals of Data Visualization: A textbook on dataviz methodology, covering topics outlined here in much greater detail\n\nR Graph Gallery: Hundreds of example data visualization with tutorials on how to make them in R\n\nR Graphics Cookbook: A textbook with detailed instructions for creating graphs in R."
  },
  {
    "objectID": "data-viz-resources.html#advanced-methods",
    "href": "data-viz-resources.html#advanced-methods",
    "title": "6  Additional Resources for Data Visualization",
    "section": "\n6.2 Advanced Methods",
    "text": "6.2 Advanced Methods\n\n\nColoring in R’s blind spot: Tutorial introducing more than 100 palettes now included with base R, as well as functions for manipulating colors (used in multiple visualizations in this chapter)\n\nggnewscale: An intuitive method for incorporating multiple color scales in a single ggplot2 graphic (used in multiple visualizations in this chapter)\n\nggrepel: Tools for avoiding overlapping text labels in ggplot2 (used in multiple visualizations in this chapter)\n\ngganimate: A principled approach to animating with ggplot2\n\n\nggiraph: Dynamic and interactive html graphics for ggplot2\n\n\nggforce: Various advanced shapes, lines, scales, and plot types for ggplot2\n\n\nggborderline Lines that pop in ggplot2 (used in multiple visualizations in this chapter)\n\nrayshader: Cinematic 3D graphics for ggplot2"
  },
  {
    "objectID": "data-viz-resources.html#packages-for-unusual-plot-types",
    "href": "data-viz-resources.html#packages-for-unusual-plot-types",
    "title": "6  Additional Resources for Data Visualization",
    "section": "\n6.3 Packages for Unusual Plot Types",
    "text": "6.3 Packages for Unusual Plot Types\n\n\nggwordcloud: Word clouds\n\nggbeeswarm: Sina (AKA beeswarm) plots—almost always an improvement on box plots or violin plots\n\nggcorrplot: Correlation matrices\n\nDifferent Ways of Plotting U.S. Map in R: A book chapter comparing eight different methods for plotting map data in R\n\nggridges: Ridgeline plots\n\nggnetwork: Network data\n\nwormsplot: step charts with smooth transitions (in development by one of the authors of this book)\n\nAdditional ggplot2 extensions can be found here."
  },
  {
    "objectID": "data-sources.html",
    "href": "data-sources.html",
    "title": "Sources of Data",
    "section": "",
    "text": "Language is everywhere. The aspiring data-science-in-psychology researcher can find data in a variety of sources, ranging from novel experiments to web scraping. In this unit, we introduce the most popular sources of data, along with the advantages and disadvantages of each one.\nIn the sections on APIs and web scraping, we give a detailed tutorial on accessing these resources with code."
  },
  {
    "objectID": "experiments.html",
    "href": "experiments.html",
    "title": "7  Experiments",
    "section": "",
    "text": "Experiments are the backbone of causal inference, and text analysis is no exception. Whether in a laboratory or on Amazon’s Mechanical Turk, experiments can be carefully controlled and are a good way to mitigate the effects of confounding variables. Though many people associate advanced natural language processing with “big data,” the methods discussed in this book can be used effectively even in small-scale laboratory experiments.\nAn example of using experiments in quantitative language research: Sap et al. (2020) had online participants write either true stories that happened to them recently, or fictional stories about the same topic. They then used a large language model, GPT, to measure two likelihoods for each sentence in the story: the likelihood of the sentence given the previous sentence, and the likelihood of the sentence given a rough summary of the story. The ratio of these two likelihoods is a measure of how predictably the story flows from one point to another. Sap et al. (2020) found that fictional stories flow much more predictably than true ones. They also found that true stories begin to flow more predictably when they are retold 2-3 months later. Sap et al. (2022) reproduced these findings using a more advanced language model, GPT-3. We will discuss these and other methods of measuring linguistic complexity in Chapter 21.\n\n\n\n\n\n\nAdvantages of Experimental Data Collection\n\n\n\n\nControl: Experiments mitigate the effects of confounding variables.\nCustomization: Experimenters can tailor the experiment to fit their particular research questions.\n\n\n\n\n\n\n\n\n\nDisadvantages of Experimental Data Collection\n\n\n\n\nExpensive\nTime-Consuming\nSmall Sample Size: Because they are costly and time-consuming, experiments generally result in small datasets.\n\n\n\n\n\n\n\n\nSap, M., Horvitz, E., Choi, Y., Smith, N. A., & Pennebaker, J. (2020). Recollection versus imagination: Exploring human memory and cognition via neural language models. In D. Jurafsky, J. Chai, N. Schluter, & J. Tetreault (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics (pp. 1970–1978). Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.acl-main.178\n\n\nSap, M., Jafarpour, A., Choi, Y., Smith, N. A., Pennebaker, J. W., & Horvitz, E. (2022). Quantifying the narrative flow of imagined versus autobiographical stories. Proceedings of the National Academy of Sciences, 119(45), e2211715119. https://doi.org/10.1073/pnas.2211715119"
  },
  {
    "objectID": "corpora.html#archived-experimental-data",
    "href": "corpora.html#archived-experimental-data",
    "title": "8  Corpus Data",
    "section": "8.1 Archived Experimental Data",
    "text": "8.1 Archived Experimental Data\nScientists who run large experiments often publish their data online for use in further research. For example, data from Sap et al. (2020) and Sap et al. (2022), summarized in Chapter 7, are available online as the Hippocorpus dataset—a dataset we will be exploring in depth in Unit 2. Another good example is the Empathic Reactions dataset (Buechel et al., 2018), used in Section 4.2.1 and Chapter 5, in which participants read news stories, rated their own empathy and distress after reading them, and then described their thoughts about them verbally.\nOpen experimental data are often linked in published papers, especially since the founding of the Center for Open Science in 2013. Many psychology-related datasets can be browsed freely on osf.io, the Harvard Dataverse, and other locations.\n\n\n\n\n\n\nAdvantages of Archived Experimental Data\n\n\n\n\nProfessional: Experiments conducted by trained academics are generally well designed.\nWell-Documented: Datasets used in published papers have extensive documentation of the methods used to produce them.\n\n\n\n\n\n\n\n\n\nDisadvantages of Archived Experimental Data\n\n\n\n\nSometimes Not Well-Documented: Datasets not used in published papers often have poor documentation.\nSmall Sample Size: Experiments often result in relatively small datasets, which can pose problems for certain NLP methods."
  },
  {
    "objectID": "corpora.html#linguistics-corpora",
    "href": "corpora.html#linguistics-corpora",
    "title": "8  Corpus Data",
    "section": "8.2 Linguistics Corpora",
    "text": "8.2 Linguistics Corpora\nThe field of linguistics has a long tradition of corpus data. Linguistics corpora provide extensive records of spoken and written speech in a wide range of contexts. These corpora are often very large and professionally curated, making them ideal for the techniques described in this book. On the other hand, they are generally curated with linguistics in mind, not psychology. This means that applying them to psychological questions requires some ingenuity.\nOne popular semi-experimental linguistics corpus is the HCRC Map Task Corpus (Anderson et al., 1991), in which pairs of participants collaborated in a communication game. In each pair, one partner could see a treasure map with a path through various landmarks, while the other partner had a similar map without a path. The first partner explained to the second how to draw the path. The partners’ communication accuracy can be measured as the distance between the drawn path and the original. Full dialogue transcriptions, as well as accuracy scores, are available online. The Map Task Corpus has been reproduced in many languages, including Hebrew, and is commonly used in psychology. For example, Dideriksen et al. (2023) used a Danish version of the Map Task Corpus, along with other dialogue corpora, to track the ways that speakers collaborate to achieve mutual understanding in different contexts.\n\nEnglish-Corpora.org: A list of the most widely used corpora of naturalistic English speech and writing, with download links for each. Also includes preprocessed data, such as word frequency counts for nearly 100 genres, from the Corpus of Contemporary American English (Davies, 2009), used in Section 5.1.1.\nUniversity of British Columbia Language Corpora List: Links to written and spoken language data in dozens of languages, including from bilingual and multilingual speakers.\nWikipedia’s List of Text Corpora\nList of NLP Corpora: Links to useful corpora for NLP tasks like task-oriented dialogue, translation, and sentiment analysis.\nConvokit Datasets: Links to written and spoken dialogues from debates, news interviews, telephone conversations, video chats, legal trials, and more.\n\n\n\n\n\n\n\nAdvantages of Linguistics Corpora\n\n\n\n\nProfessional: Linguistics corpora are generally well curated and well documented.\nEcological Validity: Corpora are often large and naturalistic—including for spoken dialogue, a domain that is otherwise out of reach for NLP.\n\n\n\n\n\n\n\n\n\nDisadvantages of Linguistics Corpora\n\n\n\n\nDomain-Specific: Linguistics corpora are generally created by linguists for linguists."
  },
  {
    "objectID": "corpora.html#data-gathered-from-the-internet",
    "href": "corpora.html#data-gathered-from-the-internet",
    "title": "8  Corpus Data",
    "section": "8.3 Data Gathered From the Internet",
    "text": "8.3 Data Gathered From the Internet\nThe Internet is full of text, and you are not the first one to want to use it for research. Many corpora of online text data are free to download.\nSome sets of Internet data are professionally curated and well balanced. For example, the Blog Authorship Corpus (Schler et al., 2006) includes 681,288 blog posts annotated with age group (binned into ages 13-17, 23-27, and 33-47) and gender of author, with an equal number of male and female bloggers in each age group. Similarly, the 20 Newsgroups data set includes 20,000 newsgroup documents, partitioned evenly across 20 different newsgroups.\nSome sets of Internet data are available only post-processing. For example, Eichstaedt et al. (2015) published Twitter n-gram (and LDA topic) frequencies by US county, along with corresponding measures of well-being (featured in Chapter 3).\nSome sets of Internet data are very lightly curated. For example, the Reddit Top 2.5 Million dataset contains the top 1,000 all-time posts from the top 2,500 subreddits in August 2013, excluding NSFW subreddits.\nSome sets of archived Internet data are not curated at all. These are sometimes referred to as data dumps. For example, Baumgartner et al. (2020) published all Reddit Submissions and Comments posted during April 2019. Even more extensive data dumps of Reddit, covering historical data back to Reddit’s inception, can be found in records of Pushshift Reddit. Similar archives exist for Twitter. Data dumps are usually in JSON format. A JSON file is like a list in R, but formatted slightly differently. For a tutorial on processing JSON data in R, see the relevant chaper in R for Data Science.\nMost research topics in psychology do not require up-to-date data. As such, historical archives can be an invaluable resource. Biester et al. (2022) is a great example:\nAn example of social media archives in psychology research: Biester et al. (2022) used patterns curated by Cohan et al. (2018) to search Pushshift Reddit for users who publicly shared a depression diagnosis (e.g. “I have been diagnosed with depression”). They then used dictionary-based methods (Chapter 14) to measure various emotional qualities in users’ posts during the weeks leading to their declaration of the depression diagnosis, and in the weeks following. They found that anxiety, sadness, and cognitive processing increase in the weeks leading up to the declaration, and decrease afterwards.\n\n\n\n\n\n\nAdvantages of Archival Internet Data\n\n\n\n\nEasy: Pre-gathered datasets are low-cost and low-effort, often for very large sample sizes.\nUnintrusive: With pre-gathered datasets, you don’t have to worry about API usage limits or web scraping etiquette.\n\n\n\n\n\n\n\n\n\nDisadvantages of Archival Internet Data\n\n\n\n\nOld: Archival data do not reflect current events or recent trends.\n\n\n\n\n\n\n\n\n\nA Disclaimer on Social Media Data Dumps\n\n\n\nSince Reddit and Twitter restricted their API access in 2023, the legal status of large archival data dumps from those platforms (such as Pushshift Reddit) has been unclear. We are not qualified to give legal advice, but as long as you are not using the data for profit, you are unlikely to get in trouble."
  },
  {
    "objectID": "corpora.html#other-public-data-sources",
    "href": "corpora.html#other-public-data-sources",
    "title": "8  Corpus Data",
    "section": "8.4 Other Public Data Sources",
    "text": "8.4 Other Public Data Sources\n\nKaggle: An online hub for data science, including many text- and psychology-related datasets\nHathiTrust: A digital library of 18+ million digitized books, including many curated collections\nForbes list of 30 Amazing (And Free) Public Data Sources\n\n\n\n\n\n\nAnderson, A. H., Bader, M., Bard, E. G., Boyle, E., Doherty, G., Garrod, S., Isard, S., Kowtko, J., McAllister, J., Miller, J., Sotillo, C., Thompson, H. S., & Weinert, R. (1991). The HCRC map task corpus. Language and Speech, 34(4), 351–366. https://doi.org/10.1177/002383099103400404\n\n\nBaumgartner, J., Zannettou, S., Keegan, B., Squire, M., & Blackburn, J. (2020). The pushshift reddit dataset. Zenodo. https://doi.org/10.5281/zenodo.3608135\n\n\nBiester, L., Pennebaker, J., & Mihalcea, R. (2022). Emotional and cognitive changes surrounding online depression identity claims. PLOS ONE, 17(12), 1–20. https://doi.org/10.1371/journal.pone.0278179\n\n\nBuechel, S., Buffone, A., Slaff, B., Ungar, L. H., & Sedoc, J. (2018). Modeling empathy and distress in reaction to news stories. CoRR, abs/1808.10399. http://arxiv.org/abs/1808.10399\n\n\nCohan, A., Desmet, B., Yates, A., Soldaini, L., MacAvaney, S., & Goharian, N. (2018). SMHD: A large-scale resource for exploring online language usage for multiple mental health conditions. In E. M. Bender, L. Derczynski, & P. Isabelle (Eds.), Proceedings of the 27th international conference on computational linguistics (pp. 1485–1497). Association for Computational Linguistics. https://aclanthology.org/C18-1126\n\n\nDavies, M. (2009). The 385+ million word corpus of contemporary american english (1990―2008+): Design, architecture, and linguistic insights. International Journal of Corpus Linguistics, 14, 159–190. https://www.english-corpora.org//coca/\n\n\nDideriksen, C., Christiansen, M. H., Tylén, K., Dingemanse, M., & Fusaroli, R. (2023). Quantifying the interplay of conversational devices in building mutual understanding. Journal of Experimental Psychology: General, 152, 864–889. https://doi.org/10.1037/xge0001301\n\n\nEichstaedt, J. C., Schwartz, H. A., Kern, M. L., Park, G., Labarthe, D. R., Merchant, R. M., Jha, S., Agrawal, M., Dziurzynski, L. A., Sap, M., Weeg, C., Larson, E. E., Ungar, L. H., & Seligman, M. E. P. (2015). Psychological language on twitter predicts county-level heart disease mortality. Psychological Science, 26(2), 159–169. https://doi.org/10.1177/0956797614557867\n\n\nSap, M., Horvitz, E., Choi, Y., Smith, N. A., & Pennebaker, J. (2020). Recollection versus imagination: Exploring human memory and cognition via neural language models. In D. Jurafsky, J. Chai, N. Schluter, & J. Tetreault (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics (pp. 1970–1978). Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.acl-main.178\n\n\nSap, M., Jafarpour, A., Choi, Y., Smith, N. A., Pennebaker, J. W., & Horvitz, E. (2022). Quantifying the narrative flow of imagined versus autobiographical stories. Proceedings of the National Academy of Sciences, 119(45), e2211715119. https://doi.org/10.1073/pnas.2211715119\n\n\nSchler, J., Koppel, M., Argamon, S., & Pennebaker, J. (2006). Effects of age and gender on blogging. 199–205."
  },
  {
    "objectID": "apis.html#api-basic-concepts",
    "href": "apis.html#api-basic-concepts",
    "title": "9  Web APIs",
    "section": "\n9.1 API Basic Concepts",
    "text": "9.1 API Basic Concepts\n\n\nRequests: Each time you visit a URL associated with an API, you are submitting a request for data.\n\nEndpoints: Every API has at least one endpoint, a contact point for particular types of requests.\n\nRate Limits: Many APIs set limits on the number of requests you can make per minute (or per second). This is because processing requests costs time and money for the host. If you go beyond the rate limit, the API will return an error like “429 Too Many Requests.”\n\n\nAuthentication: Some APIs are not open to the public, instead requiring users to apply for access or pay for a subscription. When accessing these APIs, you need an API key or an access token. This is your password for the API.\n\n\n9.1.1 vosonSML\n\nlibrary(vosonSML)\n\nFor accessing social media APIs with vosonSML, you only need two functions:\n\n\nAuthenticate() creates a credential object that contains any keys or access tokens needed to access a particular API. This credential object can be reused as long as your credentials don’t change.\n\nCollect() initiates a series of API requests and stores the results as a dataframe or list of dataframes.\n\nvosonSML also provides tools for working with network data (i.e. the ways in which users or posts are connected to one another), but these will not be covered in this textbook."
  },
  {
    "objectID": "apis.html#reddit",
    "href": "apis.html#reddit",
    "title": "9  Web APIs",
    "section": "\n9.2 Reddit",
    "text": "9.2 Reddit\nReddit generated over 3 billion posts and comments in 2022. Many of these contain long-form text. And its API is free. These traits make it very useful to researchers.\nReddit content exists on three levels:\n\n\nCommunities, called “subreddits” are spaces for users to post about a specific topic. Individual subreddits are referred to as “r/SUBREDDIT”. For example, r/dataisbeautiful is for data visualizations, r/NaturalLanguage is for posts about natural language processing, and r/SampleSize is a place to gather volunteer participants for surveys and polls. Communities are policed by moderators, users who can remove posts or ban other users from the community.\n\nPosts are posted by users to a particular subreddit. Each post has a title, which is always text, and content, which can contain text, images, and videos.\n\nComments are responses to posts, responses to responses to posts, responses to responses to responses to posts, etc. These are always text.\n\n\n9.2.1 The Reddit Algorithm\nReddit data are not representative samples of the global population. They are not even representative samples of Reddit users. This is partly due to the dynamics of the Reddit ranking algorithm, which gives precedent to viral content. The details of the algorithm are no longer public, but it is largely based on “upvotes” and “downvotes” from the community, and probably also incorporates the time since posting. The ranking system for comments is almost certainly different from the ranking system for posts. Reddit also has a “Karma” system, by which users who post popular content get subsequent content boosted. This creates an incentive system which is sometimes exploited by the advertising industry. The bottom line: Reddit posts are disproportionately viral. To partially counteract this when gathering Reddit data, set the API to sort by recency (sort = \"new\") rather than the default, “best” (Section 9.2.3).\n\n9.2.2 Communities\nTo gather retrieve the posts from a Reddit community, call Authenticate(\"reddit\") followed by Collect() with endpoint = \"listing\". To specify the particular community (or communities) in which you are interested, use the subreddits parameter. For example, the following code retrieves the 20 most recent posts from r/RedditAPIAdvocacy, a subreddit devoted to fighting restriction of the Reddit API.\n\nAPIAdvocacy_posts &lt;- \n  Authenticate(\"reddit\") |&gt;\n  Collect(\n    endpoint = \"listing\", \n    subreddits = \"RedditAPIAdvocacy\",\n    sort = \"new\",   # newest posts first\n    period = \"all\", # all time\n    max = 20,       # 20 most recent posts\n    verbose = TRUE\n    )\n\n#&gt; Collecting thread listing for subreddits...\n\n\n#&gt; Waiting between 6 and 8 seconds per request.\n\n\n#&gt; Request subreddit listing: RedditAPIAdvocacy (max items: 20).\n#&gt; subreddit_id | subreddit         | count\n#&gt; ----------------------------------------\n#&gt; t5_8dyuxh    | RedditAPIAdvocacy | 4    \n#&gt; Collected metadata for 4 threads in listings.\n#&gt; Done.\n\nhead(APIAdvocacy_posts)\n\n#&gt; # A tibble: 4 × 111\n#&gt;   approved_at_utc subreddit      selftext author_fullname saved mod_reason_title\n#&gt;   &lt;lgl&gt;           &lt;chr&gt;          &lt;chr&gt;    &lt;chr&gt;           &lt;lgl&gt; &lt;lgl&gt;           \n#&gt; 1 NA              RedditAPIAdvo… \"\"       t2_8uyouiqz     FALSE NA              \n#&gt; 2 NA              RedditAPIAdvo… \"\"       t2_177u1k       FALSE NA              \n#&gt; 3 NA              RedditAPIAdvo… \"\"       t2_g0y5d        FALSE NA              \n#&gt; 4 NA              RedditAPIAdvo… \"Last w… t2_177u1k       FALSE NA              \n#&gt; # ℹ 105 more variables: gilded &lt;int&gt;, clicked &lt;lgl&gt;, title &lt;chr&gt;,\n#&gt; #   link_flair_richtext &lt;list&gt;, subreddit_name_prefixed &lt;chr&gt;, hidden &lt;lgl&gt;,\n#&gt; #   pwls &lt;lgl&gt;, link_flair_css_class &lt;lgl&gt;, downs &lt;int&gt;,\n#&gt; #   thumbnail_height &lt;lgl&gt;, top_awarded_type &lt;lgl&gt;, hide_score &lt;lgl&gt;,\n#&gt; #   name &lt;chr&gt;, quarantine &lt;lgl&gt;, link_flair_text_color &lt;chr&gt;,\n#&gt; #   upvote_ratio &lt;dbl&gt;, author_flair_background_color &lt;lgl&gt;,\n#&gt; #   subreddit_type &lt;chr&gt;, ups &lt;int&gt;, total_awards_received &lt;int&gt;, …\n\n\nIn psychology research, the most interesting subreddits are often those devoted to psychological disorders, for example r/depression, r/socialanxiety, r/SuicideWatch, r/bipolarreddit, and r/opiates. Subreddits devoted to intellectual discussion, such as r/changemyview, r/IAmA, and r/ExplainLikeImFive, are also interesting subjects for research. Lastly, much research is devoted to the behavior of Redditors in political communities. The following is one elegant example:\nAn example of using Reddit communities in research: In a series of experiments, Ashokkumar & Pennebaker (2022) used the Reddit API and other sources to develop a dictionary-based measure of group identity strength. A dictionary is a list of words associated with a given psychological or other construct, for example associating “sleepy” and “down” with depression (see Chapter 14). Ashokkumar and Pennebaker had participants write small free-form responses and fill out questionnaires on group identity strength. They then identified existing dictionaries that were associated with the questionnaire-based measures, and used these to construct a composite measure of group identity strength. When applied to text, they called this measure “unquestioning affiliation.” Among college students, they showed that unquestioning affiliation in writing could predict whether students would drop out of college one year later. Finally, they applied their method to naturalistic data retrieved from the Reddit API, from the political communities r/The_Donald and r/hillaryclinton, and showed that users’ unquestioning affiliation predicted the duration that they would stay in the community before leaving.\n\n9.2.3 Threads\nA post with all of its associated comments is called a thread. For example, Hadley Wickham, the founder of the tidyverse, ran a thread on the r/dataisbeautiful subreddit in 2015, in which he answered commenters’ questions. To retrieve that thread, first find its URL. Do this by finding the threat on Reddit and copying the link from your web browser. Then call Authenticate(\"reddit\") and Collect(), like so:\n\n# List of thread urls (in this case only one)\nthreads &lt;- c(\"https://www.reddit.com/r/dataisbeautiful/comments/3mp9r7/im_hadley_wickham_chief_scientist_at_rstudio_and/\")\n\n# Retrieve the data\n## Since the Reddit API is open, we don't need\n## to give any passwords to Authenticate()\nhadley_threads &lt;- \n  Authenticate(\"reddit\") |&gt;\n  Collect(\n    threadUrls = threads, \n    sort = \"new\", # newest comments first\n    verbose = TRUE # give updates while running\n    )\n\n# Peak at Hadley's responses\nhadley_threads |&gt;\n  filter(user == \"hadley\") |&gt; \n  select(structure, comment) |&gt; \n  head()\n\n#&gt; # A tibble: 6 × 2\n#&gt;   structure comment                                                             \n#&gt;   &lt;chr&gt;     &lt;chr&gt;                                                               \n#&gt; 1 4_1       \"1.  Should be out early next year \\n\\n2. Unfortunately not. \\n\\n3.…\n#&gt; 2 6_1       \"I'm not aware of any discussions, but I'm not that likely to be si…\n#&gt; 3 7_1       \"They already do occur in most errors (or at least in the `tracebac…\n#&gt; 4 10_1      \"No, but the next version features customisable keyboard short cuts…\n#&gt; 5 12_1      \"I offered to help the department with the overall design of stat40…\n#&gt; 6 13_1      \"ggplot worked using function composition instead of addition. So i…\n\n\nThe resulting dataframe has many columns. The most useful are the following:\n\n\ncomment is the text of the comment itself.\n\nuser is user who posted the comment.\n\nstructure is the tree structure leading to the comment. For example, “137_6_2” is the 2nd comment on the 6th comment on the 137th comment on the original post.\n\ncomm_date is the date and time of the comment, in UTC. Since this is in character format, it needs to be converted to a datetime with lubridate::as_datetime().\n\nBy processing the structure values, we can conceptualize the thread as a tree, with the original post (OP) as the root and comments as branches:\n\nlibrary(ggraph)\n\nhadley_threads |&gt; \n  mutate(\n    level = str_count(structure, \"_\") + 1L,\n    parent = str_remove(structure, \"_[[:digit:]]+$\"),\n    parent = if_else(level == 1, \"0\", parent)\n    ) |&gt; \n  select(parent, structure) |&gt; \n  tidygraph::as_tbl_graph() |&gt; \n  ggraph(layout = 'tree', circular = FALSE) +\n    geom_edge_diagonal(alpha = .2, linewidth = 1) +\n    geom_node_point(shape = 21, fill = \"orangered\") +\n    theme_void()\n\n\n\n\nAn example of using Reddit threads in research: Xiao & Mensah (2022) collected threads from r/changemyview, a community in which the OP makes a claim, commenters make arguments against that claim, and the OP responds with “delta points” to indicate how much their view has been changed. Xiao & Mensah (2022) analyzed the frequency of delta points at each level of the thread tree, and found that the OP’s view tended to change most after the 2nd, 4th, 6th, 8th, and 10th levels of comments—in other words, every other level. They then analyzed the semantic similarity between comments using cosine similarity (Section 17.1.2) between simple word counts. The results suggested that every-other-level comments tend to elaborate and refine the comments immediately before them, so that the latter are perceived to be more persuasive.\n\n9.2.4 Other Reddit Features\nFor most applications, vosonSML is a sufficient Reddit API wrapper. For slightly more advanced functionality like searching subreddits with a keyword or retrieving a user’s comment history, we recommend Reddit Extractor.\n\n\n\n\n\n\nAdvantages of Reddit Data\n\n\n\n\n\nExplicit Communities: Reddit communities are clearly defined and explicit about their purposes. Reddit includes communities devoted to fictional storytelling, factual storytelling, personal reflection, technical advice, political discussion, joke telling, and much more. This makes it to gather a domain-specific sample.\n\nLong-form Text Responses: While some social media platforms have character limits for posts or comments, Reddit has many communities devoted to long-form text. Longer documents can make characterization of their content more accurate.\n\nAnonymity: Reddit users can remain relatively anonymous, which might encourage more honest and open sharing of experiences.\n\n\n\n\n\n\n\n\n\nDisadvantages of Reddit Data\n\n\n\n\n\nSelection Bias: Certain subreddits may attract specific demographics, leading to potential selection bias in the data.\n\nAnonymity: In some cases, anonymity may make user behavior less representative. For example, many users have multiple accounts, which they use for different activities on the platform, making users seem disproportionately narrow in their interests."
  },
  {
    "objectID": "apis.html#twitter-x",
    "href": "apis.html#twitter-x",
    "title": "9  Web APIs",
    "section": "\n9.3 Twitter / X",
    "text": "9.3 Twitter / X\nTwitter has been called the “model organism” of big data research (Tufekci, 2014). This is because, until 2023, Twitter was the largest source of free, open social text data. In contrast to Facebook, almost all Twitter activity is public. Twitter’s standardized character limit (originally 140, now 280), along with its simple network structure make structuring and the data easy.\n\n9.3.1 Followers, Mentions, and Retweets\nWhereas Reddit users subscribe to communities, Twitter users subscribe to other users. This is called “following”. They can also “retweet” other users’ posts to their own feed, or explicitly mention other users in posts using the “@” symbol. This focus on connections between individuals means that each user’s communal associations can be characterized in a rich, nuanced way.\nAn example of using Twitter networks in research: Mosleh et al. (2021) collected Twitter IDs of participants who filled out a set of questions with intuitively compelling but incorrect answers, testing analytic thinking. They then analyzed the Twitter accounts that each participant followed, and found a large cluster of accounts followed preferentially by people with less analytic thinking. This group most prominently featured Twitter accounts of retail brands.\n\n9.3.2 Geolocation on Twitter\nTwitter allows users to specify a geographical location along their post. While these user provided locations can sometimes be difficult to interpret, they allow analyses by geographic location.\nAn example of using Twitter geolocation in research: Takhteyev et al. (2012) sampled users from their posts on Twitter’s public feed, and users followed by them. They then analyzed the distribution of these ties over geographical distance, national boundaries, and language differences. They found that Twitter users are most strongly tied to people in their own city, and that between regions, the number of airline flights was the best predictor of ties.\n\n\n\n\n\n\nAdvantages of Twitter Data\n\n\n\n\n\nCharacter Limit: Twitter’s character limit means that Tweets vary less in their length. This can decrease variance in measurements.\n\nPure Network Structure: As opposed to Reddit’s distinct communities and hierarchical comment trees, Twitter has a network-like social structure in which users are linked to other individual users or posts. This structure can reveal underlying communities and the strength of the connections between them.\n\n\n\n\n\n\n\n\n\nDisadvantages of Twitter Data\n\n\n\n\n\nCharacter Limit: The character limit on tweets may limit the depth of responses and the ability to convey complex psychological experiences. The smaller sample of words in each document can also make measurement less reliable.\n\nLimited Context: Tweets may lack context, making it challenging to fully understand the meaning behind short messages.\n\nNo more free API: While lots of historical Twitter data are available on the internet see 8, the API has been prohibitively expensive since 2023."
  },
  {
    "objectID": "apis.html#mastodon",
    "href": "apis.html#mastodon",
    "title": "9  Web APIs",
    "section": "\n9.4 Mastodon",
    "text": "9.4 Mastodon\nIn some ways, you can think of Mastodon as open source Twitter. Indeed, a large portion of the Mastodon user base started using it as a replacement for Twitter after controversial events in 2022. Like Twitter users, Mastodon users post short posts to their feed, in which they can mention other users with “@” or use hashtags with “#”. Like Twitter users, Mastodon users can follow other users and be followed by them.\nDespite their similarities, social networks on Mastodon and Twitter are not the same. The biggest difference is that each Mastodon user is associated with a server (also known as an instance). Mastodon servers can have hundreds of thousands of users, or just a few. Each server runs the Mastodon source code independently and hosts all of its users’ content. Many servers have a theme based on a specific interest. It is also common for servers to be based around a particular locality, region, ethnicity, or country. For example, tech.lgbt is for LGBTQIA+ technologists, SFBA.social is for the San Francisco Bay Area in California, and fediscience.org is for active scientists.\nBecause of their topic-specific organization, Mastodon servers can function a bit like Reddit communities. Like Reddit communities, Mastodon servers can set their own rules for content. They can even change the default 500-character limit. The key difference is that on Reddit, posts are associated with a community, whereas on Mastodon, users are associated with a community. In other words, community affiliation on Mastodon is less related to the content of its posts and more related to the identity of its users. This feature can be useful when collecting user activity. For example, if we wanted to study the overall behavior of members of the LGBTQIA+ community, we could collect the activity of users affiliated with the tech.lgbt and other community-specific servers.\n\n9.4.1 Servers\nMastodon server feeds are organized chronologically (unlike Reddit or Twitter, which have complex ordering algorithms). To retrieve the most recent posts on a particular server using vosonSML, we call Authenticate(\"mastodon\") followed by Collect() with the “search” endpoint. The name of the server is identified with “instance”.\n\nmast_science &lt;- \n  Authenticate(\"mastodon\") |&gt;\n  Collect(\n    endpoint = \"search\",\n    instance = \"fediscience.org\",\n    local = TRUE,\n    numPosts = 100, # number of most recent posts\n    verbose = TRUE\n  )\n\nThe global feed (i.e. across all servers) can be accessed by setting local = FALSE. You can also retrieve posts with a specific hashtag using the hashtag parameter (e.g. hashtag = \"rstats\").\nIn all cases, the call results in a list of two dataframes, “posts” and “users”. In the “posts” dataframe we have columns such as:\n\n\ncontent.text is the text of the post itself.\n\naccount is a column of one-row dataframes containing information about the user who posted each post. You can convert this into a simple account ID column with mutate(account_id = sapply(account, function(c) pull(c,id))).\n\nid is a unique ID for the post.\n\nin_reply_to_id is the ID of the post that this post is directly responding to. This can be used to build tree structures, as we did in Section 9.2.3, but see Section 9.4.2 for an alternative way to do this.\n\nin_reply_to_account_id is the account ID of the poster to whom this post is directly responding. This can be used to build networks of users as opposed to threads.\n\ncreated_at is the date and time of the post in UTC, already in datetime format.\n\nThe “users” dataframe has the same information as the account column of “posts”, but formatted as a single dataframe with as one row per user.\n\n9.4.2 Threads\nSince Mastodon users can mark their posts as a reply to an earlier post, these posts are essentially “comments” on that original post. A chain of posts that reply in to an original post is called a thread. We can collect the contents of one or more threads with endpoint = \"thread\", by specifying the URL of the first post in the thread with threadUrls. For example, we can retrieve a thread by one the authors of this book, Almog:\n\nalmog_thread &lt;- \n  Authenticate(\"mastodon\") |&gt;\n  Collect(\n    endpoint = \"thread\",\n    threadUrls = c(\"https://fediscience.org/@almogsi/110513658753942590\"),\n    verbose = TRUE\n  )\n\nalmog_thread$posts |&gt; \n  select(created_at, content.text) |&gt; \n  head()\n\n\n9.4.3 Other Mastodon Features\nFor most applications, vosonSML is a sufficient Mastodon API wrapper. For slightly more advanced functionality like searching accounts or retrieving a user’s follows and followers, we recommend rtoot.\n\n\n\n\n\n\nAdvantages of Mastodon Data\n\n\n\n\n\nFree: Because Mastodon’s servers are decentralized (i.e. there is no single company controlling its operation), there is no concern that its data will become unavailable.\n\nNo Feed Algorithm: Mastodon does not use a black-box algorithm to decide what users see on their feed. Users have total control over their own subscriptions (e.g. users who follow more accounts have more content on their feeds). This eliminates a possible confounding factor present in many social media platforms.\n\nDomain-Specific Servers: Users are associated with a particular server, often associated with an identity or region. This allows more targeted data collection.\n\n\n\n\n\n\n\n\n\nDisadvantages of Mastodon Data\n\n\n\n\n\nSampling Bias: Because Mastodon is largely populated by users who fled from Twitter or Facebook, their demographics may be especially unrepresentative.\n\nSmaller User Base: Mastodon has a much smaller user base compared to major platforms like Twitter and Reddit."
  },
  {
    "objectID": "apis.html#youtube",
    "href": "apis.html#youtube",
    "title": "9  Web APIs",
    "section": "\n9.5 YouTube",
    "text": "9.5 YouTube\nYouTube is structurally similar to Twitter in that neither users nor posts are explicitly associated with a community. Users subscribe directly to other users’ channels. Each user can upload posts to their own channel. As on Reddit, these posts generally have a title, a description, and a space for comments.\nYouTube is different from other social media platforms in a few ways. Most obviously, posts consist primarily of videos. This means that analyzing the content of a post’s description will not give an accurate representation of the post’s content. Analyzing the content of the video directly is generally impossible, since YouTube does not allow videos to be downloaded through their API. One possible workaround uses YouTube’s automatically generated video transcriptions—though YouTube does not allow access to full transcriptions for standard API key bearers, there are some workarounds available. Nevertheless, we will limit ourselves here to text data. This primarily consists of comments on videos.\n\n9.5.1 Video Comments\nAnother important difference between YouTube and other social media platforms is in the structure of the comments. Whereas Reddit comments (or Twitter/Mastodon threads) can respond one to another, generating a deep tree-like structure, YouTube comments have only two levels of depth. In other words, comments either respond directly to a video (top-level) or they respond to a top-level comment (level two). This feature can sometimes make conversations hard to track, since level two comments may be responding to another level two comment, even if they are not explicitly marked as such. On the other hand, this feature may constrain YouTube comments to respond more directly to the video.\nThe interpretation of YouTube comments as direct responses to the original video is enhanced by the rich video format of the stimulus, which likely draws users in more than a text-only post would (cf. Yadav et al., 2011). For this reason, YouTube can be a good platform for studying responses to shared stimuli in a social context.\nTo access the the YouTube API, you will need an API key. You can get an API key in a matter of seconds by registering your project with Google cloud. Once you have the key, you can access the API through vosonSML by calling youtube_auth &lt;- Authenticate(\"youtube\", apiKey = \"xxxxxxxxxxxxxx\"), followed by Collect(). In the code below, we’ll collect the comments on 3Blue1Brown’s excellent explanation of deep learning algorithms.\n\n# retrieve youtube_auth from separate R script \n# (so as not to publicize my key)\nsource(\"~/Projects/sandbox/youtube_auth.R\")\n\n# this could also be a vector of URLs\nvideo_url &lt;- \"https://www.youtube.com/watch?v=aircAruvnKk\"\n\n# collect comments\ndeep_learning_comments &lt;- \n  youtube_auth |&gt;\n  Collect(videoIDs = video_url,\n          maxComments = 100,\n          verbose = TRUE)\n\nThis call results in a dataframe with many columns, including the following:\n\n\nComment is the text of the comment itself.\n\nAuthorDisplayName is the username of the commenter.\n\nAuthorChannelUrl is the URL for the commenter’s own channel.\n\nPublishedAt is the date and time of the comment, in UTC. Since this is in character format, it needs to be converted to a datetime with lubridate::as_datetime().\n\nCommentID is a unique ID for the comment.\n\nParentID is the ID of the comment to which this comment is responding.\n\nIf we visualize these comments like we visualized the Reddit comments in Section 9.2.3, we see that the tree has only two levels:\n\ndeep_learning_comments |&gt; \n  select(ParentID, CommentID) |&gt; \n  tidygraph::as_tbl_graph() |&gt; \n  ggraph::ggraph(layout = 'tree', circular = FALSE) +\n    ggraph::geom_edge_diagonal(alpha = .2, linewidth = 1) +\n    ggraph::geom_node_point(shape = 21, fill = \"orangered\") +\n    theme_void()\n\n\n\n\nAn example of using YouTube comments in research: Rosenbusch et al. (2019) collected comments from 20 vlogs each from 110 vloggers on YouTube, along with the transcripts of those videos. They used dictionary-based word counts (Chapter 14) to measure the emotional content of both video transcripts and video comments. Using a multilevel model, they found that both video- and channel-level emotional content independently predict commenter emotions.\n\n\n\n\n\n\nAdvantages of YouTube Data\n\n\n\n\n\nRich Stimuli: YouTube videos are extended and multimodal (i.e. they include both audio and visual stimuli). Video comments can be usefully construed as responses to that stimulus.\n\nConstrained Structure: YouTube limits its comment trees to two levels, which can simplify analyses.\n\n\n\n\n\n\n\n\n\nDisadvantages of YouTube Data\n\n\n\n\n\nMissing Context: YouTube comments respond to videos. Since videos are a generally unavailable for download and are in any case of a different medium than text-based comments, measuring the relationship between a comment and the video it refers to can be difficult.\n\nConstrained Structure: Because YouTube limits its comment trees to two levels, complex referential structures can be difficult to decode."
  },
  {
    "objectID": "apis.html#sec-other-apis",
    "href": "apis.html#sec-other-apis",
    "title": "9  Web APIs",
    "section": "\n9.6 Other Web APIs",
    "text": "9.6 Other Web APIs\nIn this chapter, we have covered four social media platforms and—with the exception of Twitter—how to access their APIs using vosonSML. Nevertheless, this has been far from an exhaustive list of social media platforms that provide APIs. Many other sources of data exist, each with their own advantages and disadvantages. Many of these do not have custom R wrappers for their APIs, but reading a little API documentation shouldn’t deter you from finding your ideal dataset. The following are some of our favorite sources of psychologically interesting data accessible with public APIs.\n\n\nStackExchange is a network of question-and-answer sites like StackOverflow (for programmers), MathOverflow (for mathematicians), Arqade (for videogamers), Cross Validated (for statistics and machine learining), Science Fiction & Fantasy (for fans) and many more. StackExchange provides a public API for retrieving questions, answers, and comments from their sites.\n\nWikipedia provides a public API for retrieving content, including individial user data.\n\nSemantic Scholar is a website for navigating academic literature. It offers a public API for accessing abstracts, references, citations, and other information about academic publications. The API also allows access to their contextualized semantic embeddings of publications see 19. Obtaining an API key requires a simple registration.\nLike Semantic Scholar, CORE offers access to full text articles and metadata for open access research papers. Obtaining access requires a simple registration.\nFacebook advertises a research-oriented API, which requires registration.\nMastodon is not the only open source, decentralized social media platform with an API. There are many, though most are small.\n\nTo access one of these APIs through R, we recommend using the httr2 package. Most APIs send data in JSON format. As we mentioned in Chapter 8, a JSON file is like a list in R, but formatted slightly differently. The httr2 package also has functions for converting JSON to R objects. In the example below, we write a function to retrieve all questions with a particular tag from a particular site on StackExchange, and format them as a tibble.\n\nlibrary(httr2)\n\n# StackExchange API search endpoint\nendpoint &lt;- request(\"https://api.stackexchange.com/2.2/search?\")\n\n# Function to retrieve posts with a given tag\nget_tagged_posts &lt;- function(tag, ...,\n                             # default params\n                             site = \"stackoverflow\",\n                             pagesize = 100,\n                             user = NULL,\n                             order = \"desc\",\n                             sort = \"activity\",\n                             page = 1) {\n  # define request params\n  params &lt;- list(\n    ...,\n    order = order,\n    sort = sort,\n    tagged = tag,\n    site = site,\n    pagesize = pagesize,\n    page = page\n    )\n  \n  # add user agent (this is polite for APIs)\n  req &lt;- endpoint |&gt; \n    req_user_agent(user)\n  \n  # add query parameters\n  req &lt;- req |&gt; \n    req_url_query(!!!params)\n  \n  # perform request + convert to list\n  resp &lt;- req |&gt; \n    req_perform() |&gt; \n    resp_body_json()\n  \n  # warn the user if API gave a \"backoff\" message, \n  if(!is.null(resp$backoff)){\n    warning(\"Received backoff request from API.\",\n            \"Wait at least \", resp$backoff, \" seconds\",\n            \" before trying again!\")\n  }\n  \n  # Convert list to tibble,\n  # keeping only relevant variables\n  tibble::tibble(\n    title = sapply(\n      resp$items, \n      function(x) x$title\n      ),\n    is_answered = sapply(\n      resp$items, \n      function(x) x$is_answered\n      ),\n    view_count = sapply(\n      resp$items, \n      function(x) x$view_count\n      ),\n    creation_date = sapply(\n      resp$items, \n      function(x) x$creation_date\n      ),\n    user_id = sapply(\n      resp$items, \n      function(x) x$user_id\n      ),\n    link = sapply(\n      resp$items, \n      function(x) x$link\n      )\n    )\n}\n\n# EXAMPLE USAGE: get 10 \"teen\"-related questions \n# from the \"parenting\" StackExchange site\nteen_posts &lt;- get_tagged_posts(\n  \"teen\", site = \"parenting\",\n  user = \"Data Science for Psychology (ds4psych@gmail.com)\",\n  pagesize = 10\n  )\n\nFor further details on accessing APIs with httr2 (including how to deal with rate limits), see the vignette.\n\n\n\n\n\nAshokkumar, A., & Pennebaker, J. W. (2022). Tracking group identity through natural language within groups. PNAS Nexus, 1(2), pgac022. https://doi.org/10.1093/pnasnexus/pgac022\n\n\nMosleh, M., Pennycook, G., & Arechar, A. (2021). Cognitive reflection correlates with behavior on twitter. Nature Communications, 12. https://doi.org/10.1038/s41467-020-20043-0\n\n\nRosenbusch, H., Evans, A. M., & Zeelenberg, M. (2019). Multilevel emotion transfer on YouTube: Disentangling the effects of emotional contagion and homophily on video audiences. Social Psychological and Personality Science, 10(8), 1028–1035. https://doi.org/10.1177/1948550618820309\n\n\nTakhteyev, Y., Gruzd, A., & Wellman, B. (2012). Geography of twitter networks. Social Networks, 34(1), 73–81. https://doi.org/https://doi.org/10.1016/j.socnet.2011.05.006\n\n\nTufekci, Z. (2014). Big questions for social media big data: Representativeness, validity and other methodological pitfalls. Proceedings of the 8th International Conference on Weblogs and Social Media, ICWSM 2014, 8. https://doi.org/10.1609/icwsm.v8i1.14517\n\n\nXiao, L., & Mensah, H. (2022). How does the thread level of a comment affect its perceived persuasiveness? A reddit study. In K. Arai (Ed.), Intelligent computing (pp. 800–813). Springer International Publishing.\n\n\nYadav, A., Phillips, M., Lundeberg, M., Koehler, M., Hilden, K., & Dirkin, K. (2011). If a picture is worth a thousand words is video worth a million? Differences in affective and cognitive processing of video and text cases. J. Computing in Higher Education, 23, 15–37. https://doi.org/10.1007/s12528-011-9042-y"
  },
  {
    "objectID": "scraping.html#be-polite",
    "href": "scraping.html#be-polite",
    "title": "10  Web Scraping",
    "section": "\n10.1 Be Polite",
    "text": "10.1 Be Polite\nOften, scraping requires the code to visit many web pages. For example, say we wanted a list of poems by English poets, along with the birth dates of their authors. We might have the computer scrape the list of English poets from the Poetry Foundation, which requires clicking through 17 pages of search results, and retrieve the author’s birth date and the URL for each author’s full list of poems. We could then have the computer visit each of those URLs and retrieve the URL for each individual poem by each author. Finally, the computer would visit each poem URL and retrieve the text of the poem. In all, this process might require visiting hundreds or even thousands of web pages.\nFor the same reasons that APIs generally have rate limits (Chapter 9), web scraping algorithms should be polite—not, for example, requesting thousands of pages in the space of a few seconds. If you are not polite, you are likely to get banned as a bot.\nTo read more about web scraping etiquette and easy ways to implement it, see the relevant chapter in R for Data Science, and the homepage of the polite package."
  },
  {
    "objectID": "scraping.html#a-simple-example",
    "href": "scraping.html#a-simple-example",
    "title": "10  Web Scraping",
    "section": "\n10.2 A Simple Example",
    "text": "10.2 A Simple Example\nTo give you a taste for what web scraping is like, we will give a simple example of scraping a single page.\nThe page in this case will be the blog reel of one the authors of this book, Louis.\n\nWe will scrape the name and date of each blog post using the rvest package. First, we retrieve the raw html code of the webpage.\n\nlibrary(rvest)\n\nhtml &lt;- read_html(\"https://rimonim.github.io/blog.html\")\n\nThen, we have to identify the name of the particular objects we are looking for. In most cases, this can be done by right clicking and pressing “Inspect Element” in the web browser.1\n\nNow that we know that blog titles are called “h3.no-anchor.listing-title”, we can extract those objects from the raw html using html_elements() and convert them to regular text using html_text2().\n\npost_titles &lt;- html |&gt; \n  html_elements(\"h3.no-anchor.listing-title\") |&gt; \n  html_text2()\n\nhead(post_titles)\n\n#&gt; [1] \"Americans Have Eight Kinds of Days\"                                               \n#&gt; [2] \"Advanced Machine Learning Approaches for Detecting Trolls on Twitter\"             \n#&gt; [3] \"Shockingly, Most Reddit Environmentalists are not Greta Thunberg\"                 \n#&gt; [4] \"Comparing Four Methods of Sentiment Analysis\"                                     \n#&gt; [5] \"Do Employees Tend to Have the Same First Name as Their Bosses?\"                   \n#&gt; [6] \"Designing a Poster to Visualize the Timeline of Philosophers in the Islamic World\"\n\n\nTo build a dataset with multiple variables, we repeat this process for each variable.\n\nposts &lt;- tibble(\n  date = html |&gt; \n    html_elements(\"div.listing-date\") |&gt; \n    html_text2(),\n  title = post_titles\n)\n\nhead(posts)\n\n#&gt; # A tibble: 6 × 2\n#&gt;   date         title                                                            \n#&gt;   &lt;chr&gt;        &lt;chr&gt;                                                            \n#&gt; 1 Aug 23, 2023 Americans Have Eight Kinds of Days                               \n#&gt; 2 Jul 20, 2023 Advanced Machine Learning Approaches for Detecting Trolls on Twi…\n#&gt; 3 Jul 12, 2023 Shockingly, Most Reddit Environmentalists are not Greta Thunberg \n#&gt; 4 Jul 10, 2023 Comparing Four Methods of Sentiment Analysis                     \n#&gt; 5 Jun 28, 2023 Do Employees Tend to Have the Same First Name as Their Bosses?   \n#&gt; 6 May 8, 2023  Designing a Poster to Visualize the Timeline of Philosophers in …\n\n\nThis has been a very basic example. For a more in-depth tutorial on web scraping (including more complex examples), see the relevant chapter in R for Data Science."
  },
  {
    "objectID": "scraping.html#footnotes",
    "href": "scraping.html#footnotes",
    "title": "10  Web Scraping",
    "section": "",
    "text": "Using “Inspect Element” on more complicated web pages can be difficult. For finding element names more easily, we recommend SelectorGadget↩︎"
  },
  {
    "objectID": "quantification.html",
    "href": "quantification.html",
    "title": "Quantifying Psychological Properties of Text",
    "section": "",
    "text": "The average native English speaker has a functionally useful vocabulary size of over 20,000 words (Hellman, 2011). 20,000 words means 20,000 variables for psychologists to analyze. But that’s not all—words can be arranged in infinite orders, drastically changing their meanings in context. How do we capture all of this complexity in simple measurements that we can use for analysis?\nIn this unit, we introduce methods for quantifying psychological properties of text. We begin with fundamental skills for language processing in R, and move gradually to more advanced methods. In the final sections, we cover recent developments in the use of large language models for psychological research.\nThroughout the section, we will provide example analyses using the Hippocorpus dataset, which contains the data from Sap et al. (2020) and Sap et al. (2022). The dataset consists primarily of texts written by online participants, who were instructed to write either true stories that happened to them recently or fictional stories about the same topic. Participants then retold the true stories several months later. The dataset also includes a variety of psychological and demographic characteristics of the participants, including gender and openness to experience. The data itself is not included in the book’s files, but you can easily download it from the website and follow along.\n\n\n\n\nHellman, A. B. (2011). Vocabulary size and depth of word knowledge in adult-onset second language acquisition. International Journal of Applied Linguistics, 21(2), 162–182.\n\n\nSap, M., Horvitz, E., Choi, Y., Smith, N. A., & Pennebaker, J. (2020). Recollection versus imagination: Exploring human memory and cognition via neural language models. In D. Jurafsky, J. Chai, N. Schluter, & J. Tetreault (Eds.), Proceedings of the 58th annual meeting of the association for computational linguistics (pp. 1970–1978). Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.acl-main.178\n\n\nSap, M., Jafarpour, A., Choi, Y., Smith, N. A., Pennebaker, J. W., & Horvitz, E. (2022). Quantifying the narrative flow of imagined versus autobiographical stories. Proceedings of the National Academy of Sciences, 119(45), e2211715119. https://doi.org/10.1073/pnas.2211715119"
  },
  {
    "objectID": "look-at-your-data.html",
    "href": "look-at-your-data.html",
    "title": "11  Look at Your Data",
    "section": "",
    "text": "Before using any theory-driven methods, it is always worthwhile to get a sense of your data more generally. This is as true for text data as it is for any other format. The process of exploring your data without a particular theory or construct in mind is called Exploratory Data Analysis (EDA).\nMany guides to EDA for text data suggest plotting histograms of text length, calculating standard metrics of valence, and worst of all, generating word clouds of global word frequency. We love computational methods and we will explore many of them in the coming chapters, but there is no denying it—the quickest, most foolproof way to explore your data is by looking at it. Just open the raw data and spend a minute or two reading through it. This is especially true for language data, since you have been training your whole life to develop an efficient, nuanced understanding of natural language. If your data are short stories, pick two or three at random and read them. If your data are Reddit comments, pick a dozen at random and read them. It is very likely that you will notice interesting patterns, or identify quirks in the data that should be filtered out before further analysis."
  },
  {
    "objectID": "quanteda.html",
    "href": "quanteda.html",
    "title": "12  Introduction to Quanteda",
    "section": "",
    "text": "For all analyses involving separating text into smaller units (e.g. words), this book will use the Quanteda family of packages. While it lacks some of the conceptual elegance of the tidyverse, Quanteda is indispensable because of its scope and its efficiency—Quanteda functions are faster to write, faster to run, and applicable to a wider variety of uses than any other text analysis framework in R. Beyond the base quanteda package, Quanteda offers a plethora of specialized extensions, for example:\n\n\nquanteda.textstats: Metrics for similarity, readability, lexical diversity, and more\n\nquanteda.dictionaries: Dictionaries for word counting applications\n\nquanteda.sentiment: Tools advanced dictionary-based sentiment analysis [see @#sec-word-counting-improvements]\n\nQuanteda is also well documented. See the quanteda tutorials webpage for details on file formats and methods not covered in this book.\n\nlibrary(quanteda)\n\nThe home-base of any Quanteda analysis is the corpus. A corpus is a static container holding a library of text documents and associated properties of those documents, called docvars. Later, when we apply complex transformations to our texts (such as splitting them into words), we will need to first store them as a corpus.\nYou can create corpora from a variety of data formats, but we will begin with a dataframe containing our text variable, story.\n\nhippocorpus_df &lt;- read_csv(\"data/hippocorpus-u20220112/hcV3-stories.csv\") |&gt; \n  select(AssignmentId, story, memType, summary, WorkerId, \n         annotatorGender, openness, timeSinceEvent)\n\n#&gt; Rows: 6854 Columns: 23\n#&gt; ── Column specification ────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr (12): AssignmentId, WorkerId, annotatorGender, annotatorRace, mainEvent,...\n#&gt; dbl (11): WorkTimeInSeconds, annotatorAge, distracted, draining, frequency, ...\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(hippocorpus_df)\n\n#&gt; # A tibble: 6 × 8\n#&gt;   AssignmentId           story memType summary WorkerId annotatorGender openness\n#&gt;   &lt;chr&gt;                  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;\n#&gt; 1 32RIADZISTQWI5XIVG5BN… Conc… imagin… My boy… E9TY34YY man                 0   \n#&gt; 2 3018Q3ZVOJCZJFDMPSFXA… The … recall… My boy… 237K2NI1 woman               1   \n#&gt; 3 3IRIK4HM3B6UQBC0HI8Q5… It s… imagin… My sis… FK5QTANB woman               0.5 \n#&gt; 4 3018Q3ZVOJCZJFDMPSFXA… Five… recall… My sis… UYOSBBRS woman               1   \n#&gt; 5 3MTMREQS4W44RBU8OMP3X… Abou… imagin… It is … 34BFLNJV man                 0.25\n#&gt; 6 3018Q3ZVOJCZJFDMPSFXA… Burn… recall… It is … L427B0E0 woman               1   \n#&gt; # ℹ 1 more variable: timeSinceEvent &lt;dbl&gt;\n\n\nTo turn this into a corpus with the corpus() constructor, specifying the text variable and a unique identifier by name. All other variables will automatically become docvars.\n\nhippocorpus_corp &lt;- corpus(hippocorpus_df, \n                           docid_field = \"AssignmentId\", \n                           text_field = \"story\")\nhippocorpus_corp\n\n#&gt; Corpus consisting of 6,854 documents and 6 docvars.\n#&gt; 32RIADZISTQWI5XIVG5BN0VMYFRS4U :\n#&gt; \"Concerts are my most favorite thing, and my boyfriend knew i...\"\n#&gt; \n#&gt; 3018Q3ZVOJCZJFDMPSFXATCQ4DARA2 :\n#&gt; \"The day started perfectly, with a great drive up to Denver f...\"\n#&gt; \n#&gt; 3IRIK4HM3B6UQBC0HI8Q5TBJZLEC61 :\n#&gt; \"It seems just like yesterday but today makes five months ago...\"\n#&gt; \n#&gt; 3018Q3ZVOJCZJFDMPSFXATCQG04RAI :\n#&gt; \"Five months ago, my niece and nephew were born.  They are my...\"\n#&gt; \n#&gt; 3MTMREQS4W44RBU8OMP3XSK8NMJAWZ :\n#&gt; \"About a month ago I went to burning man. I was having a hard...\"\n#&gt; \n#&gt; 3018Q3ZVOJCZJFDMPSFXATCQG06AR3 :\n#&gt; \"Burning Man metamorphoses was perfect. I am definitely still...\"\n#&gt; \n#&gt; [ reached max_ndoc ... 6,848 more documents ]"
  },
  {
    "objectID": "tokenization.html#tokens",
    "href": "tokenization.html#tokens",
    "title": "13  Tokenization",
    "section": "\n13.1 Tokens",
    "text": "13.1 Tokens\nThe first step in the process of turning texts into numbers is almost always tokenization. Tokenization means splitting a long text into smaller pieces, called tokens. Once we have these smaller pieces, we will be able to count them in texts and turn these counts into numeric representations of our texts. There are four main types of tokens: words, n-grams, skipgrams, and shingles.\n\n13.1.1 Words\nWords are the obvious choice for tokens. The number of possible English sentences is infinite, but the number of possible English words is much smaller—only about 20,000 (Hellman, 2011). Words are especially useful for English, since English words usually do not change their form depending on their place in a sentence, and they are nearly always separated by a space. Separating an English text into words is therefore as simple as splitting it at every space character. This is essentially the default behavior of the Quanteda tokens() function, which takes a character vector or corpus and returns a “tokens” object:\n\ntokens &lt;- tokens(\"20,000 is not so many words\")\ntokens\n\n#&gt; Tokens consisting of 1 document.\n#&gt; text1 :\n#&gt; [1] \"20,000\" \"is\"     \"not\"    \"so\"     \"many\"   \"words\"\n\n\nWe can likewise perform word-based tokenization for the entire corpus that we created in Chapter 12. To remove punctuation before doing so, we can add remove_punct = TRUE.\n\nhippocorpus_tokens &lt;- tokens(hippocorpus_corp,\n                             remove_punct = TRUE)\nhead(hippocorpus_tokens, 3)\n\n#&gt; Tokens consisting of 3 documents and 6 docvars.\n#&gt; 32RIADZISTQWI5XIVG5BN0VMYFRS4U :\n#&gt;  [1] \"Concerts\"  \"are\"       \"my\"        \"most\"      \"favorite\"  \"thing\"    \n#&gt;  [7] \"and\"       \"my\"        \"boyfriend\" \"knew\"      \"it\"        \"That's\"   \n#&gt; [ ... and 191 more ]\n#&gt; \n#&gt; 3018Q3ZVOJCZJFDMPSFXATCQ4DARA2 :\n#&gt;  [1] \"The\"       \"day\"       \"started\"   \"perfectly\" \"with\"      \"a\"        \n#&gt;  [7] \"great\"     \"drive\"     \"up\"        \"to\"        \"Denver\"    \"for\"      \n#&gt; [ ... and 171 more ]\n#&gt; \n#&gt; 3IRIK4HM3B6UQBC0HI8Q5TBJZLEC61 :\n#&gt;  [1] \"It\"        \"seems\"     \"just\"      \"like\"      \"yesterday\" \"but\"      \n#&gt;  [7] \"today\"     \"makes\"     \"five\"      \"months\"    \"ago\"       \"it\"       \n#&gt; [ ... and 254 more ]\n\n\n\n13.1.2 N-grams\nWords are great, but they do not stand alone. The sentence “20,000 is not so many words” contains the word “many”. So we can conclude that the sentence is referring to a lot of something, right? Not so fast! Word-based tokenization misses the fact the more representative unit: “not so many”. Because order creates meaning in language, it is often worthwhile to break texts into groups of a few words at a time, called n-grams. Groups of two words are called two-grams (or bigrams), groups of three words like “not so many” are called three-grams (or trigrams), and so on.\nTo get all of the two-grams and three-grams from our example text, we feed the “tokens” object we created already into the tokens_ngrams() function, and specify that we want groups of twos and threes with n = c(2L, 3L).\n\nngrams &lt;- tokens_ngrams(tokens, n = c(2L, 3L))\nngrams\n\n#&gt; Tokens consisting of 1 document.\n#&gt; text1 :\n#&gt; [1] \"20,000_is\"     \"is_not\"        \"not_so\"        \"so_many\"      \n#&gt; [5] \"many_words\"    \"20,000_is_not\" \"is_not_so\"     \"not_so_many\"  \n#&gt; [9] \"so_many_words\"\n\n\n\n13.1.3 Skipgrams\nSometimes the most important combinations of words do not appear right next to each other. For example, it might be more useful to count occurrences of “not many” than it is to count occurrences of “not so many”. We can do this by using skipgrams—n-grams of non-adjacent words. In Quanteda, we can do this again with the tokens_ngrams(), now adding the argument skip = c(0L, 1L) to specify that we want both immediately adjacent words (0L), and ones skipping one word in between (1L).\n\nskipgrams &lt;- tokens_ngrams(tokens, n = c(2L, 3L), skip = c(0L, 1L))\nskipgrams\n\n#&gt; Tokens consisting of 1 document.\n#&gt; text1 :\n#&gt;  [1] \"20,000_is\"     \"20,000_not\"    \"is_not\"        \"is_so\"        \n#&gt;  [5] \"not_so\"        \"not_many\"      \"so_many\"       \"so_words\"     \n#&gt;  [9] \"many_words\"    \"20,000_is_not\" \"20,000_is_so\"  \"20,000_not_so\"\n#&gt; [ ... and 9 more ]\n\n\n\n13.1.4 Shingles\nSometimes even individual words are too coarse. For example, the fact that our sentence contains “20,000” might be too specific to be informative, but it might be useful to know that the sentence contains “,000”, which more generally signifies big, round numbers. To get tokens like this, we need to start back at the beginning, this time separating our text into individual characters.\n\nchar_tokens &lt;- tokens(\"20,000 is not so many words\", \n                      what = \"character\")\nchar_tokens\n\n#&gt; Tokens consisting of 1 document.\n#&gt; text1 :\n#&gt;  [1] \"2\" \"0\" \",\" \"0\" \"0\" \"0\" \"i\" \"s\" \"n\" \"o\" \"t\" \"s\"\n#&gt; [ ... and 10 more ]\n\n\nNow we can create groupings of letters, called “shingles” in the same way that we made n-grams, with the tokens_ngrams() function.\n\nshingles &lt;- tokens_ngrams(char_tokens, n = c(4L))\nshingles\n\n#&gt; Tokens consisting of 1 document.\n#&gt; text1 :\n#&gt;  [1] \"2_0_,_0\" \"0_,_0_0\" \",_0_0_0\" \"0_0_0_i\" \"0_0_i_s\" \"0_i_s_n\" \"i_s_n_o\"\n#&gt;  [8] \"s_n_o_t\" \"n_o_t_s\" \"o_t_s_o\" \"t_s_o_m\" \"s_o_m_a\"\n#&gt; [ ... and 7 more ]"
  },
  {
    "objectID": "tokenization.html#custom-preprocessing",
    "href": "tokenization.html#custom-preprocessing",
    "title": "13  Tokenization",
    "section": "\n13.2 Custom Preprocessing",
    "text": "13.2 Custom Preprocessing\nWe have seen above that the tokens() function includes some basic preprocessing capabilities, for example remove_punct to remove punctuation. It also offers remove_numbers, remove_symbols (e.g. ▵, ⏱, ↤, $, ÷, ∬), and remove_url to remove URLs. Nevertheless, certain texts require more specific preprocessing steps. This can be achieved with the stringr package from the tidyverse, by modifying the raw text before turning it into a corpus object. For example, when working with texts from social media we might want to remove “RT” (a conventional indicator that the text was written by another person) with text = str_replace(text, \"RT \", \"\") or user handles (used to refer to specific users) with text = str_replace(text, '@\\\\w+:|@\\\\w+', \" \"), since neither of these are tokens that are relevant to the psychological quality of the text. In certain cases, we might be interested in counting user handles without regard to the specific user. In these cases, we might use text = str_replace(text, '@\\\\w+:|@\\\\w+', \"USER_TAG \") so that all user tags get tokenized as “USER_TAG”."
  },
  {
    "objectID": "tokenization.html#sec-quanteda-dfms",
    "href": "tokenization.html#sec-quanteda-dfms",
    "title": "13  Tokenization",
    "section": "\n13.3 Document-Feature Matrices (DFMs)",
    "text": "13.3 Document-Feature Matrices (DFMs)\nOnce we have a suitable tokens object, we can count how many times each token appears in each text. This will provide a basis for comparing those texts later on. Thinking about a text this way—as a collection of tokens with different frequencies—is often referred to as the bag of words approach. When using the bag of words approach, we ignore the order of the words. We imagine that each topic has its characteristic bag of words, and speaking or writing is just a matter of pulling them out of the bag one at a time at random. This assumption is obviously wrong, but it makes it possible to analyze text with straightforward quantitative methods. Often, this is a worthwhile trade off to make. As we advance to more and more complex methods in the coming chapters, we will find that many methods were designed to solve problems created by the overly simplistic bag of words approach.\nFor now though, let’s start with the basics. The way to represent the counts of each token (AKA “feature”) in each document is with a document-feature matrix (DFM). This is a table where each row is a document (i.e. a text in our corpus) and each column is a feature (usually a token). The cells can then contain counts of how many times a particular feature appears in a particular document.\nCreating DFMs in Quanteda is done with the dfm() function. By default, dfm() will also convert all text to lower case.\n\nhippocorpus_dfm &lt;- dfm(hippocorpus_tokens)\nhead(hippocorpus_dfm, 3)\n\n#&gt; Document-feature matrix of: 3 documents, 27,906 features (99.58% sparse) and 6 docvars.\n#&gt;                                 features\n#&gt; docs                             concerts are my most favorite thing and\n#&gt;   32RIADZISTQWI5XIVG5BN0VMYFRS4U        1   1  5    1        2     1  11\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQ4DARA2        0   0  5    0        0     0  10\n#&gt;   3IRIK4HM3B6UQBC0HI8Q5TBJZLEC61        0   0 10    3        0     1   6\n#&gt;                                 features\n#&gt; docs                             boyfriend knew it\n#&gt;   32RIADZISTQWI5XIVG5BN0VMYFRS4U         2    3  3\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQ4DARA2         1    0  2\n#&gt;   3IRIK4HM3B6UQBC0HI8Q5TBJZLEC61         0    0  6\n#&gt; [ reached max_nfeat ... 27,896 more features ]\n\n\nCongratulations! You have turned texts into numbers! Now how do we use these numbers to measure psychological constructs?\n\n\n\n\n\nHellman, A. B. (2011). Vocabulary size and depth of word knowledge in adult-onset second language acquisition. International Journal of Applied Linguistics, 21(2), 162–182."
  },
  {
    "objectID": "word-counting.html#sec-quanteda-dictionaries",
    "href": "word-counting.html#sec-quanteda-dictionaries",
    "title": "14  Dictionary-Based Word Counts",
    "section": "\n14.1 Dictionaries",
    "text": "14.1 Dictionaries\nRecall that in the Hippocorpus data, the memType variable indicates whether the participant was told to tell a story that happened to them recently (“recalled”), a story that they had already told a few months earlier (“retold”), or an entirely fictional story (“imagined”).\nSap et al. (2022) hypothesized that true autobiographical stories would include more surprising events than imagined stories. To test this hypothesis, we could use a dictionary of surprise-related words. We could try making one up:\n\nsurprise_dict &lt;- dictionary(\n    list(\n      surprise = c(\"surprise\", \"wow\", \"suddenly\", \"bang\")\n    )\n  )\nsurprise_dict\n\n#&gt; Dictionary object with 1 key entry.\n#&gt; - [surprise]:\n#&gt;   - surprise, wow, suddenly, bang\n\n\nMaking up a sentiment dictionary is not easy. Luckily, other researchers have done the work for us: The NRC Word-Emotion Association Lexicon (S. M. Mohammad & Turney, 2013; S. Mohammad & Turney, 2010), included in the quanteda.sentiment package, has a list of 534 surprise words.\n\nsurprise_dict &lt;- quanteda.sentiment::data_dictionary_NRC[\"surprise\"]\nsurprise_dict\n\n#&gt; Dictionary object with 1 key entry.\n#&gt; Polarities: pos = \"positive\"; neg = \"negative\" \n#&gt; - [surprise]:\n#&gt;   - abandonment, abduction, abrupt, accident, accidental, accidentally, accolade, advance, affront, aghast, alarm, alarming, alertness, alerts, allure, amaze, amazingly, ambush, angel, anomaly [ ... and 514 more ]\n\n\nS. M. Mohammad & Turney (2013) generated this dictionary by asking thousands of online participants to rate how much various words are associated with the emotion surprise. The final dictionary includes all the words that were consistently reported to be at least moderately associated with surprise.\nTo count how many times surprise words appear in each of our texts, we use the dfm_lookup() function.\n\nhippocorpus_surprise &lt;- hippocorpus_dfm |&gt; \n  dfm_lookup(surprise_dict)\nhippocorpus_surprise\n\n#&gt; Document-feature matrix of: 6,854 documents, 1 feature (5.09% sparse) and 6 docvars.\n#&gt;                                 features\n#&gt; docs                             surprise\n#&gt;   32RIADZISTQWI5XIVG5BN0VMYFRS4U        2\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQ4DARA2        0\n#&gt;   3IRIK4HM3B6UQBC0HI8Q5TBJZLEC61        4\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG04RAI        3\n#&gt;   3MTMREQS4W44RBU8OMP3XSK8NMJAWZ        4\n#&gt;   3018Q3ZVOJCZJFDMPSFXATCQG06AR3        6\n#&gt; [ reached max_ndoc ... 6,848 more documents ]"
  },
  {
    "objectID": "word-counting.html#sec-modeling-word-counts",
    "href": "word-counting.html#sec-modeling-word-counts",
    "title": "14  Dictionary-Based Word Counts",
    "section": "\n14.2 Modeling Word Counts",
    "text": "14.2 Modeling Word Counts\nRecall that we wanted to test whether true autobiographical stories include more surprise than imagined stories. Now that we have counted the number of surprise words in each document, how do we test our hypothesis?\nA good first step is to reattach the word counts to our original corpus. As we do this, we convert both to dataframes.\n\nhippocorpus_surprise_df &lt;- hippocorpus_surprise |&gt; \n  convert(\"data.frame\") |&gt; # convert to dataframe\n  right_join(\n    hippocorpus_corp |&gt; \n      convert(\"data.frame\") # convert to dataframe\n    )\n\nGenerally we want to control for the total number of words in each text, since longer texts have more opportunities to use surprise words. To count the total number of tokens in each text, we can use the ntoken() function on our DFM and add the result directly to the new dataframe.\n\nhippocorpus_surprise_df &lt;- hippocorpus_surprise_df |&gt; \n  mutate(wc = ntoken(hippocorpus_dfm))\n\nWe are now ready for modeling! When your dependent variable is a count of words, we recommend using negative binomial regression, available in R with the MASS package. For extra sensitivity to the variable rates at which word frequencies grow with text length (see Baayen, 2001), we will allow wc to have its own parameter in the regression.\n\nsurprise_mod &lt;- MASS::glm.nb(surprise ~ memType + wc,\n                             data = hippocorpus_surprise_df)\nsummary(surprise_mod)\n\n#&gt; \n#&gt; Call:\n#&gt; MASS::glm.nb(formula = surprise ~ memType + wc, data = hippocorpus_surprise_df, \n#&gt;     init.theta = 5.991091414, link = log)\n#&gt; \n#&gt; Coefficients:\n#&gt;                   Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)      6.806e-01  2.536e-02  26.835   &lt;2e-16 ***\n#&gt; memTyperecalled -1.722e-02  1.772e-02  -0.972   0.3310    \n#&gt; memTyperetold   -4.201e-02  2.200e-02  -1.910   0.0562 .  \n#&gt; wc               2.628e-03  8.583e-05  30.616   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for Negative Binomial(5.9911) family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 8305.6  on 6853  degrees of freedom\n#&gt; Residual deviance: 7367.3  on 6850  degrees of freedom\n#&gt; AIC: 31023\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 1\n#&gt; \n#&gt; \n#&gt;               Theta:  5.991 \n#&gt;           Std. Err.:  0.264 \n#&gt; \n#&gt;  2 x log-likelihood:  -31013.345\n\n\nLooking at the p-values for the coefficients, we see that there was no significant difference between recalled and imagined stories (p = 0.331). There was, however, a marginal difference between retold and imagined stories, such that retold stories used fewer surprise words (p = 0.056).\nWhen interpreting these results, it is important to think about what exactly we are measuring. The surprise dictionary we are using was generated be asking participants how much each word was “associated with the emotion surprise” (S. M. Mohammad & Turney, 2013). A word can be “associated with” surprise because it reflects surprise (e.g. “suddenly”), but it can also be “associated with” surprise because it reflects the exact opposite of surprise. Indeed, if we look through the dictionary, we find words like “leisure” and “lovely”.\n\nset.seed(8)\nsample(surprise_dict$surprise, 20)\n\n#&gt;  [1] \"outburst\"    \"godsend\"     \"alarming\"    \"intense\"     \"lawsuit\"    \n#&gt;  [6] \"leisure\"     \"scrimmage\"   \"curiosity\"   \"reappear\"    \"placard\"    \n#&gt; [11] \"diversion\"   \"receiving\"   \"thirst\"      \"lovely\"      \"frenetic\"   \n#&gt; [16] \"perfection\"  \"playground\"  \"fearfully\"   \"guess\"       \"unfulfilled\"\n\n\nThis means that we are not, in fact, measuring how surprising each story is. At best, we are measuring how much each story deals with surprise (or lack thereof) one way or another. This observation is just one example of a larger problem with dictionary-based word counts in general, especially when the dictionary is generated by asking participants about one construct at a time: the words in a dictionary do not uniquely reflect the construct of interest—they also reflect other conceptually related concepts. Opposites, of course, are always closely conceptually related to each other. In the next chapter, we will explore ways to improve dictionary-based counts so that they can overcome this and other problems.\n\n\n\n\n\n\n\nAdvantages of Dictionary-Based Word Counts\n\n\n\n\n\nEfficient Processing: Counting is a simple operation for computers. For very large datasets, more this can make a big difference.\n\nEasy to Interpret: Dictionaries for sentiment analysis are usually not more than a few hundred words long. This means that they are easy to read through and understand intuitively. The intuitive appeal is also good for explaining your research to others—“we counted the number of anger-related words” is a method that any non-expert can understand.\n\n\n\n\n\n\n\n\n\nDisadvantages of Dictionary-Based Word Counts\n\n\n\n\n\nNo Context: Dictionary-based word counts treat texts as bags of words. This means they entirely ignore word order (aside from the order of any n-grams that might be included in the dictionary).\n\nMay Reflect Various Constructs: Dictionaries are often generated by asking participants to identify associations with words. These associations do not necessarily reflect the construct in which the researcher is interested.\n\nUnnuanced: Words are either in a dictionary or they are not. Raw counts carry no nuance about the varying degrees to which different words may reflect the construct of interest.\n\nLimited Dictionaries Available: Dictionaries are expensive and labor intensive to produce. Researchers are generally reliant on dictionaries already produced by other teams, which may not reflect the construct of interest precisely.\n\n\n\n\n\n\n\nBaayen, R. H. (2001). Word frequency distributions. Springer Netherlands. https://link.springer.com/book/10.1007/978-94-010-0844-0\n\n\nMohammad, S. M., & Turney, P. D. (2013). Crowdsourcing a word-emotion association lexicon. Computational Intelligence, 29(3), 436–465.\n\n\nMohammad, S., & Turney, P. (2010). Emotions evoked by common words and phrases: Using Mechanical Turk to create an emotion lexicon. Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, 26–34. https://aclanthology.org/W10-0204\n\n\nSap, M., Jafarpour, A., Choi, Y., Smith, N. A., Pennebaker, J. W., & Horvitz, E. (2022). Quantifying the narrative flow of imagined versus autobiographical stories. Proceedings of the National Academy of Sciences, 119(45), e2211715119. https://doi.org/10.1073/pnas.2211715119"
  },
  {
    "objectID": "word-counting-improvements.html#polarity",
    "href": "word-counting-improvements.html#polarity",
    "title": "15  Improvements on Word Counts",
    "section": "\n15.1 Polarity",
    "text": "15.1 Polarity\nSee https://github.com/trinker/sentimentr\nPlutchik (1962) argues that the opposite of surprise is anticipation\n\nquanteda.sentiment::data_dictionary_NRC$anticipation\n\n  [1] \"abundance\"      \"accelerate\"     \"accolade\"       \"accompaniment\" \n  [5] \"achievement\"    \"acquiring\"      \"addresses\"      \"adore\"         \n  [9] \"adrift\"         \"advance\"        \"advent\"         \"adventure\"     \n [13] \"advocacy\"       \"airport\"        \"alertness\"      \"alerts\"        \n [17] \"alive\"          \"allure\"         \"aloha\"          \"ambition\"      \n [21] \"amour\"          \"analyst\"        \"angel\"          \"angling\"       \n [25] \"announcement\"   \"anticipation\"   \"anticipatory\"   \"antidote\"      \n [29] \"anxiety\"        \"anxious\"        \"appeal\"         \"applicant\"     \n [33] \"apprehensive\"   \"approaching\"    \"arbitration\"    \"archaeology\"   \n [37] \"ardent\"         \"arouse\"         \"arrival\"        \"arrive\"        \n [41] \"art\"            \"aspiration\"     \"aspire\"         \"aspiring\"      \n [45] \"astrologer\"     \"astronomer\"     \"atone\"          \"attainable\"    \n [49] \"attempt\"        \"attendance\"     \"auction\"        \"audience\"      \n [53] \"auspicious\"     \"await\"          \"award\"          \"ballot\"        \n [57] \"balm\"           \"banger\"         \"banquet\"        \"basketball\"    \n [61] \"beaming\"        \"begun\"          \"betrothed\"      \"beware\"        \n [65] \"biennial\"       \"birth\"          \"birthday\"       \"bless\"         \n [69] \"blessing\"       \"blessings\"      \"blindfold\"      \"bloom\"         \n [73] \"board\"          \"boisterous\"     \"bonus\"          \"boomerang\"     \n [77] \"bountiful\"      \"bounty\"         \"bridal\"         \"bride\"         \n [81] \"bridegroom\"     \"brilliant\"      \"broadside\"      \"brotherly\"     \n [85] \"bruise\"         \"buddy\"          \"bugle\"          \"buzz\"          \n [89] \"bye\"            \"calculation\"    \"calls\"          \"candid\"        \n [93] \"cap\"            \"captivate\"      \"career\"         \"cash\"          \n [97] \"cataract\"       \"caution\"        \"cautious\"       \"celebrated\"    \n[101] \"celebrating\"    \"celebration\"    \"celebrity\"      \"celestial\"     \n[105] \"cement\"         \"champion\"       \"changeable\"     \"chant\"         \n[109] \"charitable\"     \"chastity\"       \"cheer\"          \"cheerfulness\"  \n[113] \"cheery\"         \"cherish\"        \"chicane\"        \"child\"         \n[117] \"chocolate\"      \"chuckle\"        \"church\"         \"clamor\"        \n[121] \"clap\"           \"climax\"         \"clock\"          \"closure\"       \n[125] \"clown\"          \"clue\"           \"comfort\"        \"coming\"        \n[129] \"commemorate\"    \"commemoration\"  \"commemorative\"  \"commonplace\"   \n[133] \"compensate\"     \"competition\"    \"complement\"     \"completing\"    \n[137] \"completion\"     \"compliment\"     \"concealed\"      \"concealment\"   \n[141] \"condemnation\"   \"confession\"     \"conjecture\"     \"conjure\"       \n[145] \"consecration\"   \"consequent\"     \"conservation\"   \"conspirator\"   \n[149] \"contagion\"      \"contingent\"     \"continuation\"   \"continue\"      \n[153] \"convergence\"    \"convince\"       \"correspondence\" \"council\"       \n[157] \"countdown\"      \"court\"          \"courtship\"      \"cove\"          \n[161] \"cradle\"         \"cramp\"          \"craps\"          \"crave\"         \n[165] \"craving\"        \"cream\"          \"creeping\"       \"crescendo\"     \n[169] \"crowning\"       \"cue\"            \"cultivate\"      \"curiosity\"     \n[173] \"daily\"          \"dare\"           \"dawn\"           \"deal\"          \n[177] \"death\"          \"debenture\"      \"defense\"        \"delight\"       \n[181] \"delighted\"      \"delightful\"     \"deliverance\"    \"delivery\"      \n[185] \"denying\"        \"depart\"         \"depend\"         \"depraved\"      \n[189] \"deserve\"        \"destination\"    \"destined\"       \"detainee\"      \n[193] \"determinate\"    \"develop\"        \"devil\"          \"devout\"        \n[197] \"diagnosis\"      \"dictatorship\"   \"dietary\"        \"digress\"       \n[201] \"diplomacy\"      \"discreet\"       \"discretion\"     \"dismay\"        \n[205] \"disposed\"       \"distracting\"    \"divination\"     \"doomsday\"      \n[209] \"dove\"           \"draft\"          \"dread\"          \"dreadful\"      \n[213] \"duel\"           \"dutiful\"        \"eager\"          \"eagerness\"     \n[217] \"ecstasy\"        \"ecstatic\"       \"edification\"    \"edition\"       \n[221] \"efficient\"      \"ejaculation\"    \"elegance\"       \"elevation\"     \n[225] \"elite\"          \"emancipation\"   \"embrace\"        \"enchant\"       \n[229] \"enchanting\"     \"endanger\"       \"endeavor\"       \"engaged\"       \n[233] \"engulf\"         \"enjoy\"          \"enjoying\"       \"enroll\"        \n[237] \"entertaining\"   \"entertainment\"  \"enthusiasm\"     \"enthusiast\"    \n[241] \"epidemic\"       \"erotic\"         \"errand\"         \"escape\"        \n[245] \"eventual\"       \"eventuality\"    \"exalt\"          \"excavation\"    \n[249] \"exceed\"         \"excel\"          \"excitation\"     \"excite\"        \n[253] \"excited\"        \"excitement\"     \"exciting\"       \"exhaustion\"    \n[257] \"exigent\"        \"expect\"         \"expectancy\"     \"expectant\"     \n[261] \"expectation\"    \"expected\"       \"expecting\"      \"expedition\"    \n[265] \"experiment\"     \"explore\"        \"explosive\"      \"expose\"        \n[269] \"extricate\"      \"failing\"        \"fain\"           \"faith\"         \n[273] \"familiarity\"    \"fancy\"          \"fanfare\"        \"farm\"          \n[277] \"fate\"           \"favorable\"      \"feat\"           \"feeling\"       \n[281] \"ferment\"        \"fermentation\"   \"festival\"       \"fete\"          \n[285] \"fiesta\"         \"finally\"        \"firstborn\"      \"fitting\"       \n[289] \"flinch\"         \"flirt\"          \"football\"       \"forearm\"       \n[293] \"foreboding\"     \"forecast\"       \"foresee\"        \"foreseen\"      \n[297] \"foresight\"      \"forewarned\"     \"forming\"        \"fortune\"       \n[301] \"frantic\"        \"friendly\"       \"frisky\"         \"fulfillment\"   \n[305] \"fun\"            \"gain\"           \"gambler\"        \"gambling\"      \n[309] \"generosity\"     \"germ\"           \"germination\"    \"gift\"          \n[313] \"glad\"           \"glide\"          \"glimmer\"        \"glorify\"       \n[317] \"glory\"          \"glow\"           \"god\"            \"good\"          \n[321] \"goodness\"       \"gradual\"        \"graduation\"     \"grandchildren\" \n[325] \"grant\"          \"grasping\"       \"gravitate\"      \"grim\"          \n[329] \"grin\"           \"grow\"           \"guillotine\"     \"habitual\"      \n[333] \"hankering\"      \"hap\"            \"happen\"         \"happiness\"     \n[337] \"happy\"          \"harbinger\"      \"harvest\"        \"haste\"         \n[341] \"headlight\"      \"healing\"        \"heavenly\"       \"heft\"          \n[345] \"hero\"           \"heroism\"        \"heyday\"         \"highest\"       \n[349] \"hire\"           \"holiday\"        \"holiness\"       \"homeless\"      \n[353] \"honeymoon\"      \"hope\"           \"hopeful\"        \"horizon\"       \n[357] \"horoscope\"      \"humanitarian\"   \"hungry\"         \"hunter\"        \n[361] \"hunting\"        \"hurried\"        \"hurry\"          \"hymn\"          \n[365] \"hype\"           \"hypothesis\"     \"illuminate\"     \"immature\"      \n[369] \"immaturity\"     \"immediately\"    \"immerse\"        \"imminent\"      \n[373] \"immortality\"    \"impatient\"      \"impending\"      \"importance\"    \n[377] \"improve\"        \"improvise\"      \"inaugural\"      \"inauguration\"  \n[381] \"incite\"         \"income\"         \"independence\"   \"infant\"        \n[385] \"infanticide\"    \"infinity\"       \"inheritance\"    \"inoculation\"   \n[389] \"inquiry\"        \"inspiration\"    \"inspire\"        \"install\"       \n[393] \"instructions\"   \"instructor\"     \"intended\"       \"interim\"       \n[397] \"interminable\"   \"intermission\"   \"intimate\"       \"intimately\"    \n[401] \"intrigue\"       \"intuitively\"    \"investigation\"  \"invitation\"    \n[405] \"invite\"         \"inviting\"       \"invocation\"     \"invoke\"        \n[409] \"jackpot\"        \"jeopardy\"       \"journey\"        \"judicial\"      \n[413] \"judiciary\"      \"kindred\"        \"kiss\"           \"labor\"         \n[417] \"labyrinth\"      \"lagging\"        \"latent\"         \"laughter\"      \n[421] \"launch\"         \"leisure\"        \"lessen\"         \"lesson\"        \n[425] \"letter\"         \"lettered\"       \"liberate\"       \"liberation\"    \n[429] \"liberty\"        \"linger\"         \"litigate\"       \"localize\"      \n[433] \"long\"           \"longing\"        \"loom\"           \"lottery\"       \n[437] \"lovely\"         \"lover\"          \"luck\"           \"lull\"          \n[441] \"luscious\"       \"lust\"           \"magical\"        \"magnificence\"  \n[445] \"magnificent\"    \"mail\"           \"majestic\"       \"marriage\"      \n[449] \"marry\"          \"matchmaker\"     \"maternal\"       \"matrimony\"     \n[453] \"medal\"          \"mediate\"        \"mediator\"       \"medical\"       \n[457] \"meditate\"       \"merge\"          \"midwife\"        \"mill\"          \n[461] \"miracle\"        \"mobile\"         \"momentum\"       \"monetary\"      \n[465] \"money\"          \"morals\"         \"morn\"           \"morrow\"        \n[469] \"mortification\"  \"mother\"         \"motion\"         \"mountain\"      \n[473] \"musical\"        \"mutable\"        \"mysterious\"     \"mystery\"       \n[477] \"nascent\"        \"navigable\"      \"navigator\"      \"neighbor\"      \n[481] \"neighborhood\"   \"nervous\"        \"network\"        \"neutral\"       \n[485] \"nobility\"       \"noncompliance\"  \"notification\"   \"nurture\"       \n[489] \"oasis\"          \"objective\"      \"obliging\"       \"offset\"        \n[493] \"olfactory\"      \"omen\"           \"ominous\"        \"ongoing\"       \n[497] \"onset\"          \"opera\"          \"opponent\"       \"opportunity\"   \n[501] \"optimism\"       \"oracle\"         \"ordination\"     \"organ\"         \n[505] \"organization\"   \"orgasm\"         \"outdo\"          \"overdue\"       \n[509] \"overthrow\"      \"overture\"       \"owing\"          \"paddle\"        \n[513] \"parade\"         \"paragon\"        \"paralysis\"      \"pare\"          \n[517] \"parole\"         \"passenger\"      \"passion\"        \"passionate\"    \n[521] \"patience\"       \"patient\"        \"pay\"            \"peace\"         \n[525] \"peaceful\"       \"perfect\"        \"perfection\"     \"peril\"         \n[529] \"perilous\"       \"perpetuate\"     \"perpetuity\"     \"picket\"        \n[533] \"picnic\"         \"plan\"           \"planning\"       \"playground\"    \n[537] \"plea\"           \"pleasant\"       \"plight\"         \"plump\"         \n[541] \"poke\"           \"possess\"        \"possibility\"    \"powerful\"      \n[545] \"practise\"       \"praiseworthy\"   \"pray\"           \"precarious\"    \n[549] \"precious\"       \"precursor\"      \"predict\"        \"prediction\"    \n[553] \"predilection\"   \"predispose\"     \"preliminary\"    \"preparation\"   \n[557] \"preparatory\"    \"prepare\"        \"prepared\"       \"preparedness\"  \n[561] \"prerequisite\"   \"prescient\"      \"present\"        \"preservative\"  \n[565] \"pretty\"         \"prevail\"        \"prevention\"     \"priesthood\"    \n[569] \"princely\"       \"probability\"    \"probation\"      \"proceeding\"    \n[573] \"production\"     \"proficiency\"    \"prognosis\"      \"prognostic\"    \n[577] \"progress\"       \"progression\"    \"prologue\"       \"prophecy\"      \n[581] \"prophet\"        \"prophetic\"      \"prophylactic\"   \"prospectively\" \n[585] \"prosper\"        \"proud\"          \"providing\"      \"pry\"           \n[589] \"public\"         \"punctual\"       \"punished\"       \"punt\"          \n[593] \"puppy\"          \"quest\"          \"quicken\"        \"quote\"         \n[597] \"rabid\"          \"radiance\"       \"raffle\"         \"rail\"          \n[601] \"ram\"            \"rapture\"        \"raving\"         \"readiness\"     \n[605] \"ready\"          \"receiving\"      \"recipient\"      \"recognizable\"  \n[609] \"recombination\"  \"reconciliation\" \"reconstruct\"    \"reconstruction\"\n[613] \"recreation\"     \"recreational\"   \"recurrent\"      \"regatta\"       \n[617] \"regularity\"     \"rehabilitation\" \"rejoice\"        \"rejoicing\"     \n[621] \"rekindle\"       \"remedy\"         \"renovate\"       \"renovation\"    \n[625] \"repay\"          \"representing\"   \"rescue\"         \"respect\"       \n[629] \"responsive\"     \"restlessness\"   \"restorative\"    \"result\"        \n[633] \"resultant\"      \"retirement\"     \"reunion\"        \"revenge\"       \n[637] \"revere\"         \"revival\"        \"revive\"         \"revolution\"    \n[641] \"reward\"         \"ribbon\"         \"ripen\"          \"rising\"        \n[645] \"risk\"           \"risky\"          \"romance\"        \"romantic\"      \n[649] \"roulette\"       \"saint\"          \"saintly\"        \"salary\"        \n[653] \"saliva\"         \"salvation\"      \"sanctify\"       \"sanctuary\"     \n[657] \"savor\"          \"scare\"          \"scientist\"      \"score\"         \n[661] \"scrutinize\"     \"secular\"        \"seductive\"      \"seek\"          \n[665] \"sensual\"        \"sensuality\"     \"sentence\"       \"sequel\"        \n[669] \"serenity\"       \"serial\"         \"sex\"            \"shackle\"       \n[673] \"shaky\"          \"share\"          \"sharpen\"        \"shining\"       \n[677] \"ship\"           \"shiver\"         \"shopping\"       \"shortly\"       \n[681] \"signify\"        \"simmer\"         \"simmering\"      \"simplify\"      \n[685] \"sing\"           \"skewed\"         \"sneaking\"       \"sonar\"         \n[689] \"sorcery\"        \"soundness\"      \"spa\"            \"sparkle\"       \n[693] \"spear\"          \"spectacular\"    \"speculative\"    \"spirits\"       \n[697] \"splendor\"       \"star\"           \"starry\"         \"start\"         \n[701] \"stealthy\"       \"sterling\"       \"store\"          \"strategist\"    \n[705] \"stripped\"       \"strive\"         \"submit\"         \"subscribe\"     \n[709] \"succeed\"        \"succeeding\"     \"success\"        \"successful\"    \n[713] \"sun\"            \"sundial\"        \"sunny\"          \"sunset\"        \n[717] \"superstitious\"  \"supremacy\"      \"surprisingly\"   \"surround\"      \n[721] \"suspense\"       \"suspicious\"     \"sweet\"          \"sweetheart\"    \n[725] \"sweets\"         \"swim\"           \"symphony\"       \"synchronize\"   \n[729] \"tabulate\"       \"tantalizing\"    \"tease\"          \"tempest\"       \n[733] \"testament\"      \"theology\"       \"theory\"         \"thermocouple\"  \n[737] \"thirst\"         \"thought\"        \"threaten\"       \"thrill\"        \n[741] \"thrilling\"      \"thriving\"       \"tickle\"         \"time\"          \n[745] \"timidity\"       \"tomorrow\"       \"top\"            \"torture\"       \n[749] \"towering\"       \"track\"          \"transcendence\"  \"transitional\"  \n[753] \"treadmill\"      \"treasure\"       \"treat\"          \"tree\"          \n[757] \"tremor\"         \"trepidation\"    \"tribunal\"       \"tributary\"     \n[761] \"triumph\"        \"triumphant\"     \"trophy\"         \"twinkle\"       \n[765] \"ultimate\"       \"ultimately\"     \"unaccountable\"  \"unbeaten\"      \n[769] \"unbridled\"      \"uncontrollable\" \"undecided\"      \"undertaking\"   \n[773] \"undisclosed\"    \"undoubted\"      \"undying\"        \"uneasiness\"    \n[777] \"unexpected\"     \"unexplained\"    \"unfold\"         \"unfulfilled\"   \n[781] \"unification\"    \"university\"     \"unknown\"        \"unpublished\"   \n[785] \"unresolved\"     \"unsurpassed\"    \"untold\"         \"unverified\"    \n[789] \"uphill\"         \"uplift\"         \"uprising\"       \"urgency\"       \n[793] \"urgent\"         \"utopian\"        \"vacation\"       \"venerable\"     \n[797] \"veracity\"       \"verge\"          \"victory\"        \"vigil\"         \n[801] \"vigilance\"      \"vindication\"    \"virginity\"      \"vision\"        \n[805] \"visionary\"      \"visitor\"        \"visor\"          \"volatility\"    \n[809] \"volunteer\"      \"voluptuous\"     \"vote\"           \"vow\"           \n[813] \"voyage\"         \"wait\"           \"warn\"           \"warned\"        \n[817] \"watch\"          \"weigh\"          \"weight\"         \"whim\"          \n[821] \"white\"          \"wilderness\"     \"winner\"         \"winning\"       \n[825] \"winnings\"       \"wireless\"       \"wishful\"        \"withstand\"     \n[829] \"wizard\"         \"wont\"           \"worm\"           \"worry\"         \n[833] \"worrying\"       \"worship\"        \"yearning\"       \"young\"         \n[837] \"youth\"          \"zeal\"           \"zest\"          \n\nquanteda.sentiment::data_dictionary_NRC$surprise\n\n  [1] \"abandonment\"     \"abduction\"       \"abrupt\"          \"accident\"       \n  [5] \"accidental\"      \"accidentally\"    \"accolade\"        \"advance\"        \n  [9] \"affront\"         \"aghast\"          \"alarm\"           \"alarming\"       \n [13] \"alertness\"       \"alerts\"          \"allure\"          \"amaze\"          \n [17] \"amazingly\"       \"ambush\"          \"angel\"           \"anomaly\"        \n [21] \"apparition\"      \"applause\"        \"art\"             \"aspiration\"     \n [25] \"assail\"          \"assessment\"      \"astonishingly\"   \"astonishment\"   \n [29] \"attacking\"       \"avalanche\"       \"award\"           \"bang\"           \n [33] \"banger\"          \"betray\"          \"bewildered\"      \"bewilderment\"   \n [37] \"birthday\"        \"bizarre\"         \"blast\"           \"blessings\"      \n [41] \"blindfold\"       \"blitz\"           \"bloodshed\"       \"bomb\"           \n [45] \"bonus\"           \"break\"           \"brighten\"        \"buck\"           \n [49] \"burlesque\"       \"cable\"           \"cadaver\"         \"camouflage\"     \n [53] \"camouflaged\"     \"candid\"          \"captivate\"       \"carnage\"        \n [57] \"catastrophe\"     \"catch\"           \"celebration\"     \"celebrity\"      \n [61] \"ceremony\"        \"chance\"          \"changeable\"      \"chant\"          \n [65] \"cheer\"           \"cheerful\"        \"cherish\"         \"chicane\"        \n [69] \"chimera\"         \"chuckle\"         \"clamor\"          \"climax\"         \n [73] \"clown\"           \"coincidence\"     \"compensate\"      \"complement\"     \n [77] \"compliment\"      \"concealed\"       \"confession\"      \"conjure\"        \n [81] \"coup\"            \"crash\"           \"cream\"           \"crescendo\"      \n [85] \"crowning\"        \"curiosity\"       \"cyclone\"         \"daemon\"         \n [89] \"dawn\"            \"deal\"            \"death\"           \"deceit\"         \n [93] \"decoy\"           \"defy\"            \"delighted\"       \"deluge\"         \n [97] \"destination\"     \"detonate\"        \"devastation\"     \"differently\"    \n[101] \"dire\"            \"disaster\"        \"dismay\"          \"dismissal\"      \n[105] \"disruption\"      \"dissolution\"     \"distress\"        \"disturbance\"    \n[109] \"divergent\"       \"diversion\"       \"divorce\"         \"dolphin\"        \n[113] \"dreadfully\"      \"dynamic\"         \"eager\"           \"earthquake\"     \n[117] \"ecstatic\"        \"ejaculation\"     \"electric\"        \"elusive\"        \n[121] \"embarrassment\"   \"embrace\"         \"emergency\"       \"enchant\"        \n[125] \"enliven\"         \"entertainment\"   \"enthusiasm\"      \"enthusiast\"     \n[129] \"epidemic\"        \"erotic\"          \"erratic\"         \"erupt\"          \n[133] \"eruption\"        \"evanescence\"     \"examination\"     \"excavation\"     \n[137] \"excel\"           \"excitation\"      \"excite\"          \"excited\"        \n[141] \"excitement\"      \"exciting\"        \"exclaim\"         \"exhilaration\"   \n[145] \"exigent\"         \"expect\"          \"experiment\"      \"explode\"        \n[149] \"explosion\"       \"explosive\"       \"fainting\"        \"fanfare\"        \n[153] \"favorable\"       \"fearfully\"       \"feat\"            \"feeling\"        \n[157] \"festival\"        \"fete\"            \"fiesta\"          \"finally\"        \n[161] \"flinch\"          \"flirt\"           \"fluke\"           \"foresee\"        \n[165] \"fortune\"         \"frantic\"         \"freakish\"        \"frenetic\"       \n[169] \"fright\"          \"frighten\"        \"frightened\"      \"frisky\"         \n[173] \"gambler\"         \"gambling\"        \"gape\"            \"garish\"         \n[177] \"gasp\"            \"gawk\"            \"generosity\"      \"gift\"           \n[181] \"glimmer\"         \"glorify\"         \"godsend\"         \"good\"           \n[185] \"goodness\"        \"graduation\"      \"gratify\"         \"greatness\"      \n[189] \"greeting\"        \"grin\"            \"guess\"           \"gulp\"           \n[193] \"hap\"             \"hermaphrodite\"   \"hero\"            \"heroic\"         \n[197] \"heroism\"         \"highest\"         \"hilarious\"       \"hoax\"           \n[201] \"holiness\"        \"honeymoon\"       \"hope\"            \"hopeful\"        \n[205] \"horde\"           \"horror\"          \"howl\"            \"humanitarian\"   \n[209] \"hypertrophy\"     \"hypothesis\"      \"illegitimate\"    \"illuminate\"     \n[213] \"illumination\"    \"illusion\"        \"immediacy\"       \"immerse\"        \n[217] \"improvisation\"   \"improvise\"       \"incendiary\"      \"incident\"       \n[221] \"incontinence\"    \"independence\"    \"inexplicable\"    \"infant\"         \n[225] \"infarct\"         \"infrequent\"      \"inheritance\"     \"insolvency\"     \n[229] \"inspired\"        \"insult\"          \"intense\"         \"interrupt\"      \n[233] \"intrigue\"        \"intruder\"        \"intrusive\"       \"invade\"         \n[237] \"invite\"          \"inviting\"        \"jackpot\"         \"jerk\"           \n[241] \"jest\"            \"joker\"           \"jolt\"            \"jubilant\"       \n[245] \"jubilee\"         \"judgment\"        \"kidnap\"          \"kiss\"           \n[249] \"labor\"           \"larger\"          \"latent\"          \"laugh\"          \n[253] \"laughter\"        \"lawsuit\"         \"leave\"           \"leery\"          \n[257] \"leisure\"         \"liberate\"        \"liberation\"      \"liberty\"        \n[261] \"lightning\"       \"lose\"            \"lovely\"          \"loyal\"          \n[265] \"luck\"            \"lucky\"           \"lunge\"           \"magical\"        \n[269] \"magician\"        \"magnificent\"     \"majestic\"        \"manslaughter\"   \n[273] \"marry\"           \"marvel\"          \"medal\"           \"memorable\"      \n[277] \"merriment\"       \"mimicry\"         \"miracle\"         \"miraculous\"     \n[281] \"misbehavior\"     \"mishap\"          \"modify\"          \"money\"          \n[285] \"monstrosity\"     \"morals\"          \"mouth\"           \"murder\"         \n[289] \"murderous\"       \"musical\"         \"mutiny\"          \"mysterious\"     \n[293] \"mystery\"         \"mystic\"          \"nab\"             \"nefarious\"      \n[297] \"newcomer\"        \"nullify\"         \"obit\"            \"obliging\"       \n[301] \"occasional\"      \"oddity\"          \"opera\"           \"optimism\"       \n[305] \"ordeal\"          \"organization\"    \"originality\"     \"outburst\"       \n[309] \"outcry\"          \"outrageous\"      \"overdue\"         \"overestimate\"   \n[313] \"palpable\"        \"pang\"            \"parade\"          \"paralyzed\"      \n[317] \"peaceful\"        \"perchance\"       \"perfection\"      \"peri\"           \n[321] \"perjury\"         \"picnic\"          \"pitfall\"         \"placard\"        \n[325] \"playful\"         \"playground\"      \"pleasant\"        \"plunder\"        \n[329] \"polarity\"        \"pop\"             \"postponement\"    \"practiced\"      \n[333] \"praiseworthy\"    \"prank\"           \"pray\"            \"precious\"       \n[337] \"premature\"       \"present\"         \"preservative\"    \"presto\"         \n[341] \"prick\"           \"princely\"        \"procession\"      \"proficiency\"    \n[345] \"prowl\"           \"puma\"            \"punch\"           \"purity\"         \n[349] \"quickness\"       \"quicksilver\"     \"quote\"           \"raffle\"         \n[353] \"raid\"            \"randomly\"        \"rapid\"           \"rapt\"           \n[357] \"rarity\"          \"rave\"            \"raving\"          \"reappear\"       \n[361] \"receiving\"       \"recklessness\"    \"reflex\"          \"rejoice\"        \n[365] \"rejoicing\"       \"rekindle\"        \"remarkable\"      \"rescue\"         \n[369] \"resignation\"     \"revenge\"         \"reversal\"        \"revolt\"         \n[373] \"revolution\"      \"reward\"          \"rhythmical\"      \"riddle\"         \n[377] \"riotous\"         \"romance\"         \"rupture\"         \"sabotage\"       \n[381] \"saint\"           \"saintly\"         \"sally\"           \"sanctify\"       \n[385] \"scare\"           \"score\"           \"scorpion\"        \"scream\"         \n[389] \"screech\"         \"scrimmage\"       \"secrecy\"         \"senseless\"      \n[393] \"sensual\"         \"shatter\"         \"shell\"           \"shock\"          \n[397] \"shockingly\"      \"shopping\"        \"shot\"            \"shout\"          \n[401] \"shriek\"          \"shrill\"          \"simplify\"        \"singularly\"     \n[405] \"sisterhood\"      \"slam\"            \"slap\"            \"slaughter\"      \n[409] \"slaughtering\"    \"slayer\"          \"slip\"            \"slush\"          \n[413] \"smile\"           \"snag\"            \"sneak\"           \"sneeze\"         \n[417] \"somatic\"         \"sorcery\"         \"spa\"             \"sparkle\"        \n[421] \"spectacular\"     \"spirits\"         \"splash\"          \"splendid\"       \n[425] \"splendor\"        \"stab\"            \"stagger\"         \"startle\"        \n[429] \"startling\"       \"steady\"          \"stealth\"         \"stealthily\"     \n[433] \"stealthy\"        \"strangle\"        \"stunned\"         \"subito\"         \n[437] \"subversive\"      \"succeed\"         \"sudden\"          \"suddenly\"       \n[441] \"sun\"             \"sunny\"           \"supremacy\"       \"surge\"          \n[445] \"surprise\"        \"surprised\"       \"surprising\"      \"surprisingly\"   \n[449] \"suspense\"        \"sweet\"           \"swerve\"          \"synchronize\"    \n[453] \"syncope\"         \"tackle\"          \"tantalizing\"     \"teach\"          \n[457] \"tempest\"         \"terrorist\"       \"thief\"           \"thirst\"         \n[461] \"thrill\"          \"thrilling\"       \"thwart\"          \"tickle\"         \n[465] \"topple\"          \"transcendence\"   \"treachery\"       \"treason\"        \n[469] \"treat\"           \"tree\"            \"trepidation\"     \"trick\"          \n[473] \"trickery\"        \"trip\"            \"trophy\"          \"trump\"          \n[477] \"tumult\"          \"unanticipated\"   \"unbeaten\"        \"unbridled\"      \n[481] \"uncanny\"         \"uncertain\"       \"uncontrollable\"  \"uncover\"        \n[485] \"underestimate\"   \"undiscovered\"    \"unexpected\"      \"unexpectedly\"   \n[489] \"unforeseen\"      \"unfulfilled\"     \"unguarded\"       \"unimaginable\"   \n[493] \"unintended\"      \"unintentional\"   \"unintentionally\" \"unique\"         \n[497] \"unprecedented\"   \"unpredictable\"   \"unstable\"        \"unsuspecting\"   \n[501] \"urgency\"         \"urgent\"          \"vanish\"          \"vanished\"       \n[505] \"variable\"        \"veer\"            \"veracity\"        \"victimized\"     \n[509] \"violation\"       \"violent\"         \"visor\"           \"volatility\"     \n[513] \"volcano\"         \"vote\"            \"warn\"            \"warned\"         \n[517] \"weight\"          \"whim\"            \"wild\"            \"wildfire\"       \n[521] \"winner\"          \"winning\"         \"wireless\"        \"wizard\"         \n[525] \"wonderful\"       \"wonderfully\"     \"worm\"            \"wreck\"          \n[529] \"yell\"            \"yelp\"            \"young\"           \"youth\"          \n[533] \"zany\"            \"zeal\""
  },
  {
    "objectID": "word-counting-improvements.html#norms",
    "href": "word-counting-improvements.html#norms",
    "title": "15  Improvements on Word Counts",
    "section": "\n15.2 Norms",
    "text": "15.2 Norms\nCalled “Valence” in quanteda.sentiment\n\n\n\n\nPlutchik, R. (1962). The emotions. Random House."
  },
  {
    "objectID": "dla.html#sec-freq-ratios",
    "href": "dla.html#sec-freq-ratios",
    "title": "16  Differential Language Analysis",
    "section": "\n16.1 Frequency Ratios",
    "text": "16.1 Frequency Ratios\nThe most intuitive way to compare texts from two groups is one we already explored in depth in Chapter 5: frequency ratios. To get frequency ratios from a Quanteda DFM, we use the textstat_frequency() function from the quanteda.textstats package, with the groups parameter set to the categorical variable of interest. Let’s compare true stories from fictional ones in the Hippocorpus data.\n\nlibrary(quanteda.textstats)\n\nimagined_vs_recalled &lt;- hippocorpus_dfm |&gt; \n  textstat_frequency(groups = memType)\n\nhead(imagined_vs_recalled)\n\n#&gt;   feature frequency rank docfreq    group\n#&gt; 1       i     32069    1    2686 imagined\n#&gt; 2     the     25825    2    2742 imagined\n#&gt; 3      to     24102    3    2746 imagined\n#&gt; 4     and     20842    4    2707 imagined\n#&gt; 5       a     16943    5    2714 imagined\n#&gt; 6     was     15404    6    2618 imagined\n\n\nThe resulting dataframe has one row per feature per category. frequency is the number of times the feature appears in the group, rank is the ordering from highest to lowest frequency within each group, and docfreq is the number of documents in the group in which the feature appears at least once. To compare imagined stories to recalled ones, we can calculate frequency ratios.\n\nimagined_vs_recalled &lt;- imagined_vs_recalled |&gt; \n  filter(group %in% c(\"imagined\", \"recalled\")) |&gt; \n  pivot_wider(id_cols = \"feature\", \n              names_from = \"group\", \n              values_from = \"frequency\",\n              names_prefix = \"count_\") |&gt; \n  mutate(imagined_freq_ratio = count_imagined/count_recalled)\n\nhead(imagined_vs_recalled)\n\n#&gt; # A tibble: 6 × 4\n#&gt;   feature count_imagined count_recalled imagined_freq_ratio\n#&gt;   &lt;chr&gt;            &lt;dbl&gt;          &lt;dbl&gt;               &lt;dbl&gt;\n#&gt; 1 i                32069          32903               0.975\n#&gt; 2 the              25825          31514               0.819\n#&gt; 3 to               24102          27379               0.880\n#&gt; 4 and              20842          26444               0.788\n#&gt; 5 a                16943          19849               0.854\n#&gt; 6 was              15404          19483               0.791\n\n\nWe can now plot a rotated F/F plot, as in Chapter 5.\n\nlibrary(ggiraph, verbose = FALSE)\nlibrary(ggrepel)\n\nset.seed(2023)\np &lt;- imagined_vs_recalled |&gt; \n  mutate(\n    # calculate total frequency\n    common = count_imagined + count_recalled,\n    # remove single quotes (for html)\n    feature = str_replace_all(feature, \"'\", \"`\")) |&gt; \n  ggplot(aes(imagined_freq_ratio, common, \n             label = feature,\n             color = imagined_freq_ratio,\n             tooltip = feature, \n             data_id = feature\n             )) +\n    geom_point_interactive() +\n    geom_text_repel_interactive(size = 2) +\n    scale_y_continuous(\n      trans = \"log2\", breaks = ~.x,\n      minor_breaks = ~2^(seq(0,log2(.x[2]))),\n      labels = c(\"Rare\", \"Common\")\n      ) +   \n    scale_x_continuous(\n      trans = \"log10\", limits = c(1/10,10),\n      breaks = c(1/10, 1, 10),\n      labels = c(\"10x More Common\\nin Recalled Stories\",\n                 \"Equal Proportion\",\n                 \"10x More Common\\nin Imagined Stories\")\n      ) +\n    scale_color_gradientn(\n      colors = c(\"#023903\", \n                 \"#318232\",\n                 \"#E2E2E2\", \n                 \"#9B59A7\",\n                 \"#492050\"), \n      trans = \"log2\", # log scale for ratios\n      guide = \"none\"\n      ) +\n    labs(\n      title = \"Words in Imagined and Recalled Stories\",\n      x = \"\",\n      y = \"Total Frequency\",\n      color = \"\"\n    ) +\n    # fixed coordinates since x and y use the same units\n    coord_fixed(ratio = 1/8) + \n    theme_minimal()\n\ngirafe_options(\n  girafe(ggobj = p),\n  opts_tooltip(css = \"font-family:sans-serif;font-size:1em;color:Black;\")\n  )\n\n\n\n\n\n\nThis view allows us to explore individual words that are characteristic of one or the other group. It also shows the overall shape of the distribution—the slight skew to the left indicates that recalled stories tend to be slightly longer than true ones."
  },
  {
    "objectID": "dla.html#sec-keyness",
    "href": "dla.html#sec-keyness",
    "title": "16  Differential Language Analysis",
    "section": "\n16.2 Keyness",
    "text": "16.2 Keyness\nWorking with frequency ratios has an intuitive appeal; “10 times more common in group A than group B” is a statement that anyone can understand. Even so, frequency ratios are statistically misleading, since they do not account for random sampling error. For example, a word that appears 1000 times in group A and 100 times in group B is much more convincingly representative of group A than a word that appears 10 times in group A and 1 time in group B, even though the frequency ratio is identical. Furthermore, simple frequency ratios do not account for base rates—as we saw in the the F/F plot in Section 16.1, ratios can be skewed to one side simply because texts in one group are longer than those in another. These problems can be solved by using more statistically motivated methods for group comparisons, sometimes referred to as keyness statistics.\nQuanteda’s default keyness statistic is none other than the chi-squared value, which compares the observed frequencies to the expected ones if there were no difference between the groups. We can compute this statistic directly from the DFM using the textstat_keyness() function, with the “target” parameter set to one of the two groups of documents, in this case imagined ones, or docvars(imagined_vs_recalled_dfm, \"memType\") == \"imagined\". The function keeps the chi-squared statistic positive or flips it to negative based on whether the frequency in the target group is higher or lower than the expected value.\n\n# Filtered DFM\nimagined_vs_recalled_dfm &lt;- hippocorpus_dfm |&gt; \n  # only keep features that appear in at least 30 documents\n  dfm_trim(min_docfreq = 30) |&gt;\n  # only keep imagined and recalled stories (not retold)\n  dfm_subset(memType %in% c(\"imagined\", \"recalled\"))\n\n# Calculate Keyness\nimagined_keyness &lt;- imagined_vs_recalled_dfm |&gt; \n  textstat_keyness(docvars(imagined_vs_recalled_dfm, \"memType\") == \"imagined\")\n\nhead(imagined_keyness)\n\n#&gt;   feature      chi2 p n_target n_reference\n#&gt; 1     ago 223.40058 0     1714        1109\n#&gt; 2       i 194.40823 0    32069       32903\n#&gt; 3   can't 100.66063 0      466         246\n#&gt; 4     i'm  82.27271 0     1010         747\n#&gt; 5    just  81.62334 0     2613        2307\n#&gt; 6  really  72.28766 0     1892        1622\n\n\nWe can use Quanteda to generate a simple plot of the most extreme values of our keyness statistic, using the textplot_keyness() function, from the quanteda.textplots package.\n\nimagined_keyness |&gt; \n  quanteda.textplots::textplot_keyness() +\n    labs(title = \"Words in Imagined (target) and Recalled (reference) Stories\")\n\n\n\n\nWe can also represent the keyness values as a word cloud, as we did for frequency ratios in Chapter 5.\n\nlibrary(ggwordcloud)\nset.seed(2)\n\nimagined_keyness |&gt; \n  # only words with significant difference to p &lt; .01\n  filter(p &lt; .01) |&gt; \n  # arrange in descending order\n  arrange(desc(abs(chi2))) |&gt; \n  # plot\n  ggplot(aes(label = feature, \n             size = chi2, \n             color = chi2 &gt; 0,\n             angle_group = chi2 &gt; 0)) +\n    geom_text_wordcloud_area(show.legend = TRUE) + \n    scale_size_area(max_size = 20, guide = \"none\") +\n    scale_color_discrete(\n      name = \"\",\n      breaks = c(FALSE, TRUE),\n      labels = c(\"More in Recalled\", \n                 \"More in Imagined\")\n      ) +\n    labs(caption = \"Only words with a significant difference between the groups (p &lt; .01) were included.\") +\n    theme_void() # blank background"
  },
  {
    "objectID": "vectorspace-intro.html#distance-and-similarity",
    "href": "vectorspace-intro.html#distance-and-similarity",
    "title": "17  Thinking in Vector Space",
    "section": "\n17.1 Distance and Similarity",
    "text": "17.1 Distance and Similarity\n\n17.1.1 Euclidean Distance\n\\[\nd\\left( A,B\\right)   = \\sqrt {\\sum _{i=1}^{n}  \\left( A_{i}-B_{i}\\right)^2 }\n\\]\n\n17.1.2 Cosine Similarity\n\\[\nCosine(A,B) = \\frac{A \\cdot B}{|A||B|} = \\frac{\\sum _{i=1}^{n}  A_{i}B_{i}}{\\sqrt {\\sum _{i=1}^{n} A_{i}^2} \\cdot \\sqrt {\\sum _{i=1}^{n} B_{i}^2}}\n\\]\n\n\n\n\nAlammar, J. (2019). The illustrated Word2vec. In Jay Alammar – Visualizing machine learning one concept at a time. http://jalammar.github.io/illustrated-word2vec/"
  },
  {
    "objectID": "vectorspace-intro.html#footnotes",
    "href": "vectorspace-intro.html#footnotes",
    "title": "17  Thinking in Vector Space",
    "section": "",
    "text": "This section is adapted from Alammar (2019)↩︎"
  },
  {
    "objectID": "decontextualized-embeddings.html#the-distributional-hypothesis",
    "href": "decontextualized-embeddings.html#the-distributional-hypothesis",
    "title": "18  Bag of Word Embeddings: Decontextualized Models",
    "section": "\n18.1 The Distributional Hypothesis",
    "text": "18.1 The Distributional Hypothesis\nHence “Distributional Semantic Models” (DSMs)\n\nTwo words are similar in meaning not because of their mutual co-occurrence score, but rather if they have similar global distributional patterns over all contexts. Due to this, synonyms, which tend to very rarely co-occur directly, will have very similar mean- ings in DSMs (Günther et al., 2019)."
  },
  {
    "objectID": "decontextualized-embeddings.html#lsa",
    "href": "decontextualized-embeddings.html#lsa",
    "title": "18  Bag of Word Embeddings: Decontextualized Models",
    "section": "\n18.2 LSA",
    "text": "18.2 LSA\nDFM -&gt; Weighting (tf-idf, PPMI) -&gt; Dimensionality reduction\n“Indexing by Latent Semantic Analysis” (1990)\nGarcia & Sikström (2013)\nhttps://quanteda.io/articles/pkgdown/examples/lsa.html"
  },
  {
    "objectID": "decontextualized-embeddings.html#lda",
    "href": "decontextualized-embeddings.html#lda",
    "title": "18  Bag of Word Embeddings: Decontextualized Models",
    "section": "\n18.3 LDA",
    "text": "18.3 LDA"
  },
  {
    "objectID": "decontextualized-embeddings.html#word2vec",
    "href": "decontextualized-embeddings.html#word2vec",
    "title": "18  Bag of Word Embeddings: Decontextualized Models",
    "section": "\n18.4 Word2vec",
    "text": "18.4 Word2vec\nChatterjee et al. (2023)"
  },
  {
    "objectID": "decontextualized-embeddings.html#glove",
    "href": "decontextualized-embeddings.html#glove",
    "title": "18  Bag of Word Embeddings: Decontextualized Models",
    "section": "\n18.5 GloVe",
    "text": "18.5 GloVe\nPennington et al. (2014)\nGloVe is built on the same metric that we used in Chapter 5 and ?sec-eda - relative frequency ratios. Rather than comparing two word frequencies in two groups of texts, it instead compares co-occurrence with one word to co-occurrence with another.\n\n18.5.1 Training a Custom GloVe Model\nhttps://quanteda.io/articles/pkgdown/replication/text2vec.html\n\n\n\n\nChatterjee, P., Mishra, H., & Mishra, A. (2023). Does the first letter of one’s name affect life decisions? A natural language processing examination of nominative determinism. Journal of Personality and Social Psychology, 125. https://doi.org/10.1037/pspa0000347\n\n\nGarcia, D., & Sikström, S. (2013). Quantifying the Semantic Representations of Adolescents’ Memories of Positive and Negative Life Events. Journal of Happiness Studies, 14(4), 1309–1323. https://doi.org/10.1007/s10902-012-9385-8\n\n\nGünther, F., Rinaldi, L., & Marelli, M. (2019). Vector-space models of semantic representation from a cognitive perspective: A discussion of common misconceptions. Perspectives on Psychological Science, 14(6), 1006–1033. https://doi.org/10.1177/1745691619861372\n\n\nIndexing by latent semantic analysis. (1990). Journal of the Association for Information Science and Technology, 41(6), 391–407. https://doi.org/10.1002/(SICI)1097-4571(199009)41:6&lt;391::AID-ASI1&gt;3.0.CO;2-9\n\n\nPennington, J., Socher, R., & Manning, C. D. (2014). GloVe: Global vectors for word representation. Empirical Methods in Natural Language Processing (EMNLP), 1532–1543. http://www.aclweb.org/anthology/D14-1162"
  },
  {
    "objectID": "contextualized-embeddings.html",
    "href": "contextualized-embeddings.html",
    "title": "19  Contextualization and Large Language Models",
    "section": "",
    "text": "19.0.1 Hugging Face and the text Package\nO. N. E. Kjell et al. (2021)\nO. Kjell et al. (2022)\n\n\n\n\nKjell, O. N. E., Giorgi, S., & Schwartz, H. A. (2021). The text-package: An r-package for analyzing and visualizing human language using natural language processing and deep learning. PsyArXiv. https://doi.org/10.31234/osf.io/293kt\n\n\nKjell, O., Sikström, S., Kjell, K., & Schwartz, H. (2022). Natural language analyzed with AI-based transformers predict traditional subjective well-being measures approaching the theoretical upper limits in accuracy. Scientific Reports, 12, 3918. https://doi.org/10.1038/s41598-022-07520-w"
  },
  {
    "objectID": "navigating-vectorspace.html#sec-parallelograms",
    "href": "navigating-vectorspace.html#sec-parallelograms",
    "title": "20  Navigating Vector Space",
    "section": "\n20.1 Parallelograms",
    "text": "20.1 Parallelograms\nIntroduced with word2vec by Mikolov et al. (2013)\nGlove (Pennington et al., 2014) is designed with this property in mind. Transformer models are not.\nIn CCR, for example, there’s a difference between agreeing with statements in a questionnaire and actually writing like a questionnaire. How do I know that I’m measuring how anxious/depressed the comments are and not how questionnaire-y they are? Potential workaround: reverse all of the statements in the questionnaire (as for reverse-coded items), add them to the original questionnaire, and get the average embedding of the construct-neutral questionnaire. Then subtract this embedding from the original questionnaire average embedding."
  },
  {
    "objectID": "navigating-vectorspace.html#sec-advanced-similarity",
    "href": "navigating-vectorspace.html#sec-advanced-similarity",
    "title": "20  Navigating Vector Space",
    "section": "\n20.2 Advanced Similarity Measures",
    "text": "20.2 Advanced Similarity Measures\n\nJensen–Shannon distance"
  },
  {
    "objectID": "navigating-vectorspace.html#sec-dimension-projection",
    "href": "navigating-vectorspace.html#sec-dimension-projection",
    "title": "20  Navigating Vector Space",
    "section": "\n20.3 Dimension Projection",
    "text": "20.3 Dimension Projection"
  },
  {
    "objectID": "navigating-vectorspace.html#sec-machine-learning-methods",
    "href": "navigating-vectorspace.html#sec-machine-learning-methods",
    "title": "20  Navigating Vector Space",
    "section": "\n20.4 Machine Learning Methods",
    "text": "20.4 Machine Learning Methods\nChersoni et al. (2021) used partial least squares regression to map distributional embeddings to human-rated semantic features derived from research in cognitive psychology.\n\n\n\n\nChersoni, E., Santus, E., Huang, C.-R., & Lenci, A. (2021). Decoding word embeddings with brain-based semantic features. Computational Linguistics, 47(3), 663–698. https://doi.org/10.1162/coli_a_00412\n\n\nGünther, F., Rinaldi, L., & Marelli, M. (2019). Vector-space models of semantic representation from a cognitive perspective: A discussion of common misconceptions. Perspectives on Psychological Science, 14(6), 1006–1033. https://doi.org/10.1177/1745691619861372\n\n\nMikolov, T., Yih, W., & Zweig, G. (2013). Linguistic regularities in continuous space word representations. In L. Vanderwende, H. Daumé III, & K. Kirchhoff (Eds.), Proceedings of the 2013 conference of the north American chapter of the association for computational linguistics: Human language technologies (pp. 746–751). Association for Computational Linguistics. https://aclanthology.org/N13-1090\n\n\nPennington, J., Socher, R., & Manning, C. D. (2014). GloVe: Global vectors for word representation. Empirical Methods in Natural Language Processing (EMNLP), 1532–1543. http://www.aclweb.org/anthology/D14-1162"
  },
  {
    "objectID": "linguistic-complexity.html",
    "href": "linguistic-complexity.html",
    "title": "21  Measures of Linguistic Complexity",
    "section": "",
    "text": "This section has not yet been written. Check again in a few months!"
  },
  {
    "objectID": "audio-video-image.html",
    "href": "audio-video-image.html",
    "title": "Audio, Video, and Image Data",
    "section": "",
    "text": "This section has not yet been written. Check again in a few months!"
  }
]