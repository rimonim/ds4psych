<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Louis Teitelbaum and Almog Simchon">
<title>Data Science for Psychology: Natural Language - 19&nbsp; Contextualization With Large Language Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./navigating-vectorspace.html" rel="next">
<link href="./decontextualized-embeddings.html" rel="prev">
<link href="./images/favicon.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./quantification.html">Quantifying Psychological Properties of Text</a></li><li class="breadcrumb-item"><a href="./contextualized-embeddings.html"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Contextualization With Large Language Models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Data Science for Psychology: Natural Language</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/rimonim/ds4psych" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
<li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=%7Curl%7C">
              <i class="bi bi-bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=%7Curl%7C">
              <i class="bi bi-bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=%7Curl%7C">
              <i class="bi bi-bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
      </ul>
</div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction: Why Does Psychology Need Natural Language?</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ethics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Ethics of Data Science in Psychology</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./data-viz.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Visualization</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./aesthetics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Why Aesthetic Choices are Important</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./telling-a-story.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Don’t Distract From the Story</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./word-viz.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Visualizing Distributions of Words</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data-viz-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Additional Resources for Data Visualization</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./data-sources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sources of Data</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./experiments.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Experiments</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./corpora.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Corpus Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./apis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Web APIs</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./scraping.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Web Scraping</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./quantification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quantifying Psychological Properties of Text</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./look-at-your-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Look at Your Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quanteda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Introduction to Quanteda</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tokenization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Tokenization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./word-counting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Dictionary-Based Word Counts</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dla.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Open Vocabulary Word Counting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./word-counting-improvements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Transforming Word Counts</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./vectorspace-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Introduction to Vector Space</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./decontextualized-embeddings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Word Embeddings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./contextualized-embeddings.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Contextualization With Large Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./navigating-vectorspace.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Navigating Vector Space</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Topic Modeling</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linguistic-complexity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Measures of Linguistic Complexity</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./audio-video-image.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Audio, Video, and Image Data</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./transcription.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Transcribing Audio</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Additional Resources</span></span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#hugging-face-and-the-text-package" id="toc-hugging-face-and-the-text-package" class="nav-link active" data-scroll-target="#hugging-face-and-the-text-package"><span class="header-section-number">19.1</span> Hugging Face and the <code>text</code> Package</a>
  <ul class="collapse">
<li><a href="#managing-computational-load" id="toc-managing-computational-load" class="nav-link" data-scroll-target="#managing-computational-load"><span class="header-section-number">19.1.1</span> Managing Computational Load</a></li>
  <li><a href="#choosing-the-right-model" id="toc-choosing-the-right-model" class="nav-link" data-scroll-target="#choosing-the-right-model"><span class="header-section-number">19.1.2</span> Choosing the Right Model</a></li>
  </ul>
</li>
  <li><a href="#dimensionality-reduction" id="toc-dimensionality-reduction" class="nav-link" data-scroll-target="#dimensionality-reduction"><span class="header-section-number">19.2</span> Dimensionality Reduction</a></li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/rimonim/ds4psych/blob/main/contextualized-embeddings.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/rimonim/ds4psych/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./quantification.html">Quantifying Psychological Properties of Text</a></li><li class="breadcrumb-item"><a href="./contextualized-embeddings.html"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Contextualization With Large Language Models</span></a></li></ol></nav><div class="quarto-title">
<h1 class="title"><span id="sec-contextualized-embeddings" class="quarto-section-identifier"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Contextualization With Large Language Models</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><p>The models we discussed in <a href="decontextualized-embeddings.html#sec-word-embeddings" class="quarto-xref"><span>Section 18.3</span></a> represent the meaning of each token as a point in multidimensional space: a word embedding. Word embeddings generated by models like word2vec or GloVe are often referred to as <strong>decontextualized embeddings</strong>. This name is a bit confusing, since as we saw in <a href="decontextualized-embeddings.html#sec-word-embeddings" class="quarto-xref"><span>Section 18.3</span></a>, the whole point of those models is to associate tokens with the contexts in which they tend to appear. A better name might be <em>average context embeddings</em>, since the best they can hope to represent is the average of the contexts in which a token appears throughout the training corpus. For example, consider the following uses of the token “short”.</p>
<blockquote class="blockquote">
<p>My dad is very <em>short</em>.</p>
</blockquote>
<blockquote class="blockquote">
<p>My blender broke because of a <em>short</em> circuit.</p>
</blockquote>
<blockquote class="blockquote">
<p>That video was anything but <em>short</em>.</p>
</blockquote>
<blockquote class="blockquote">
<p>I can’t pay because I’m <em>short</em> on cash at the moment.</p>
</blockquote>
<p>Any speaker of English can easily see that the word “short” means something different in each one of these examples. But because word2vec and similar models are trained to predict the context based on only a single word at a time, their representation of the word <em>short</em> will only capture that word’s average meaning.</p>
<p>How can we move beyond the average meaning and capture the different meanings words take on in different contexts? You are probably already familiar with Large Language Models (LLMs) like ChatGPT and Claude. At their core, much of what these models do is exactly this: They find the intricate relationships between tokens in a text and use them to develop a new understanding of what these tokens mean in the particular context of that text. For this reason, embeddings produced by these models are often referred to as <strong>contextualized embeddings</strong>.</p>
<p>Even if you are familiar with ChatGPT, you may not have realized that it uses embeddings. What do embeddings have to do with generating text? The core of all modern LLMs is a model called the <em>transformer</em>. We will not cover exactly how transformers work—for an intuitive introduction, see <a href="https://youtu.be/wjZofJX0v4M?si=Jx0fvQns3nqukk6e">3blue1brown’s video explanation</a>. For the purposes of this book, all you need to know is this: Transformers start by converting all the words in a text into word embeddings, just like word2vec or GloVe. At the start, these word embeddings represent the average meaning of each word. The transformer then estimates how each word in the text might be relevant for better understanding the meaning of the other words. For example, if “circuit” appears right after “short”, the embedding of “short” should probably be tweaked. Once it has identified this connection, the transformer computes what “circuit” should add to a word that it is associated with, moving the “short” embedding closer to embeddings for electrical concepts. A full LLM has many <em>layers</em>. In each layer, the LLM identifies more connections between embeddings and shifts the embeddings in the vector space to add more nuance to their representations. When it gets to the final layer, the LLM uses the enriched embeddings of the words in the text for whatever task it was trained to do (e.g.&nbsp;predicting what the next word will be, or identifying whether the text is spam or not).</p>
<p>Even though LLMs can be extremely complex and capture many nonlinear relationships between concepts, the transformer architecture forces them to organize their embeddings in a roughly linear space in which each direction has a consistent meaning—a critical property for analyzing embeddings with cosine similarity and other straightforward methods. In order to extract an LLM’s rich, contextualized embeddings, all we need to do is run it on a text and stop it before it finishes predicting the next word (or whatever else it was trained to do). This way, we can read the LLM’s mind, capturing all of the rich associations it has with the text.</p>
<section id="hugging-face-and-the-text-package" class="level2" data-number="19.1"><h2 data-number="19.1" class="anchored" data-anchor-id="hugging-face-and-the-text-package">
<span class="header-section-number">19.1</span> Hugging Face and the <code>text</code> Package</h2>
<p>Leading commercial LLMs like GPT-4 are hidden behind APIs so that their inner workings are kept secret. We therefore cannot access the embeddings of these high profile models. Nevertheless, plenty of models that are almost as good are open source and easily accessible through <a href="https://huggingface.co/docs/transformers/index">Hugging Face Transformers</a>. New open source models are added to Hugging Face every day, often by leading companies like Google and Meta. Any text-based transformer model can be accessed in R using the <a href="https://r-text.org"><code>text</code></a> package <span class="citation" data-cites="kjell_etal_2021">(<a href="#ref-kjell_etal_2021" role="doc-biblioref">Kjell et al., 2021</a>)</span>. The <code>text</code> package makes running models and extracting embeddings easy even for those of us who barely understand how the models work.</p>
<p>The <code>text</code> package runs Python code behind the scenes, so you will have to set up a Python environment for it to run properly. For instructions on how to do this, see <a href="https://www.r-text.org/articles/huggingface_in_r_extended_installation_guide.html">here</a>. Once you have the package installed and working, you can begin generating contextualized embeddings for texts with the <code><a href="https://r-text.org/reference/textEmbed.html">textEmbed()</a></code> function. Let’s use the second to last layer of the <a href="https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2"><code>all-MiniLM-L12-v2</code></a> model to embed the Hippocorpus texts as 384-dimensional vectors. Since the model includes preprocessing and tokenization built in, we can feed it the raw texts as a character vector. By default, <code><a href="https://r-text.org/reference/textEmbed.html">textEmbed()</a></code> creates the full text embedding by averaging the contextualized embeddings of each token in the text. This aggregation can take some time (especially for long texts like the Hippocorpus stories), so here we’ll just use the embedding of the <code>[CLS]</code> (classification) token. The <code>[CLS]</code> token is a special token that models based on <a href="https://jalammar.github.io/illustrated-bert/">BERT</a> add to each text. Because the <code>[CLS]</code> token does not have a “real” meaning, but rather is inserted at the same place in every text, its contextualized embedding represents the gist of each text as a whole. In training, BERT models use the contextualized embedding of the <code>[CLS]</code> token to predict whether a given text does or does not come after the input text. This makes it a good general use embedding for when aggregating across all tokens is too time-consuming<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. Even when the output is limited to the <code>[CLS]</code> token, <code><a href="https://r-text.org/reference/textEmbed.html">textEmbed()</a></code> can take a few hours to run.</p>
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-text.org/">text</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># full texts (as character vector)</span></span>
<span><span class="va">hippocorpus_texts</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_df</span><span class="op">$</span><span class="va">story</span></span>
<span></span>
<span><span class="co"># embed the texts</span></span>
<span><span class="va">hippocorpus_sbert</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://r-text.org/reference/textEmbed.html">textEmbed</a></span><span class="op">(</span></span>
<span>  <span class="va">hippocorpus_texts</span>,</span>
<span>  model <span class="op">=</span> <span class="st">"sentence-transformers/all-MiniLM-L12-v2"</span>, <span class="co"># model name</span></span>
<span>  layers <span class="op">=</span> <span class="op">-</span><span class="fl">2</span>,  <span class="co"># second to last layer (default)</span></span>
<span>  tokens_select <span class="op">=</span> <span class="st">"[CLS]"</span>, <span class="co"># use only [CLS] token</span></span>
<span>  dim_name <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  keep_token_embeddings <span class="op">=</span> <span class="cn">FALSE</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># text embeddings as dataframe</span></span>
<span><span class="va">hippocorpus_sbert</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_sbert</span><span class="op">$</span><span class="va">texts</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>ID <span class="op">=</span> <span class="va">hippocorpus_df</span><span class="op">$</span><span class="va">AssignmentId</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># rejoin other variables</span></span>
<span><span class="va">hippocorpus_sbert</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_df</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">rename</span><span class="op">(</span>ID <span class="op">=</span> <span class="va">AssignmentId</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">left_join</span><span class="op">(</span><span class="va">hippocorpus_sbert</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="managing-computational-load" class="level3" data-number="19.1.1"><h3 data-number="19.1.1" class="anchored" data-anchor-id="managing-computational-load">
<span class="header-section-number">19.1.1</span> Managing Computational Load</h3>
<p>Running large neural networks on your personal computer can be time consuming at best. At worst, your computer runs out of memory and your R session crashes. If you are having problems like these, here are some ways you might lessen the computational load when calling <code><a href="https://r-text.org/reference/textEmbed.html">textEmbed()</a></code>:</p>
<ul>
<li>Use a smaller model. Hugging Face model pages generally state how many parameters the model has. This is generally a good indication of how much computational capacity the model needs. For example, consider using <code>distilbert-base-uncased</code> (67M params) or <code>albert-base-v2</code> (11.8M params) instead of <code>bert-base-uncased</code> (110M params).</li>
<li>Make sure you are asking for only one layer at a time (e.g.&nbsp;<code>layers = -2</code>).</li>
<li>If you do not need individual token embeddings, set <code>keep_token_embeddings = FALSE</code>.</li>
<li>To avoid aggregation costs, only ask for individual token embeddings (e.g.&nbsp;<code>tokens_select = "[CLS]"</code>). Not every model uses <code>CLS</code>. make sure that you specify a token that is used by the model you are running—If you’re not sure which special tokens your model uses, try embedding a single text with <code>tokens_select = NULL</code> and <code>keep_token_embeddings = TRUE</code>, and examining the results.</li>
<li>Run the model on your GPU with <code>device = 'gpu'</code> (not available for Apple M1 and M2 chips).</li>
<li>If running the full dataset at once is too much for you or your computer, you can break it up into smaller groups of texts, run each on its own, and join them back together afterward.</li>
</ul>
<p>Before you run <code><a href="https://r-text.org/reference/textEmbed.html">textEmbed()</a></code> on your full dataset, always try running it on two or three texts first. This way, you can get a sense of how long it will take, and make sure that the output is to your liking.</p>
</section><section id="choosing-the-right-model" class="level3" data-number="19.1.2"><h3 data-number="19.1.2" class="anchored" data-anchor-id="choosing-the-right-model">
<span class="header-section-number">19.1.2</span> Choosing the Right Model</h3>
<p>New LLMs are published on Hugging Face every day, and choosing one for your research can be daunting. Part of the beauty of LLMs is their ability to generalize—most popular models nowadays are trained on enormous datasets with a wide variety of content, and even the smaller models perform more than well enough to capture straightforward psychological concepts like emotional valence <span class="citation" data-cites="kjell_etal_2022">(<a href="#ref-kjell_etal_2022" role="doc-biblioref">Kjell et al., 2022</a>)</span>. Even if your model isn’t perfect on every text, research generally relies on statistical patterns, so if your sample size is large enough, it shouldn’t matter. Even with that disclaimer, we can give a few recommendations:</p>
<ul>
<li>
<a href="https://huggingface.co/google-bert/bert-base-uncased">BERT</a> <span class="citation" data-cites="devlin_etal_2019">(<a href="#ref-devlin_etal_2019" role="doc-biblioref">Devlin et al., 2019</a>)</span> and <a href="https://huggingface.co/distilbert/distilbert-base-uncased">DistilBERT</a> <span class="citation" data-cites="sanh_etal_2020">(<a href="#ref-sanh_etal_2020" role="doc-biblioref">Sanh et al., 2020</a>)</span> models are reliable and well-studied. The BERT architecture was designed to be applicable to various tasks (not just next-word prediction, like GPT models), so it is likely to generalize well to whatever you need it for. Also, they use the <code>[CLS]</code> token, which provides a good general-purpose embedding for cases in which aggregating all token embeddings is too computationally demanding.</li>
<li>
<a href="https://huggingface.co/FacebookAI/roberta-base">RoBERTa</a> <span class="citation" data-cites="liu_etal_2019">(<a href="#ref-liu_etal_2019" role="doc-biblioref">Liu et al., 2019</a>)</span> is a refined version of BERT that is known to perform better in identifying personal characteristics of the author of a text. <span class="citation" data-cites="ganesan_etal_2021">Ganesan et al. (<a href="#ref-ganesan_etal_2021" role="doc-biblioref">2021</a>)</span> found that embeddings from the second to last layer of RoBERTa (averaged across tokens) outperformed BERT, <a href="https://huggingface.co/xlnet/xlnet-base-cased">XLNet</a>, and <a href="https://huggingface.co/openai-community/gpt2">GPT-2</a> on predicting gender, age, income, openness, extraversion, and suicide risk from social media posts and student essays. <span class="citation" data-cites="matero_etal_2022">Matero et al. (<a href="#ref-matero_etal_2022" role="doc-biblioref">2022</a>)</span> likewise found that RoBERTa was better than other models at predicting depression, and specifically recommended layer 19 when using the <code>roberta-large</code> model. <a href="https://huggingface.co/distilbert/distilroberta-base">DistilRoBERTa</a> is a lightweight alternative with only slightly worse performance <span class="citation" data-cites="matero_etal_2022">(<a href="#ref-matero_etal_2022" role="doc-biblioref">Matero et al., 2022</a>)</span>, and <a href="https://huggingface.co/docs/transformers/en/model_doc/xlm-roberta">XLM-RoBERTa</a> is favored for analysis of non-English texts. RoBERTa models use the <code>&lt;s&gt;</code> (start) token instead of <code>[CLS]</code>. Nevertheless since RoBERTa is not generally trained on next sentence prediction, the behavior of the <code>&lt;s&gt;</code> will depend on the way the particular model you are using was trained.</li>
<li>
<a href="https://huggingface.co/mental/mental-roberta-base">MentalRoBERTa</a> <span class="citation" data-cites="ji_etal_2021">(<a href="#ref-ji_etal_2021" role="doc-biblioref">Ji et al., 2021</a>)</span> is a version of RoBERTa that was fine-tuned on posts from Reddit communities dealing with mental health issues (e.g.&nbsp;r/depression, r/SuicideWatch, r/Anxiety).</li>
<li>
<a href="https://www.sbert.net/docs/pretrained_models.html">SBERT</a>: Word2vec, GloVe, and related models (<a href="decontextualized-embeddings.html#sec-word-embeddings" class="quarto-xref"><span>Section 18.3</span></a>) have architectures that guarantee vector spaces with consistent geometric properties, allowing researchers to confidently compute averages between vectors and interpret linear directions as encoding unique semantic meanings <span class="citation" data-cites="ethayarajh_etal_2019">(<a href="#ref-ethayarajh_etal_2019" role="doc-biblioref">Ethayarajh et al., 2019</a>)</span>. In contrast, the ways that LLMs organize meaning in their embedding spaces are not well understood and may not always lend themselves to simple measures like those described in this book <span class="citation" data-cites="cai_etal_2021 li_etal_2020 reif_etal_2019">(<a href="#ref-cai_etal_2021" role="doc-biblioref">Cai et al., 2021</a>; <a href="#ref-li_etal_2020" role="doc-biblioref">Li et al., 2020</a>; <a href="#ref-reif_etal_2019" role="doc-biblioref">Reif et al., 2019</a>)</span>. Some researchers have tried to fix this problem by creating models that encourage embeddings to spread out evenly in the embedding space, or that explicitly optimize for reliable cosine similarity metrics. One popular line of such models is <a href="https://www.sbert.net/docs/pretrained_models.html">SBERT</a>. These models, including <a href="https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2"><code>all-MiniLM-L12-v2</code></a>, which we use here, have been demonstrated to work well for directional measures like cosine similarity.</li>
</ul></section></section><section id="dimensionality-reduction" class="level2" data-number="19.2"><h2 data-number="19.2" class="anchored" data-anchor-id="dimensionality-reduction">
<span class="header-section-number">19.2</span> Dimensionality Reduction</h2>
<p>Some studies suggest that using Principle Component Analysis (PCA; see <a href="decontextualized-embeddings.html#sec-lsa-variations" class="quarto-xref"><span>Section 18.2.1</span></a>) to reduce the dimensionality of a set of contextualized embeddings can help their ability to quantify psychological characteristics. Specifically, <span class="citation" data-cites="ganesan_etal_2021">Ganesan et al. (<a href="#ref-ganesan_etal_2021" role="doc-biblioref">2021</a>)</span> found that reducing RoBERTa embeddings to 64 dimensions using PCA is sometimes better and never worse than using them raw (at least when mapping them to psychological characteristics using machine learning techniques; see <a href="navigating-vectorspace.html#sec-machine-learning-methods" class="quarto-xref"><span>Section 20.2.4</span></a>). This may be because PCA recenters the embeddings and emphasizes the salient differences between the documents of a particular dataset, which are otherwise fairly similar to each other. To reduce the dimensionality of your embeddings, use the function provided here:</p>
<div class="cell">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># `data`: a dataframe with one embedding per row</span></span>
<span><span class="co"># `cols`: tidyselect - columns that contain numeric embedding values</span></span>
<span><span class="co"># `reduce_to`: number of dimensions to keep</span></span>
<span><span class="co"># `scale`: perform scaling in addition to centering?</span></span>
<span><span class="va">reduce_dimensionality</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">data</span>, <span class="va">cols</span>, <span class="va">reduce_to</span>, <span class="va">scale</span> <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">in_dat</span> <span class="op">&lt;-</span> <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">data</span>, <span class="op">{</span><span class="op">{</span> <span class="va">cols</span> <span class="op">}</span><span class="op">}</span><span class="op">)</span></span>
<span>  <span class="va">pca</span> <span class="op">&lt;-</span> <span class="fu">stats</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html">prcomp</a></span><span class="op">(</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">in_dat</span>, scale <span class="op">=</span> <span class="va">scale</span>, rank. <span class="op">=</span> <span class="va">reduce_to</span><span class="op">)</span></span>
<span>  <span class="va">out_dat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span><span class="op">(</span><span class="va">pca</span><span class="op">$</span><span class="va">x</span><span class="op">)</span></span>
<span>  <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/bind_cols.html">bind_cols</a></span><span class="op">(</span> <span class="fu">select</span><span class="op">(</span><span class="va">data</span>, <span class="op">-</span><span class="op">{</span><span class="op">{</span> <span class="va">cols</span> <span class="op">}</span><span class="op">}</span><span class="op">)</span>, <span class="va">out_dat</span> <span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># reduce dimensionality of SBERT embeddings from 384 to 64</span></span>
<span><span class="va">hippocorpus_sbert_64d</span> <span class="op">&lt;-</span> <span class="fu">reduce_dimensionality</span><span class="op">(</span></span>
<span>  <span class="va">hippocorpus_sbert</span>, </span>
<span>  <span class="va">Dim1</span><span class="op">:</span><span class="va">Dim384</span>, </span>
<span>  reduce_to <span class="op">=</span> <span class="fl">64</span></span>
<span>  <span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Dimensionality reduction can also be useful for visualizing your embeddings. Let’s plot our Hippocampus texts by reducing their 768-dimensional embeddings to 2 dimensions that can be mapped to the x and y axis.</p>
<div class="cell">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># reduce dimensionality of SBERT embeddings from 384 to 2</span></span>
<span><span class="va">hippocorpus_sbert_2d</span> <span class="op">&lt;-</span> <span class="fu">reduce_dimensionality</span><span class="op">(</span></span>
<span>  <span class="va">hippocorpus_bert</span>, </span>
<span>  <span class="va">Dim1</span><span class="op">:</span><span class="va">Dim384</span>, </span>
<span>  reduce_to <span class="op">=</span> <span class="fl">2</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># plot</span></span>
<span><span class="va">hippocorpus_sbert_2d</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">PC1</span>, <span class="va">PC2</span>, color <span class="op">=</span> <span class="va">memType</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">geom_point</span><span class="op">(</span>alpha <span class="op">=</span> <span class="fl">.8</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">scale_color_brewer</span><span class="op">(</span>palette <span class="op">=</span> <span class="st">"Paired"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">labs</span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Hippocorpus Story Embeddings, Reduced Dimensionality"</span>,</span>
<span>         color <span class="op">=</span> <span class="st">"Autobiographical\nStory Type"</span>,</span>
<span>         x <span class="op">=</span> <span class="st">"Principle Component 1"</span>,</span>
<span>         y <span class="op">=</span> <span class="st">"Principle Component 2"</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">theme_minimal</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="contextualized-embeddings_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="768"></p>
</figure>
</div>
</div>
</div>
<p>The first two PCA components are the directions along which the embeddings in the dataset spread out the most. There do seem to be slightly more imagined stories in the bottom right, but otherwise story type seems mostly unrelated to these dimensions. This means that the main ways in which the stories are different from one another are not the ways that imagined, recalled, and retold stories are different from one another. This makes sense—the stories are about all sorts of events, and may even use slightly different dialects of English. The differences that we are interested in are more subtle.</p>
<hr>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advantages of Contextualized Embeddings
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Can Represent Multiple Senses of Each Word</strong></li>
<li>
<strong>Sensitive to Word Order and Negation:</strong> LLMs can tell the difference between “he’s funny but not smart” and “he’s smart but not funny.”</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Disadvantages of Contextualized Embeddings
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Computationally Expensive</strong></li>
<li>
<strong>Mysterious Vector Spaces:</strong> Unlike simple word embedding models (<a href="decontextualized-embeddings.html#sec-word-embeddings" class="quarto-xref"><span>Section 18.3</span></a>), LLMs may sometimes organize embeddings in nonlinear patterns. For example, GPT models tend to arrange their embeddings in a spiral <span class="citation" data-cites="cai_etal_2021">(<a href="#ref-cai_etal_2021" role="doc-biblioref">Cai et al., 2021</a>)</span>. Specialized models like <a href="https://www.sbert.net/docs/pretrained_models.html">SBERT</a> can help with this problem, but are unlikely to solve it entirely.</li>
</ul>
</div>
</div>
<hr>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-cai_etal_2021" class="csl-entry" role="listitem">
Cai, X., Huang, J., Bian, Y., &amp; Church, K. (2021). Isotropy in the contextual embedding space: Clusters and manifolds. <em>International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=xYGNO86OWDH">https://openreview.net/forum?id=xYGNO86OWDH</a>
</div>
<div id="ref-devlin_etal_2019" class="csl-entry" role="listitem">
Devlin, J., Chang, M.-W., Lee, K., &amp; Toutanova, K. (2019). <em>BERT: Pre-training of deep bidirectional transformers for language understanding</em>. <a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a>
</div>
<div id="ref-ethayarajh_etal_2019" class="csl-entry" role="listitem">
Ethayarajh, K., Duvenaud, D., &amp; Hirst, G. (2019). <em>Towards understanding linear word analogies</em>. <a href="https://arxiv.org/abs/1810.04882">https://arxiv.org/abs/1810.04882</a>
</div>
<div id="ref-ganesan_etal_2021" class="csl-entry" role="listitem">
Ganesan, A., Matero, M., Ravula, A. R., Vu, H., &amp; Schwartz, H. A. (2021). Empirical evaluation of pre-trained transformers for human-level NLP: The role of sample size and dimensionality. <em>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>. <a href="https://doi.org/10.18653/v1/2021.naacl-main.357">https://doi.org/10.18653/v1/2021.naacl-main.357</a>
</div>
<div id="ref-ji_etal_2021" class="csl-entry" role="listitem">
Ji, S., Zhang, T., Ansari, L., Fu, J., Tiwari, P., &amp; Cambria, E. (2021). <em>MentalBERT: Publicly available pretrained language models for mental healthcare</em>. <a href="https://arxiv.org/abs/2110.15621">https://arxiv.org/abs/2110.15621</a>
</div>
<div id="ref-kjell_etal_2021" class="csl-entry" role="listitem">
Kjell, O., Giorgi, S., &amp; Schwartz, H. A. (2021). <em>The text-package: An r-package for analyzing and visualizing human language using natural language processing and deep learning</em>. PsyArXiv. <a href="https://doi.org/10.31234/osf.io/293kt">https://doi.org/10.31234/osf.io/293kt</a>
</div>
<div id="ref-kjell_etal_2022" class="csl-entry" role="listitem">
Kjell, O., Sikström, S., Kjell, K., &amp; Schwartz, H. (2022). Natural language analyzed with AI-based transformers predict traditional subjective well-being measures approaching the theoretical upper limits in accuracy. <em>Scientific Reports</em>, <em>12</em>, 3918. <a href="https://doi.org/10.1038/s41598-022-07520-w">https://doi.org/10.1038/s41598-022-07520-w</a>
</div>
<div id="ref-li_etal_2020" class="csl-entry" role="listitem">
Li, B., Zhou, H., He, J., Wang, M., Yang, Y., &amp; Li, L. (2020). <em>On the sentence embeddings from pre-trained language models</em>. <a href="https://arxiv.org/abs/2011.05864">https://arxiv.org/abs/2011.05864</a>
</div>
<div id="ref-liu_etal_2019" class="csl-entry" role="listitem">
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., &amp; Stoyanov, V. (2019). <em>RoBERTa: A robustly optimized BERT pretraining approach</em>. <a href="https://arxiv.org/abs/1907.11692">https://arxiv.org/abs/1907.11692</a>
</div>
<div id="ref-matero_etal_2022" class="csl-entry" role="listitem">
Matero, M., Hung, A., &amp; Schwartz, H. A. (2022). <em>Evaluating contextual embeddings and their extraction layers for depression assessment</em>. <a href="https://arxiv.org/abs/2112.13795">https://arxiv.org/abs/2112.13795</a>
</div>
<div id="ref-reif_etal_2019" class="csl-entry" role="listitem">
Reif, E., Yuan, A., Wattenberg, M., Viegas, F. B., Coenen, A., Pearce, A., &amp; Kim, B. (2019). Visualizing and measuring the geometry of BERT. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlché-Buc, E. Fox, &amp; R. Garnett (Eds.), <em>Advances in neural information processing systems</em> (Vol. 32). Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/159c1ffe5b61b41b3c4d8f4c2150f6c4-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2019/file/159c1ffe5b61b41b3c4d8f4c2150f6c4-Paper.pdf</a>
</div>
<div id="ref-reimers_gurevych_2019" class="csl-entry" role="listitem">
Reimers, N., &amp; Gurevych, I. (2019). Sentence-<span>BERT</span>: Sentence embeddings using <span>S</span>iamese <span>BERT</span>-networks. In K. Inui, J. Jiang, V. Ng, &amp; X. Wan (Eds.), <em>Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)</em> (pp. 3982–3992). Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D19-1410">https://doi.org/10.18653/v1/D19-1410</a>
</div>
<div id="ref-sanh_etal_2020" class="csl-entry" role="listitem">
Sanh, V., Debut, L., Chaumond, J., &amp; Wolf, T. (2020). <em>DistilBERT, a distilled version of BERT: Smaller, faster, cheaper and lighter</em>. <a href="https://arxiv.org/abs/1910.01108">https://arxiv.org/abs/1910.01108</a>
</div>
</div>
</section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr>
<ol>
<li id="fn1"><p>Nevertheless, using <code>[CLS]</code> embeddings is slightly inferior to averaging the contextualized embeddings across tokens <span class="citation" data-cites="reimers_gurevych_2019">(<a href="#ref-reimers_gurevych_2019" role="doc-biblioref">Reimers &amp; Gurevych, 2019</a>)</span>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/ds4psych\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./decontextualized-embeddings.html" class="pagination-link" aria-label="Word Embeddings">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Word Embeddings</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./navigating-vectorspace.html" class="pagination-link" aria-label="Navigating Vector Space">
        <span class="nav-page-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Navigating Vector Space</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>Data Science for Psychology was written by <a href="https://rimonim.github.io">Louis Teitelbaum</a> and <a href="https://almogsi.com">Almog Simchon</a>.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/rimonim/ds4psych/blob/main/contextualized-embeddings.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/rimonim/ds4psych/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a> and is powered by <a href="https://www.netlify.com/">Netlify</a></p>
</div>
  </div>
</footer>


</body></html>