<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Louis Teitelbaum and Almog Simchon">
<title>Data Science for Psychology: Natural Language - 20&nbsp; Navigating Vector Space</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./lda.html" rel="next">
<link href="./contextualized-embeddings.html" rel="prev">
<link href="./images/favicon.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><style>
.r-output code {
  word-break: break-wor !important;
  white-space: pre-wrap !important;
}
</style>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./quantification.html">Quantifying Psychological Properties of Text</a></li><li class="breadcrumb-item"><a href="./navigating-vectorspace.html"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Navigating Vector Space</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Data Science for Psychology: Natural Language</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/rimonim/ds4psych" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
<li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=%7Curl%7C">
              <i class="bi bi-bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=%7Curl%7C">
              <i class="bi bi-bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=%7Curl%7C">
              <i class="bi bi-bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
      </ul>
</div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction: Why Does Psychology Need Natural Language?</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ethics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Ethics of Data Science in Psychology</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./data-viz.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Visualization</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./aesthetics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Why Aesthetic Choices are Important</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./telling-a-story.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Don’t Distract From the Story</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./word-viz.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Visualizing Distributions of Words</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data-viz-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Additional Resources for Data Visualization</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./data-sources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sources of Data</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./experiments.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Experiments</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./corpora.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Corpus Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./apis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Web APIs</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./scraping.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Web Scraping</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./quantification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quantifying Psychological Properties of Text</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./look-at-your-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Look at Your Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quanteda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Introduction to Quanteda</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tokenization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Tokenization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./word-counting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Dictionary-Based Word Counts</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dla.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Open Vocabulary Word Counting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./word-counting-improvements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Transforming Word Counts</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./vectorspace-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Introduction to Vector Space</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./decontextualized-embeddings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Word Embeddings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./contextualized-embeddings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Contextualization With Large Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./navigating-vectorspace.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Navigating Vector Space</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Topic Modeling</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linguistic-complexity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Measures of Linguistic Complexity</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./querying_llms.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Just Ask an LLM</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./audio-video-image.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Audio, Video, and Image Data</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./transcription.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Transcribing Audio</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Additional Resources</span></span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#representing-psychological-constructs" id="toc-representing-psychological-constructs" class="nav-link active" data-scroll-target="#representing-psychological-constructs"><span class="header-section-number">20.1</span> Representing Psychological Constructs</a>
  <ul class="collapse">
<li><a href="#distributed-dictionary-representation-ddr" id="toc-distributed-dictionary-representation-ddr" class="nav-link" data-scroll-target="#distributed-dictionary-representation-ddr"><span class="header-section-number">20.1.1</span> Distributed Dictionary Representation (DDR)</a></li>
  <li><a href="#contextualized-construct-representation-ccr" id="toc-contextualized-construct-representation-ccr" class="nav-link" data-scroll-target="#contextualized-construct-representation-ccr"><span class="header-section-number">20.1.2</span> Contextualized Construct Representation (CCR)</a></li>
  <li><a href="#correlational-methods" id="toc-correlational-methods" class="nav-link" data-scroll-target="#correlational-methods"><span class="header-section-number">20.1.3</span> Correlational Methods</a></li>
  </ul>
</li>
  <li>
<a href="#reasoning-in-vector-space-beyond-cosine-similarity" id="toc-reasoning-in-vector-space-beyond-cosine-similarity" class="nav-link" data-scroll-target="#reasoning-in-vector-space-beyond-cosine-similarity"><span class="header-section-number">20.2</span> Reasoning in Vector Space: Beyond Cosine Similarity</a>
  <ul class="collapse">
<li><a href="#sec-parallelograms" id="toc-sec-parallelograms" class="nav-link" data-scroll-target="#sec-parallelograms"><span class="header-section-number">20.2.1</span> Parallelograms</a></li>
  <li><a href="#sec-advanced-similarity" id="toc-sec-advanced-similarity" class="nav-link" data-scroll-target="#sec-advanced-similarity"><span class="header-section-number">20.2.2</span> Advanced Similarity Measures</a></li>
  <li><a href="#sec-dimension-projection" id="toc-sec-dimension-projection" class="nav-link" data-scroll-target="#sec-dimension-projection"><span class="header-section-number">20.2.3</span> Semantic Projection</a></li>
  <li><a href="#sec-machine-learning-methods" id="toc-sec-machine-learning-methods" class="nav-link" data-scroll-target="#sec-machine-learning-methods"><span class="header-section-number">20.2.4</span> Machine Learning Methods</a></li>
  </ul>
</li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/rimonim/ds4psych/blob/main/navigating-vectorspace.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/rimonim/ds4psych/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./quantification.html">Quantifying Psychological Properties of Text</a></li><li class="breadcrumb-item"><a href="./navigating-vectorspace.html"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Navigating Vector Space</span></a></li></ol></nav><div class="quarto-title">
<h1 class="title"><span id="sec-navigating-vectorspace" class="quarto-section-identifier"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Navigating Vector Space</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><div class="callout callout-style-simple callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This page is still under construction. Come back soon!
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<section id="representing-psychological-constructs" class="level2" data-number="20.1"><h2 data-number="20.1" class="anchored" data-anchor-id="representing-psychological-constructs">
<span class="header-section-number">20.1</span> Representing Psychological Constructs</h2>
<p>In <a href="decontextualized-embeddings.html" class="quarto-xref"><span>Chapter 18</span></a> we measured the surprise in texts by comparing their embeddings to that of a single word: “surprised”. But does the embedding of the word “surprised” fully capture the concept of surprise as an emotion? Faced with this question of construct validity, we have two options:</p>
<ol type="1">
<li>
<strong>Conduct a Validation Study:</strong> We could find or construct a dataset of texts that were rated by a human (or ideally, multiple humans) on the extent to which they reflect the emotion of surprise. We could then compare our embedding-based surprise scores to the human rating and demonstrate that they correlate strongly. We could further note areas of disagreement between the human and embedding-based measures and investigate whether these reflect a difference between the constructs they are measuring.</li>
<li>
<strong>Use an Already-Validated Construct Definition:</strong> Properly validating a new measure is hard work. When possible, psychology researchers often prefer to use an existing measure that has already been carefully validated in the past. But embeddings are very new to the field, so few if any validated vector representations of constructs are available. As it turns out, this is not a problem—any language-based psychological measure can be represented as a vector!</li>
</ol>
<section id="distributed-dictionary-representation-ddr" class="level3" data-number="20.1.1"><h3 data-number="20.1.1" class="anchored" data-anchor-id="distributed-dictionary-representation-ddr">
<span class="header-section-number">20.1.1</span> Distributed Dictionary Representation (DDR)</h3>
<p>Let’s begin with a straightforward sort of psychological measure—the dictionary. We have already discussed dictionaries extensively in <a href="word-counting.html" class="quarto-xref"><span>Chapter 14</span></a> and noted that psychology researchers have been constructing, validating, and publicizing dictionaries for decades (<a href="word-counting.html#sec-dictionary-sources" class="quarto-xref"><span>Section 14.6</span></a>). But these dictionaries are designed for word counting—How do we apply them to a vector-based analysis? <span class="citation" data-cites="garten_etal_2018">Garten et al. (<a href="#ref-garten_etal_2018" role="doc-biblioref">2018</a>)</span> propose a simple solution: Get word embeddings (<a href="decontextualized-embeddings.html#sec-word-embeddings" class="quarto-xref"><span>Section 18.3</span></a>) for each word in the dictionary, and average them together to create a single Distributed Dictionary Representation (DDR). The dictionary construct can then be measured by comparing text embeddings to the DDR.</p>
<p>DDR cannot entirely replace word counts; for linguistic concepts like pronoun use or the passive voice, dictionary-based word counts are still necessary. But DDR is ideal for studies of abstract constructs like emotions, that refer to the general gist of a text rather than particular words. The rich representations of word embeddings allow DDR to capture even the subtlest associations between words and constructs, and to precisely reflect the <em>extent</em> to which the words are associated with that construct. It can do this even for texts that do not contain any dictionary words. Because embeddings are continuous and already calibrated to the probabilities of word use in language, DDR also avoids the difficult statistical problems that arise due to the strange distributions of word counts (<a href="word-counting-improvements.html" class="quarto-xref"><span>Chapter 16</span></a>).</p>
<p><span class="citation" data-cites="garten_etal_2018">Garten et al. (<a href="#ref-garten_etal_2018" role="doc-biblioref">2018</a>)</span> found that DDR works best with smaller dictionaries of only the words most directly connected to the construct being measured (around 30 words worked best in their experiments). The diminished effectiveness of longer dictionaries is likely due to the properties of word embeddings—while the <em>direction</em> of a word embedding represents its meaning (i.e.&nbsp;its average context), the <em>magnitude</em> of a word embedding (i.e.&nbsp;how far it would move another vector if the two were added together) represents how specific it is to that context (<a href="decontextualized-embeddings.html#sec-embedding-magnitude" class="quarto-xref"><span>Section 18.3.4</span></a>). This is why an accurate embedding of a full text can be obtained by averaging the embeddings of each of its words—the embeddings automatically devalue frequent and uninformative words, and emphasize the words that are representative of the text’s specific meaning <span class="citation" data-cites="ethayarajh_etal_2019">(<a href="#ref-ethayarajh_etal_2019" role="doc-biblioref">Ethayarajh et al., 2019</a>)</span>. Overvaluing informative words is a desirable property for raw texts, in which uninformative words tend to be very frequent. But dictionaries only include one of each word. In longer dictionaries with more infrequent, tangentially connected words, averaging word embeddings will therefore <em>overvalue</em> those infrequent words and skew the DDR. This can be fixed with Garten et al.’s method of picking out only the most informative words. Alternatively, it could be fixed by measuring the frequency of each dictionary word in a corpus and weighting the average embedding by that frequency. This method is actually more consistent with the way most dictionaries are validated, by counting the frequencies of dictionary words in text (<a href="word-counting.html" class="quarto-xref"><span>Chapter 14</span></a>).</p>
<p>Let’s measure surprise in the Hippocorpus texts by computing a DDR of the NRC Word-Emotion Association Lexicon <span class="citation" data-cites="mohammad_turney_2010 mohammad_turney_2013">(<a href="#ref-mohammad_turney_2013" role="doc-biblioref">S. M. Mohammad &amp; Turney, 2013</a>; <a href="#ref-mohammad_turney_2010" role="doc-biblioref">S. Mohammad &amp; Turney, 2010</a>)</span> which we used in <a href="word-counting.html" class="quarto-xref"><span>Chapter 14</span></a>. To correct for word informativeness, we will weight the dictionary word embeddings by their frequency in the corpus.</p>
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># load surprise dictionary</span></span>
<span><span class="va">surprise_dict</span> <span class="op">&lt;-</span> <span class="fu">quanteda.sentiment</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/quanteda.sentiment/man/data_dictionary_NRC.html">data_dictionary_NRC</a></span><span class="op">[</span><span class="st">"surprise"</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># estimate frequency of dictionary words</span></span>
<span><span class="va">surprise_dict_freqs</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_dfm</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">dfm_keep</span><span class="op">(</span><span class="va">surprise_dict</span><span class="op">$</span><span class="va">surprise</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">quanteda.textstats</span><span class="fu">::</span><span class="fu"><a href="https://quanteda.io/reference/textstat_frequency.html">textstat_frequency</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="va">feature</span>, <span class="va">frequency</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># word2vec embeddings of dictionary words</span></span>
<span><span class="va">surprise_ddr</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">word2vec_mod</span>, <span class="va">surprise_dict</span><span class="op">$</span><span class="va">surprise</span>, type <span class="op">=</span> <span class="st">"embedding"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">as_tibble</span><span class="op">(</span>rownames <span class="op">=</span> <span class="st">"feature"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">left_join</span><span class="op">(</span><span class="va">surprise_dict_freqs</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">replace_na</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>frequency <span class="op">=</span> <span class="fl">0</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># average dictionary embedding (weighted by frequency)</span></span>
<span><span class="va">surprise_ddr</span> <span class="op">&lt;-</span> <span class="va">surprise_ddr</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">summarise</span><span class="op">(</span><span class="fu">across</span><span class="op">(</span><span class="va">V1</span><span class="op">:</span><span class="va">V300</span>, <span class="op">~</span><span class="fu"><a href="https://rdrr.io/r/stats/weighted.mean.html">weighted.mean</a></span><span class="op">(</span><span class="va">.x</span>, <span class="va">frequency</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="va">V1</span><span class="op">:</span><span class="va">V300</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/unlist.html">unlist</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># document embeddings</span></span>
<span><span class="va">hippocorpus_word2vec</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_dfm</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">textstat_embedding</span><span class="op">(</span><span class="va">word2vec_mod</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># score documents by surprise</span></span>
<span><span class="va">hippocorpus_surprise_ddr</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_word2vec</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">rowwise</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span></span>
<span>    surprise <span class="op">=</span> <span class="fu">cos_sim</span><span class="op">(</span><span class="fu">c_across</span><span class="op">(</span><span class="va">V1</span><span class="op">:</span><span class="va">V300</span><span class="op">)</span>, <span class="va">surprise_ddr</span><span class="op">)</span>,</span>
<span>    <span class="co"># transform cosine similarity to stay between 0 and 1</span></span>
<span>    surprise <span class="op">=</span> <span class="va">surprise</span><span class="op">/</span><span class="fl">2</span> <span class="op">+</span> <span class="fl">1</span><span class="op">/</span><span class="fl">2</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">ungroup</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">V1</span><span class="op">:</span><span class="va">V300</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># rejoin docvars</span></span>
<span><span class="va">hippocorpus_surprise_ddr</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_surprise_ddr</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">bind_cols</span><span class="op">(</span><span class="fu">docvars</span><span class="op">(</span><span class="va">hippocorpus_corp</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>With the new measure of surprise, we can retest the hypothesis that true autobiographical stories include more surprise than imagined stories.</p>
<div class="cell">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># logistic regression</span></span>
<span><span class="va">surprise_mod_ddr</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span></span>
<span>  <span class="va">surprise</span> <span class="op">~</span> <span class="va">memType</span>, </span>
<span>  family <span class="op">=</span> <span class="va">binomial</span>,</span>
<span>  <span class="va">hippocorpus_surprise_ddr</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">surprise_mod_ddr</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="r-output"><code>#&gt; 
#&gt; Call:
#&gt; glm(formula = surprise ~ memType, family = binomial, data = hippocorpus_surprise_ddr)
#&gt; 
#&gt; Coefficients:
#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)    
#&gt; (Intercept)      1.81235    0.05484  33.049    memTyperecalled -0.02041    0.07711  -0.265    0.791    
#&gt; memTyperetold   -0.02250    0.09587  -0.235    0.814    
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; (Dispersion parameter for binomial family taken to be 1)
#&gt; 
#&gt;     Null deviance: 6.9881  on 6853  degrees of freedom
#&gt; Residual deviance: 6.8986  on 6851  degrees of freedom
#&gt; AIC: 2103.6
#&gt; 
#&gt; Number of Fisher Scoring iterations: 5
</code></pre>
</div>
<p>We again find no significant difference in surprise between remembered and recalled stories. This is consistent with our results from <a href="word-counting.html" class="quarto-xref"><span>Chapter 14</span></a>, where we tested the same hypothesis with the same dictionary, but used word counts rather than embeddings.</p>
<section id="ddr-for-word-by-word-analysis" class="level4" data-number="20.1.1.1"><h4 data-number="20.1.1.1" class="anchored" data-anchor-id="ddr-for-word-by-word-analysis">
<span class="header-section-number">20.1.1.1</span> DDR For Word-by-Word Analysis</h4>
<p>Another advantage of DDR over dictionary-based word counts is that DDR enables word-by-word analysis of text. It is not very informative to count how many surprise words are in each word (it will either be one or zero), but we can compare the embedding of each word to the surprise DDR. This allows us to see how a construct spreads out within a single text. As an example, let’s take a single story from the Hippocorpus:</p>
<div class="cell">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># full text as string</span></span>
<span><span class="va">story</span> <span class="op">&lt;-</span> <span class="fu">word</span><span class="op">(</span><span class="va">hippocorpus_df</span><span class="op">$</span><span class="va">story</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span>, end <span class="op">=</span> <span class="fl">140L</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="va">story</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="r-output"><code>#&gt; It seems just like yesterday but today makes five months ago it happened. I had been watching my phone like an owl for the past week. I was waiting for a work related call that my team was waiting for to close a important deal. It wasnt the call I expected though. It was for  my sister was in labor with the twins. My sister is only 7 months pregnant. I got the call shortly after arriving at work. Just as fast I was back out the door and on my way to the hospital. When I arrived my sister had just delivered and I just was in awe. Even though they were a bit small they were mighty. They were the most precious things I had ever seen. I held my niece and nephew and couldnt stop crying.
</code></pre>
</div>
<p>To visualize surprise within this text, we can separate it into words and find the embedding of each word. Rather than averaging all of these embeddings together to get the embedding of the full text, we can compute a rolling average, averaging each word’s embedding with those of its neighbors.</p>
<div class="cell">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># separate into vector of tokens</span></span>
<span><span class="va">story</span> <span class="op">&lt;-</span> <span class="fu">word</span><span class="op">(</span><span class="va">hippocorpus_df</span><span class="op">$</span><span class="va">story</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span>, end <span class="op">=</span> <span class="fl">140L</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">tokens</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/base/character.html">as.character</a></span><span class="op">(</span><span class="op">)</span></span>
<span>  </span>
<span><span class="co"># rolling average of embeddings</span></span>
<span><span class="va">story_surprise</span> <span class="op">&lt;-</span> <span class="fu">as_tibble</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">word2vec_mod</span>, <span class="va">story</span>, type <span class="op">=</span> <span class="st">"embedding"</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span></span>
<span>    <span class="fu">across</span><span class="op">(</span></span>
<span>      <span class="va">V1</span><span class="op">:</span><span class="va">V300</span>, </span>
<span>      <span class="op">~</span><span class="fu">zoo</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/zoo/man/rollapply.html">rollapply</a></span><span class="op">(</span></span>
<span>        <span class="va">.x</span>, <span class="fl">4</span>, <span class="va">mean</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span>, </span>
<span>        align <span class="op">=</span> <span class="st">"center"</span>,</span>
<span>        fill <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">.x</span>, <span class="fl">1</span><span class="op">)</span>, <span class="cn">NA</span>, <span class="fu"><a href="https://rdrr.io/r/utils/head.html">tail</a></span><span class="op">(</span><span class="va">.x</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span>        <span class="op">)</span></span>
<span>      <span class="op">)</span></span>
<span>    <span class="op">)</span></span>
<span>  </span>
<span><span class="co"># vector of computed surprise (cosine similarity)</span></span>
<span><span class="va">story_surprise</span> <span class="op">&lt;-</span> <span class="va">story_surprise</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">rowwise</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>surprise <span class="op">=</span> <span class="fu">cos_sim</span><span class="op">(</span><span class="fu">c_across</span><span class="op">(</span><span class="va">V1</span><span class="op">:</span><span class="va">V300</span><span class="op">)</span>, <span class="va">surprise_ddr</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">pull</span><span class="op">(</span><span class="va">surprise</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now visualize the surprise in each word of the text. Since <code>ggplot2</code> makes it difficult to plot dynamically colored text in one continuous chunk, we will use ANSI color codes to print the our text directly to the console.</p>
<div class="cell">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># (see https://www.hackitu.de/termcolor256/ for info on ANSI colors)</span></span>
<span><span class="co"># blue-red heat scale</span></span>
<span><span class="va">ansi_scale</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span></span>
<span>  <span class="fl">063</span>, <span class="fl">105</span>, <span class="fl">147</span>, <span class="fl">189</span>, <span class="fl">188</span>, <span class="fl">230</span>, <span class="fl">223</span>, </span>
<span>  <span class="fl">224</span>, <span class="fl">217</span>, <span class="fl">210</span>, <span class="fl">203</span>, <span class="fl">196</span>, <span class="fl">160</span>, <span class="fl">124</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># turn scale value into ANSI color code</span></span>
<span><span class="va">map_to_ansi</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">ansi_scale</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">x_new</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">x</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span><span class="op">*</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">ansi_scale</span><span class="op">)</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/diff.html">diff</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/range.html">range</a></span><span class="op">(</span><span class="va">x</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="fl">1</span></span>
<span>  <span class="va">x_new</span></span>
<span>  <span class="va">ansi_scale</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">x_new</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="va">story_surprise</span> <span class="op">&lt;-</span> <span class="fu">map_to_ansi</span><span class="op">(</span><span class="va">story_surprise</span>, <span class="va">ansi_scale</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># print</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">story_surprise</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="kw">if</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">story_surprise</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="va">story</span><span class="op">[</span><span class="va">i</span><span class="op">]</span>, <span class="st">" "</span><span class="op">)</span></span>
<span>  <span class="op">}</span><span class="kw">else</span><span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"\033[48;5;"</span>, <span class="va">story_surprise</span><span class="op">[</span><span class="va">i</span><span class="op">]</span>, <span class="st">"m"</span>, <span class="va">story</span><span class="op">[</span><span class="va">i</span><span class="op">]</span>, <span class="st">" \033[0m"</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="r-output"><code>#&gt; <span style="background-color: #D7D7FF;">It </span><span style="background-color: #AF0000;">seems just </span>like  <span style="background-color: #FF5F5F;">yesterday </span><span style="background-color: #FFD7D7;">but </span><span style="background-color: #FF8787;">today </span><span style="background-color: #D7D7D7;">makes </span><span style="background-color: #D7D7FF;">five </span><span style="background-color: #FFFFD7;">months </span><span style="background-color: #FFAFAF;">ago it </span><span style="background-color: #FF0000;">happened </span><span style="background-color: #FF8787;">. </span><span style="background-color: #FFD7AF;">I </span><span style="background-color: #FF8787;">had been watching </span><span style="background-color: #FF0000;">my </span><span style="background-color: #FF8787;">phone </span><span style="background-color: #FFD7D7;">like </span><span style="background-color: #FFAFAF;">an </span><span style="background-color: #D7D7D7;">owl </span><span style="background-color: #FFFFD7;">for the </span><span style="background-color: #D7D7D7;">past </span><span style="background-color: #FFD7D7;">week </span><span style="background-color: #FFAFAF;">. </span><span style="background-color: #FF0000;">I was </span><span style="background-color: #FFD7D7;">waiting </span><span style="background-color: #FF8787;">for </span><span style="background-color: #D7D7FF;">a </span><span style="background-color: #AFAFFF;">work </span><span style="background-color: #FFD7AF;">related </span><span style="background-color: #FFAFAF;">call </span><span style="background-color: #D70000;">that </span><span style="background-color: #FF0000;">my </span><span style="background-color: #D70000;">team </span><span style="background-color: #FF8787;">was </span><span style="background-color: #FFD7D7;">waiting </span><span style="background-color: #FF8787;">for </span><span style="background-color: #FFFFD7;">to </span><span style="background-color: #FFD7D7;">close </span><span style="background-color: #FF8787;">a </span><span style="background-color: #FFD7D7;">important </span><span style="background-color: #FF8787;">deal </span><span style="background-color: #FFFFD7;">. </span><span style="background-color: #FFD7D7;">It wasnt </span><span style="background-color: #FF8787;">the </span><span style="background-color: #FF5F5F;">call </span><span style="background-color: #FF0000;">I </span><span style="background-color: #FF5F5F;">expected </span><span style="background-color: #FFAFAF;">though . </span><span style="background-color: #FFD7AF;">It </span><span style="background-color: #FF8787;">was </span><span style="background-color: #FFAFAF;">for my sister </span><span style="background-color: #FFFFD7;">was </span><span style="background-color: #D7D7D7;">in labor </span><span style="background-color: #FFD7AF;">with the </span><span style="background-color: #FFD7D7;">twins </span><span style="background-color: #D7D7D7;">. My </span><span style="background-color: #FFD7D7;">sister </span><span style="background-color: #FFFFD7;">is only 7 </span><span style="background-color: #5F5FFF;">months </span><span style="background-color: #FFD7AF;">pregnant </span><span style="background-color: #FF0000;">. </span><span style="background-color: #D70000;">I got </span><span style="background-color: #FF5F5F;">the </span><span style="background-color: #FFD7AF;">call shortly </span><span style="background-color: #FFFFD7;">after </span><span style="background-color: #FFAFAF;">arriving </span><span style="background-color: #FFD7AF;">at </span><span style="background-color: #FFD7D7;">work . </span><span style="background-color: #FFD7AF;">Just </span><span style="background-color: #FF8787;">as fast </span><span style="background-color: #FF0000;">I </span><span style="background-color: #FF5F5F;">was </span><span style="background-color: #FF8787;">back </span><span style="background-color: #FFD7D7;">out </span><span style="background-color: #FFFFD7;">the </span><span style="background-color: #D7D7FF;">door </span><span style="background-color: #FFFFD7;">and </span><span style="background-color: #FF8787;">on my </span><span style="background-color: #FF5F5F;">way </span><span style="background-color: #FF8787;">to </span><span style="background-color: #D7D7D7;">the </span><span style="background-color: #FFD7AF;">hospital </span><span style="background-color: #FFAFAF;">. </span><span style="background-color: #FF5F5F;">When </span><span style="background-color: #FF0000;">I </span><span style="background-color: #FF5F5F;">arrived </span><span style="background-color: #FF8787;">my </span><span style="background-color: #D70000;">sister </span><span style="background-color: #FF5F5F;">had </span><span style="background-color: #FF8787;">just </span><span style="background-color: #FF0000;">delivered and </span><span style="background-color: #D70000;">I just was </span><span style="background-color: #FFAFAF;">in awe . Even </span><span style="background-color: #FFD7D7;">though they </span><span style="background-color: #FFAFAF;">were </span><span style="background-color: #FFD7D7;">a </span><span style="background-color: #FF5F5F;">bit </span><span style="background-color: #FF8787;">small </span><span style="background-color: #FFD7D7;">they were </span><span style="background-color: #FFFFD7;">mighty . </span><span style="background-color: #D7D7D7;">They </span><span style="background-color: #FFFFD7;">were </span><span style="background-color: #FFD7AF;">the </span><span style="background-color: #FF5F5F;">most </span><span style="background-color: #FF0000;">precious </span><span style="background-color: #AF0000;">things </span><span style="background-color: #FF0000;">I </span><span style="background-color: #FFAFAF;">had </span><span style="background-color: #FFFFD7;">ever </span><span style="background-color: #FFD7D7;">seen </span><span style="background-color: #FFD7AF;">. </span><span style="background-color: #FFD7D7;">I </span><span style="background-color: #FF8787;">held </span><span style="background-color: #FFD7AF;">my </span><span style="background-color: #FFFFD7;">niece </span><span style="background-color: #8787FF;">and </span><span style="background-color: #D7D7D7;">nephew </span><span style="background-color: #FFD7AF;">and </span><span style="background-color: #FFAFAF;">couldnt stop </span>crying  .
</code></pre>
</div>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advantages of DDR
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Richer, More Robust Construct Representation Than Word Counting</strong></li>
<li><strong>Avoids Statistical Problems With Word Count Distributions</strong></li>
<li><strong>Enables Word-by-Word Analysis</strong></li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Disadvantages of DDR
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>
<strong>Can Implicitly Encode Associated Constructs:</strong> For example, if surprised texts tend to have positive valence in the data used to train the word embedding model, the DDR for surprise may embed some positive valence as well. This can be remedied by constructing a DDR for positive valence as well, and using it as a statistical control when testing hypotheses.</li>
<li>
<strong>May Not Work With Contextualized Embeddings:</strong> Even if we assume that contextualized embeddings conform to the geometrical properties associated with word embeddings, LLMs are not designed to embed single words, which is required for DDR.</li>
<li>
<strong>Not Appropriate for Linguistic Measures:</strong> Word embeddings encode the general gist of a text, whereas constructs like passive voice or pronoun use refer to specific words.</li>
</ul>
</div>
</div>
</section></section><section id="contextualized-construct-representation-ccr" class="level3" data-number="20.1.2"><h3 data-number="20.1.2" class="anchored" data-anchor-id="contextualized-construct-representation-ccr">
<span class="header-section-number">20.1.2</span> Contextualized Construct Representation (CCR)</h3>
<p>Dictionaries are not the only validated psychological measures that we can apply using embeddings. With contextualized embeddings, we can extract the gist of any text and compare it to that of any other text. <span class="citation" data-cites="atari_etal_2023">Atari et al. (<a href="#ref-atari_etal_2023" role="doc-biblioref">2023</a>)</span> propose to do this with the most popular form of psychometric scale: the questionnaire. Psychologists have been using questionnaires to measure things for over a century, and tens of thousands of validated questionnaires are now available <a href="https://www.apa.org/pubs/databases/psyctests">online</a>. The LLM embedding of a questionnaire is referred to as a Contextualized Construct Representation (CCR).</p>
<p>We can use CCR to measure surprise in the Hippocorpus texts. For our questionnaire, we will use an adapted version of the surprise scale used in <span class="citation" data-cites="choi_choi_2010">D. Choi &amp; Choi (<a href="#ref-choi_choi_2010" role="doc-biblioref">2010</a>)</span> and <span class="citation" data-cites="choi_nisbett_2000">I. Choi &amp; Nisbett (<a href="#ref-choi_nisbett_2000" role="doc-biblioref">2000</a>)</span>.</p>
<div class="cell">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">surprise_items</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span></span>
<span>  <span class="st">"I was extremely surprised by the outcome of the event."</span>,</span>
<span>  <span class="st">"The outcome of the event was extremely interesting."</span>,</span>
<span>  <span class="st">"The outcome of the event was extremely new."</span></span>
<span>  <span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-style-simple callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Beware of Reverse Coding!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Many questionnaires include reverse-coded items (e.g.&nbsp;“I often feel happy” on a depression questionnaire). The easiest way to deal with these is to manually add negations to flip their meaning (e.g.&nbsp;“I <em>do not</em> often feel happy”).</p>
</div>
</div>
<p>The first step in using CCR is to compute contextualized embeddings for the texts in our dataset. We already did this in <a href="contextualized-embeddings.html" class="quarto-xref"><span>Chapter 19</span></a>. The next step is to compute contextualized embeddings for the items in the questionnaire, and average them to produce a CCR.</p>
<div class="cell">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># embed items (using the same model as we used before)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-text.org/">text</a></span><span class="op">)</span></span>
<span>  </span>
<span><span class="va">surprise_sbert</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://r-text.org/reference/textEmbed.html">textEmbed</a></span><span class="op">(</span></span>
<span>  <span class="va">surprise_items</span>,</span>
<span>  model <span class="op">=</span> <span class="st">"sentence-transformers/all-MiniLM-L12-v2"</span>, <span class="co"># model name</span></span>
<span>  layers <span class="op">=</span> <span class="op">-</span><span class="fl">2</span>,  <span class="co"># second to last layer (default)</span></span>
<span>  tokens_select <span class="op">=</span> <span class="st">"[CLS]"</span>, <span class="co"># use only [CLS] token</span></span>
<span>  dim_name <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  keep_token_embeddings <span class="op">=</span> <span class="cn">FALSE</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># compute CCR by averaging item embeddings</span></span>
<span><span class="va">surprise_ccr</span> <span class="op">&lt;-</span> <span class="va">surprise_sbert</span><span class="op">$</span><span class="va">texts</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">summarise</span><span class="op">(</span><span class="fu">across</span><span class="op">(</span><span class="fu">everything</span><span class="op">(</span><span class="op">)</span>, <span class="va">mean</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/unlist.html">unlist</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now measure surprise in the Hippocorpus texts by computing the cosine similarity between their embeddings and the surprise CCR.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<div class="cell">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># score documents by surprise</span></span>
<span><span class="va">hippocorpus_surprise_ccr</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_sbert</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">rowwise</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span></span>
<span>    surprise <span class="op">=</span> <span class="fu">cos_sim</span><span class="op">(</span><span class="fu">c_across</span><span class="op">(</span><span class="va">Dim1</span><span class="op">:</span><span class="va">Dim384</span><span class="op">)</span>, <span class="va">surprise_ccr</span><span class="op">)</span>,</span>
<span>    <span class="co"># transform cosine similarity to stay between 0 and 1</span></span>
<span>    surprise <span class="op">=</span> <span class="va">surprise</span><span class="op">/</span><span class="fl">2</span> <span class="op">+</span> <span class="fl">1</span><span class="op">/</span><span class="fl">2</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">ungroup</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">Dim1</span><span class="op">:</span><span class="va">Dim384</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># logistic regression</span></span>
<span><span class="va">surprise_mod_ccr</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span></span>
<span>  <span class="va">surprise</span> <span class="op">~</span> <span class="va">memType</span>, </span>
<span>  family <span class="op">=</span> <span class="va">binomial</span>,</span>
<span>  <span class="va">hippocorpus_surprise_ccr</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">surprise_mod_ccr</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="r-output"><code>#&gt; 
#&gt; Call:
#&gt; glm(formula = surprise ~ memType, family = binomial, data = hippocorpus_surprise_ccr)
#&gt; 
#&gt; Coefficients:
#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)    
#&gt; (Intercept)      5.63709    0.32025  17.602    memTyperecalled -0.02168    0.44955  -0.048    0.962    
#&gt; memTyperetold   -0.02781    0.55768  -0.050    0.960    
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; (Dispersion parameter for binomial family taken to be 1)
#&gt; 
#&gt;     Null deviance: 0.57625  on 6853  degrees of freedom
#&gt; Residual deviance: 0.57283  on 6851  degrees of freedom
#&gt; AIC: 55.455
#&gt; 
#&gt; Number of Fisher Scoring iterations: 9
</code></pre>
</div>
<p>Once again we find no significant difference in surprise between remembered and recalled stories. Don’t take this result too seriously though, since CCR has a fundamental problem that needs to be addressed.</p>
<p>Embeddings capture the overall “vibes” of a text, including its tone and dialect. With CCR, we are comparing the “vibes” of a questionnaire written by academics to the “vibes” of narratives written by Hippocorpus participants. By comparing these vectors, we are not just measuring how much surprise is in each text—we are also measuring the extent to which each text is written in the style of a questionnaire written by academics. This introduces a confounding variable into our analysis—questionnaire-ness.</p>
<p>How can we be sure that we are measuring surprise and not questionnaire-ness? We can’t, but there are some methods that might help. We will explore these in <a href="#sec-ccr-improvement" class="quarto-xref"><span>Section 20.2.1.1</span></a> and <a href="#sec-dimension-projection" class="quarto-xref"><span>Section 20.2.3</span></a>.</p>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advantages of CCR
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><hr></li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Disadvantages of CCR
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><hr></li>
</ul>
</div>
</div>
</section><section id="correlational-methods" class="level3" data-number="20.1.3"><h3 data-number="20.1.3" class="anchored" data-anchor-id="correlational-methods">
<span class="header-section-number">20.1.3</span> Correlational Methods</h3>
<p>i.e.&nbsp;averaging from group in training set</p>
</section></section><section id="reasoning-in-vector-space-beyond-cosine-similarity" class="level2" data-number="20.2"><h2 data-number="20.2" class="anchored" data-anchor-id="reasoning-in-vector-space-beyond-cosine-similarity">
<span class="header-section-number">20.2</span> Reasoning in Vector Space: Beyond Cosine Similarity</h2>
<section id="sec-parallelograms" class="level3" data-number="20.2.1"><h3 data-number="20.2.1" class="anchored" data-anchor-id="sec-parallelograms">
<span class="header-section-number">20.2.1</span> Parallelograms</h3>
<p>Introduced with word2vec by <span class="citation" data-cites="mikolov_etal_2013">Mikolov et al. (<a href="#ref-mikolov_etal_2013" role="doc-biblioref">2013</a>)</span></p>
<p>Glove <span class="citation" data-cites="pennington_etal_2014">(<a href="#ref-pennington_etal_2014" role="doc-biblioref">Pennington et al., 2014</a>)</span> is designed with this property in mind. Transformer models are not.</p>
<ul>
<li>Transformer models, including BERT, tend to generate embedding spaces that do not center at zero and which tend to form a narrow cone in the vector space <span class="citation" data-cites="ethayarajh_2019 gao_etal_2019">(<a href="#ref-ethayarajh_2019" role="doc-biblioref">Ethayarajh, 2019</a>; <a href="#ref-gao_etal_2019" role="doc-biblioref">Gao et al., 2019</a>)</span>
<ul>
<li>in BERT, token embeddings in the same sentence become more dissimilar to each other in upper layers but are still more similar than randomly sampled words are on average</li>
<li>BERT aggregated text embeddings perform worse than word2vec and GloVe when analyzed using cosine similarity <span class="citation" data-cites="reimers_gurevych_2019">(<a href="#ref-reimers_gurevych_2019" role="doc-biblioref">Reimers &amp; Gurevych, 2019</a>)</span>, though averaging the embeddings from the last two layers of BERT can improve this <span class="citation" data-cites="li_etal_2020">(<a href="#ref-li_etal_2020" role="doc-biblioref">Li et al., 2020</a>)</span>
</li>
</ul>
static embeddings created by taking the first principal component of a word’s contextualized representations out- perform GloVe and FastText embeddings on many word vector benchmarks <span class="citation" data-cites="ethayarajh_2019">(<a href="#ref-ethayarajh_2019" role="doc-biblioref">Ethayarajh, 2019</a>)</span>
</li>
</ul>
<section id="sec-ccr-improvement" class="level4" data-number="20.2.1.1"><h4 data-number="20.2.1.1" class="anchored" data-anchor-id="sec-ccr-improvement">
<span class="header-section-number">20.2.1.1</span> Improving CCR With Geometric Reasoning</h4>
<p>In CCR, for example, there’s a difference between agreeing with statements in a questionnaire and actually writing like a questionnaire. How do I know that I’m measuring how anxious/depressed the comments are and not how questionnaire-y they are? Potential workaround: reverse all of the statements in the questionnaire (as for reverse-coded items), add them to the original questionnaire, and get the average embedding of the construct-neutral questionnaire. Then subtract this embedding from the original questionnaire average embedding.</p>
<div class="cell">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">surprise_items_pos</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span></span>
<span>  <span class="st">"I was extremely surprised by the outcome of the event."</span>,</span>
<span>  <span class="st">"The outcome of the event was extremely interesting."</span>,</span>
<span>  <span class="st">"The outcome of the event was extremely new."</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="va">surprise_items_neg</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span></span>
<span>  <span class="st">"I was not surprised at all by the outcome of the event."</span>,</span>
<span>  <span class="st">"The outcome of the event was not interesting at all."</span>,</span>
<span>  <span class="st">"The outcome of the event was not new at all."</span></span>
<span>  <span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section></section><section id="sec-advanced-similarity" class="level3" data-number="20.2.2"><h3 data-number="20.2.2" class="anchored" data-anchor-id="sec-advanced-similarity">
<span class="header-section-number">20.2.2</span> Advanced Similarity Measures</h3>
<section id="dot-product" class="level4" data-number="20.2.2.1"><h4 data-number="20.2.2.1" class="anchored" data-anchor-id="dot-product">
<span class="header-section-number">20.2.2.1</span> Dot Product</h4>
</section><section id="jaccard-similarity" class="level4" data-number="20.2.2.2"><h4 data-number="20.2.2.2" class="anchored" data-anchor-id="jaccard-similarity">
<span class="header-section-number">20.2.2.2</span> Jaccard similarity</h4>
</section><section id="mutual-information" class="level4" data-number="20.2.2.3"><h4 data-number="20.2.2.3" class="anchored" data-anchor-id="mutual-information">
<span class="header-section-number">20.2.2.3</span> Mutual Information</h4>
</section><section id="jensenshannon-divergence" class="level4" data-number="20.2.2.4"><h4 data-number="20.2.2.4" class="anchored" data-anchor-id="jensenshannon-divergence">
<span class="header-section-number">20.2.2.4</span> Jensen–Shannon divergence</h4>
</section></section><section id="sec-dimension-projection" class="level3" data-number="20.2.3"><h3 data-number="20.2.3" class="anchored" data-anchor-id="sec-dimension-projection">
<span class="header-section-number">20.2.3</span> Semantic Projection</h3>
<p><span class="citation" data-cites="grand_etal_2022">Grand et al. (<a href="#ref-grand_etal_2022" role="doc-biblioref">2022</a>)</span></p>
<section id="improving-ccr-with-semantic-projection" class="level4" data-number="20.2.3.1"><h4 data-number="20.2.3.1" class="anchored" data-anchor-id="improving-ccr-with-semantic-projection">
<span class="header-section-number">20.2.3.1</span> Improving CCR With Semantic Projection</h4>
<p><strong>An example of using dimension projection and CCR in research:</strong> <span class="citation" data-cites="simchon_etal_2023">Simchon et al. (<a href="#ref-simchon_etal_2023" role="doc-biblioref">2023</a>)</span> collected 10,000 messages from the <a href="https://www.reddit.com/r/depression">r/depression</a> subreddit, along with a control group of 100 messages each from 100 randomly selected subreddits. They then used a variant of SBERT, <code>all-MiniLM-L6-v2</code> (see <a href="contextualized-embeddings.html" class="quarto-xref"><span>Chapter 19</span></a>), to compute CCR embeddings of a psychological questionnaire measuring “locus of control,” the feeling that you have control over your own life. The questionnaire included items measuring an internal locus of control (“I have control”), and items measuring an external locus of control (“External forces have control”). Simchon et al.&nbsp;constructed an anchored vector to capture the direction between internal and external locus of control, and projected embeddings of the Reddit posts onto that vector to measure how much each post reflected an internal vs.&nbsp;an external locus of control. They found that posts in r/depression exhibited a more external locus of control than posts in the control group.</p>
</section></section><section id="sec-machine-learning-methods" class="level3" data-number="20.2.4"><h3 data-number="20.2.4" class="anchored" data-anchor-id="sec-machine-learning-methods">
<span class="header-section-number">20.2.4</span> Machine Learning Methods</h3>
<p><span class="citation" data-cites="kjell_etal_2022">Kjell et al. (<a href="#ref-kjell_etal_2022" role="doc-biblioref">2022</a>)</span></p>
<p><span class="citation" data-cites="chersoni_etal_2021">Chersoni et al. (<a href="#ref-chersoni_etal_2021" role="doc-biblioref">2021</a>)</span> used partial least squares regression to map distributional embeddings to human-rated semantic features derived from research in cognitive psychology.</p>
<p>Some research advises using both the <code>[CLS]</code> token and an aggregation of the other token embeddings <span class="citation" data-cites="lee_etal_2023">(<a href="#ref-lee_etal_2023" role="doc-biblioref">Lee et al., 2023</a>)</span></p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-atari_etal_2023" class="csl-entry" role="listitem">
Atari, M., Omrani, A., &amp; Dehghani, M. (2023). <em>Contextualized construct representation: Leveraging psychometric scales to advance theory-driven text analysis</em>. PsyArXiv. <a href="https://doi.org/10.31234/osf.io/m93pd">https://doi.org/10.31234/osf.io/m93pd</a>
</div>
<div id="ref-chersoni_etal_2021" class="csl-entry" role="listitem">
Chersoni, E., Santus, E., Huang, C.-R., &amp; Lenci, A. (2021). Decoding word embeddings with brain-based semantic features. <em>Computational Linguistics</em>, <em>47</em>(3), 663–698. <a href="https://doi.org/10.1162/coli_a_00412">https://doi.org/10.1162/coli_a_00412</a>
</div>
<div id="ref-choi_choi_2010" class="csl-entry" role="listitem">
Choi, D., &amp; Choi, I. (2010). A comparison of hindsight bias in groups and individuals: The moderating role of plausibility. <em>Journal of Applied Social Psychology</em>, <em>40</em>(2), 325–343. <a href="https://search.ebscohost.com/login.aspx?direct=true&amp;amp;db=sxi&amp;amp;AN=48116256&amp;amp;site=ehost-live">https://search.ebscohost.com/login.aspx?direct=true&amp;amp;db=sxi&amp;amp;AN=48116256&amp;amp;site=ehost-live</a>
</div>
<div id="ref-choi_nisbett_2000" class="csl-entry" role="listitem">
Choi, I., &amp; Nisbett, R. E. (2000). Cultural psychology of surprise: Holistic theories and recognition of contradiction. <em>Journal of Personality and Social Psychology</em>, <em>79</em>(6), 890–905.
</div>
<div id="ref-ethayarajh_2019" class="csl-entry" role="listitem">
Ethayarajh, K. (2019). <em>How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings</em>. <a href="https://arxiv.org/abs/1909.00512">https://arxiv.org/abs/1909.00512</a>
</div>
<div id="ref-ethayarajh_etal_2019" class="csl-entry" role="listitem">
Ethayarajh, K., Duvenaud, D., &amp; Hirst, G. (2019). <em>Towards understanding linear word analogies</em>. <a href="https://arxiv.org/abs/1810.04882">https://arxiv.org/abs/1810.04882</a>
</div>
<div id="ref-gao_etal_2019" class="csl-entry" role="listitem">
Gao, J., He, D., Tan, X., Qin, T., Wang, L., &amp; Liu, T.-Y. (2019). <em>Representation degeneration problem in training natural language generation models</em>. <a href="https://arxiv.org/abs/1907.12009">https://arxiv.org/abs/1907.12009</a>
</div>
<div id="ref-garten_etal_2018" class="csl-entry" role="listitem">
Garten, J., Hoover, J., Johnson, K. M., Boghrati, R., Iskiwitch, C., &amp; Dehghani, M. (2018). Dictionaries and distributions: Combining expert knowledge and large scale textual data content analysis: Distributed dictionary representation. <em>Behavior Research Methods</em>, <em>50</em>, 344–361.
</div>
<div id="ref-grand_etal_2022" class="csl-entry" role="listitem">
Grand, G., Blank, I. A., Pereira, F., &amp; Fedorenko, E. (2022). Semantic projection recovers rich human knowledge of multiple object features from word embeddings. <em>Nature Human Behaviour</em>, <em>6</em>(7), 975–987. <a href="https://doi.org/10.1038/s41562-022-01316-8">https://doi.org/10.1038/s41562-022-01316-8</a>
</div>
<div id="ref-kjell_etal_2022" class="csl-entry" role="listitem">
Kjell, O., Sikström, S., Kjell, K., &amp; Schwartz, H. (2022). Natural language analyzed with AI-based transformers predict traditional subjective well-being measures approaching the theoretical upper limits in accuracy. <em>Scientific Reports</em>, <em>12</em>, 3918. <a href="https://doi.org/10.1038/s41598-022-07520-w">https://doi.org/10.1038/s41598-022-07520-w</a>
</div>
<div id="ref-lee_etal_2023" class="csl-entry" role="listitem">
Lee, K., Choi, G., &amp; Choi, C. (2023). Use all tokens method to improve semantic relationship learning. <em>Expert Systems with Applications</em>, <em>233</em>, 120911. https://doi.org/<a href="https://doi.org/10.1016/j.eswa.2023.120911">https://doi.org/10.1016/j.eswa.2023.120911</a>
</div>
<div id="ref-li_etal_2020" class="csl-entry" role="listitem">
Li, B., Zhou, H., He, J., Wang, M., Yang, Y., &amp; Li, L. (2020). <em>On the sentence embeddings from pre-trained language models</em>. <a href="https://arxiv.org/abs/2011.05864">https://arxiv.org/abs/2011.05864</a>
</div>
<div id="ref-mikolov_etal_2013" class="csl-entry" role="listitem">
Mikolov, T., Yih, W., &amp; Zweig, G. (2013). Linguistic regularities in continuous space word representations. In L. Vanderwende, H. Daumé III, &amp; K. Kirchhoff (Eds.), <em>Proceedings of the 2013 conference of the north <span>A</span>merican chapter of the association for computational linguistics: Human language technologies</em> (pp. 746–751). Association for Computational Linguistics. <a href="https://aclanthology.org/N13-1090">https://aclanthology.org/N13-1090</a>
</div>
<div id="ref-mohammad_turney_2013" class="csl-entry" role="listitem">
Mohammad, S. M., &amp; Turney, P. D. (2013). Crowdsourcing a word-emotion association lexicon. <em>Computational Intelligence</em>, <em>29</em>(3), 436–465.
</div>
<div id="ref-mohammad_turney_2010" class="csl-entry" role="listitem">
Mohammad, S., &amp; Turney, P. (2010). Emotions evoked by common words and phrases: Using <span>M</span>echanical <span>T</span>urk to create an emotion lexicon. <em>Proceedings of the <span>NAACL</span> <span>HLT</span> 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text</em>, 26–34. <a href="https://aclanthology.org/W10-0204">https://aclanthology.org/W10-0204</a>
</div>
<div id="ref-pennington_etal_2014" class="csl-entry" role="listitem">
Pennington, J., Socher, R., &amp; Manning, C. D. (2014). GloVe: Global vectors for word representation. <em>Empirical Methods in Natural Language Processing (EMNLP)</em>, 1532–1543. <a href="http://www.aclweb.org/anthology/D14-1162">http://www.aclweb.org/anthology/D14-1162</a>
</div>
<div id="ref-reimers_gurevych_2019" class="csl-entry" role="listitem">
Reimers, N., &amp; Gurevych, I. (2019). Sentence-<span>BERT</span>: Sentence embeddings using <span>S</span>iamese <span>BERT</span>-networks. In K. Inui, J. Jiang, V. Ng, &amp; X. Wan (Eds.), <em>Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)</em> (pp. 3982–3992). Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D19-1410">https://doi.org/10.18653/v1/D19-1410</a>
</div>
<div id="ref-simchon_etal_2023" class="csl-entry" role="listitem">
Simchon, A., Hadar, B., &amp; Gilead, M. (2023). A computational text analysis investigation of the relation between personal and linguistic agency. <em>Communications Psychology</em>, 1–9. <a href="https://doi.org/10.1038/s44271-023-00020-1">https://doi.org/10.1038/s44271-023-00020-1</a>
</div>
</div>
</section></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr>
<ol>
<li id="fn1"><p>Cosine similarity is appropriate here because our contextualized embeddings were generated by an SBERT model which was designed to be used with cosine similarity. If we had used another model such as RoBERTa, Euclidean distance might be more appropriate.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/ds4psych\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./contextualized-embeddings.html" class="pagination-link" aria-label="Contextualization With Large Language Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Contextualization With Large Language Models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./lda.html" class="pagination-link" aria-label="Topic Modeling">
        <span class="nav-page-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Topic Modeling</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>Data Science for Psychology was written by <a href="https://rimonim.github.io">Louis Teitelbaum</a> and <a href="https://almogsi.com">Almog Simchon</a>.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/rimonim/ds4psych/blob/main/navigating-vectorspace.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/rimonim/ds4psych/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a> and is powered by <a href="https://www.netlify.com/">Netlify</a></p>
</div>
  </div>
</footer>


</body></html>