<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Louis Teitelbaum and Almog Simchon">
<title>Data Science for Psychology: Natural Language - 20&nbsp; Navigating Vector Space</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./lda.html" rel="next">
<link href="./contextualized-embeddings.html" rel="prev">
<link href="./images/favicon.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><style>
.r-output code {
  word-break: break-wor !important;
  white-space: pre-wrap !important;
}
</style>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./quantification.html">Quantifying Psychological Properties of Text</a></li><li class="breadcrumb-item"><a href="./navigating-vectorspace.html"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Navigating Vector Space</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Data Science for Psychology: Natural Language</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/rimonim/ds4psych" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
<li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=%7Curl%7C">
              <i class="bi bi-bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=%7Curl%7C">
              <i class="bi bi-bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=%7Curl%7C">
              <i class="bi bi-bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
      </ul>
</div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction: Why Does Psychology Need Natural Language?</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ethics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Ethics of Data Science in Psychology</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./data-viz.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Visualization</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./aesthetics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Why Aesthetic Choices are Important</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./telling-a-story.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Don’t Distract From the Story</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./word-viz.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Visualizing Distributions of Words</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data-viz-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Additional Resources for Data Visualization</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./data-sources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sources of Data</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./experiments.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Experiments</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./corpora.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Corpus Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./apis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Web APIs</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./scraping.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Web Scraping</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./quantification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quantifying Psychological Properties of Text</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./look-at-your-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Look at Your Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quanteda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Introduction to Quanteda</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tokenization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Tokenization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./word-counting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Dictionary-Based Word Counts</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dla.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Open Vocabulary Word Counting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./word-counting-improvements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Transforming Word Counts</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./vectorspace-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Introduction to Vector Space</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./decontextualized-embeddings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Word Embeddings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./contextualized-embeddings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Contextualization With Large Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./navigating-vectorspace.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Navigating Vector Space</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Topic Modeling</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linguistic-complexity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Measures of Linguistic Complexity</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./querying_llms.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Just Ask an LLM</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./audio-video-image.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Audio, Video, and Image Data</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./transcription.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Transcribing Audio</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Additional Resources</span></span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#representing-psychological-constructs" id="toc-representing-psychological-constructs" class="nav-link active" data-scroll-target="#representing-psychological-constructs"><span class="header-section-number">20.1</span> Representing Psychological Constructs</a>
  <ul class="collapse">
<li><a href="#sec-ddr" id="toc-sec-ddr" class="nav-link" data-scroll-target="#sec-ddr"><span class="header-section-number">20.1.1</span> Distributed Dictionary Representation (DDR)</a></li>
  <li><a href="#sec-ccr" id="toc-sec-ccr" class="nav-link" data-scroll-target="#sec-ccr"><span class="header-section-number">20.1.2</span> Contextualized Construct Representation (CCR)</a></li>
  </ul>
</li>
  <li>
<a href="#reasoning-in-vector-space-beyond-cosine-similarity-and-dot-products" id="toc-reasoning-in-vector-space-beyond-cosine-similarity-and-dot-products" class="nav-link" data-scroll-target="#reasoning-in-vector-space-beyond-cosine-similarity-and-dot-products"><span class="header-section-number">20.2</span> Reasoning in Vector Space: Beyond Cosine Similarity and Dot Products</a>
  <ul class="collapse">
<li><a href="#sec-parallelograms" id="toc-sec-parallelograms" class="nav-link" data-scroll-target="#sec-parallelograms"><span class="header-section-number">20.2.1</span> Additive Analogies</a></li>
  <li><a href="#sec-dimension-projection" id="toc-sec-dimension-projection" class="nav-link" data-scroll-target="#sec-dimension-projection"><span class="header-section-number">20.2.2</span> Anchored Vectors For Better Construct Representations</a></li>
  <li><a href="#sec-correlational-anchors" id="toc-sec-correlational-anchors" class="nav-link" data-scroll-target="#sec-correlational-anchors"><span class="header-section-number">20.2.3</span> Correlational Anchored Vectors</a></li>
  <li><a href="#sec-machine-learning-methods" id="toc-sec-machine-learning-methods" class="nav-link" data-scroll-target="#sec-machine-learning-methods"><span class="header-section-number">20.2.4</span> Machine Learning Methods</a></li>
  </ul>
</li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/rimonim/ds4psych/blob/main/navigating-vectorspace.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/rimonim/ds4psych/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./quantification.html">Quantifying Psychological Properties of Text</a></li><li class="breadcrumb-item"><a href="./navigating-vectorspace.html"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Navigating Vector Space</span></a></li></ol></nav><div class="quarto-title">
<h1 class="title"><span id="sec-navigating-vectorspace" class="quarto-section-identifier"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Navigating Vector Space</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><div class="callout callout-style-simple callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This page is still under construction. Come back soon!
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<section id="representing-psychological-constructs" class="level2" data-number="20.1"><h2 data-number="20.1" class="anchored" data-anchor-id="representing-psychological-constructs">
<span class="header-section-number">20.1</span> Representing Psychological Constructs</h2>
<p>In <a href="decontextualized-embeddings.html" class="quarto-xref"><span>Chapter 18</span></a> we measured the surprise in texts by comparing their embeddings to that of a single word: “surprised”. But does the embedding of the word “surprised” fully capture the concept of surprise as an emotion? Faced with this question of construct validity, we have two options:</p>
<ol type="1">
<li>
<strong>Conduct a Validation Study:</strong> We could find or construct a dataset of texts that were rated by a human (or ideally, multiple humans) on the extent to which they reflect the emotion of surprise. We could then compare our embedding-based surprise scores to the human ratings.</li>
<li>
<strong>Use an Already-Validated Construct Definition:</strong> Properly validating a new measure is hard work. When possible, psychology researchers often prefer to use an existing measure that has already been carefully validated in the past.</li>
</ol>
<p>The second option may seem difficult, since embeddings are very new to the field, so few if any validated vector representations of constructs are available. As it turns out, this is not a problem—any language-based psychological measure can be represented as a vector! Psychology has used language-based measures like dictionaries and questionnaires for over a century. To smoothly continue this existing research in the age of vector spaces, let’s consider how to translate between the two.</p>
<section id="sec-ddr" class="level3" data-number="20.1.1"><h3 data-number="20.1.1" class="anchored" data-anchor-id="sec-ddr">
<span class="header-section-number">20.1.1</span> Distributed Dictionary Representation (DDR)</h3>
<p>Let’s begin with a straightforward sort of psychological measure—the dictionary. We have already discussed dictionaries extensively in <a href="word-counting.html" class="quarto-xref"><span>Chapter 14</span></a> and noted that psychology researchers have been constructing, validating, and publicizing dictionaries for decades (<a href="word-counting.html#sec-dictionary-sources" class="quarto-xref"><span>Section 14.6</span></a>). But these dictionaries are designed for word counting—How do we apply them to a vector-based analysis? <span class="citation" data-cites="garten_etal_2018">Garten et al. (<a href="#ref-garten_etal_2018" role="doc-biblioref">2018</a>)</span> propose a simple solution: Get word embeddings (<a href="decontextualized-embeddings.html#sec-word-embeddings" class="quarto-xref"><span>Section 18.3</span></a>) for each word in the dictionary, and average them together to create a single Distributed Dictionary Representation (DDR). The dictionary construct can then be measured by comparing text embeddings to the DDR.</p>
<p>DDR cannot entirely replace word counts; for linguistic concepts like pronoun use or the passive voice, dictionary-based word counts are still necessary. But DDR is ideal for studies of abstract constructs like emotions, that refer to the general gist of a text rather than particular words. The rich representation of word embeddings allows DDR to capture even the subtlest associations between words and constructs, and to precisely reflect the <em>extent</em> to which each word is associated with each construct. It can do this even for texts that do not contain any dictionary words. Because embeddings are continuous and already calibrated to the probabilities of word use in language, DDR also avoids the difficult statistical problems that arise due to the strange distributions of word counts (<a href="word-counting-improvements.html" class="quarto-xref"><span>Chapter 16</span></a>).</p>
<p><span class="citation" data-cites="garten_etal_2018">Garten et al. (<a href="#ref-garten_etal_2018" role="doc-biblioref">2018</a>)</span> found that DDR works best with smaller dictionaries of only the words most directly connected to the construct being measured (around 30 words worked best in their experiments). Word embeddings work by overvaluing informative words (<a href="decontextualized-embeddings.html#sec-embedding-magnitude" class="quarto-xref"><span>Section 18.3.4</span></a>)—a desirable property for raw texts, in which uninformative words tend to be very frequent. But dictionaries only include one of each word. In longer dictionaries with more infrequent, tangentially connected words, averaging word embeddings will therefore <em>overvalue</em> those infrequent words and skew the DDR. This can be fixed with Garten et al.’s method of picking out only the most informative words. Alternatively, it could be fixed by measuring the frequency of each dictionary word in a corpus and weighting the average embedding by that frequency. This method is actually more consistent with the way most dictionaries are validated, by counting the frequencies of dictionary words in text (<a href="word-counting.html" class="quarto-xref"><span>Chapter 14</span></a>).</p>
<p>Let’s measure surprise in the Hippocorpus texts by computing a DDR of the NRC Word-Emotion Association Lexicon <span class="citation" data-cites="mohammad_turney_2010 mohammad_turney_2013">(<a href="#ref-mohammad_turney_2013" role="doc-biblioref">S. M. Mohammad &amp; Turney, 2013</a>; <a href="#ref-mohammad_turney_2010" role="doc-biblioref">S. Mohammad &amp; Turney, 2010</a>)</span>, which we used in <a href="word-counting.html" class="quarto-xref"><span>Chapter 14</span></a>. To correct for word informativeness, we will weight the dictionary word embeddings by their frequency in the corpus.</p>
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># load surprise dictionary</span></span>
<span><span class="va">surprise_dict</span> <span class="op">&lt;-</span> <span class="fu">quanteda.sentiment</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/quanteda.sentiment/man/data_dictionary_NRC.html">data_dictionary_NRC</a></span><span class="op">[</span><span class="st">"surprise"</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># estimate frequency of dictionary words</span></span>
<span><span class="va">surprise_dict_freqs</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_dfm</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">dfm_keep</span><span class="op">(</span><span class="va">surprise_dict</span><span class="op">$</span><span class="va">surprise</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">quanteda.textstats</span><span class="fu">::</span><span class="fu"><a href="https://quanteda.io/reference/textstat_frequency.html">textstat_frequency</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="va">feature</span>, <span class="va">frequency</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># word2vec embeddings of dictionary words</span></span>
<span><span class="va">surprise_ddr</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">word2vec_mod</span>, <span class="va">surprise_dict</span><span class="op">$</span><span class="va">surprise</span>, type <span class="op">=</span> <span class="st">"embedding"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">as_tibble</span><span class="op">(</span>rownames <span class="op">=</span> <span class="st">"feature"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">left_join</span><span class="op">(</span><span class="va">surprise_dict_freqs</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">replace_na</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>frequency <span class="op">=</span> <span class="fl">0</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># average dictionary embedding (weighted by frequency)</span></span>
<span><span class="va">surprise_ddr</span> <span class="op">&lt;-</span> <span class="va">surprise_ddr</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">summarise</span><span class="op">(</span><span class="fu">across</span><span class="op">(</span><span class="va">V1</span><span class="op">:</span><span class="va">V300</span>, <span class="op">~</span><span class="fu"><a href="https://rdrr.io/r/stats/weighted.mean.html">weighted.mean</a></span><span class="op">(</span><span class="va">.x</span>, <span class="va">frequency</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="va">V1</span><span class="op">:</span><span class="va">V300</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/unlist.html">unlist</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># document embeddings</span></span>
<span><span class="va">hippocorpus_word2vec</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_dfm</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">textstat_embedding</span><span class="op">(</span><span class="va">word2vec_mod</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># score documents by surprise</span></span>
<span><span class="va">hippocorpus_surprise_ddr</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_word2vec</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">rowwise</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span></span>
<span>    surprise <span class="op">=</span> <span class="fu">cos_sim</span><span class="op">(</span><span class="fu">c_across</span><span class="op">(</span><span class="va">V1</span><span class="op">:</span><span class="va">V300</span><span class="op">)</span>, <span class="va">surprise_ddr</span><span class="op">)</span>,</span>
<span>    <span class="co"># transform cosine similarity to stay between 0 and 1</span></span>
<span>    surprise <span class="op">=</span> <span class="va">surprise</span><span class="op">/</span><span class="fl">2</span> <span class="op">+</span> <span class="fl">1</span><span class="op">/</span><span class="fl">2</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">ungroup</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">V1</span><span class="op">:</span><span class="va">V300</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># rejoin docvars</span></span>
<span><span class="va">hippocorpus_surprise_ddr</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_surprise_ddr</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">bind_cols</span><span class="op">(</span><span class="fu">docvars</span><span class="op">(</span><span class="va">hippocorpus_corp</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>With the new measure of surprise, we can retest the hypothesis that true autobiographical stories include more surprise than imagined stories.</p>
<div class="cell">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># logistic regression</span></span>
<span><span class="va">surprise_mod_ddr</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span></span>
<span>  <span class="va">surprise</span> <span class="op">~</span> <span class="va">memType</span>, </span>
<span>  family <span class="op">=</span> <span class="va">binomial</span>,</span>
<span>  <span class="va">hippocorpus_surprise_ddr</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">surprise_mod_ddr</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="r-output"><code>#&gt; 
#&gt; Call:
#&gt; glm(formula = surprise ~ memType, family = binomial, data = hippocorpus_surprise_ddr)
#&gt; 
#&gt; Coefficients:
#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)    
#&gt; (Intercept)      1.81235    0.05484  33.049    memTyperecalled -0.02041    0.07711  -0.265    0.791    
#&gt; memTyperetold   -0.02250    0.09587  -0.235    0.814    
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; (Dispersion parameter for binomial family taken to be 1)
#&gt; 
#&gt;     Null deviance: 6.9881  on 6853  degrees of freedom
#&gt; Residual deviance: 6.8986  on 6851  degrees of freedom
#&gt; AIC: 2103.6
#&gt; 
#&gt; Number of Fisher Scoring iterations: 5
</code></pre>
</div>
<p>We again find no significant difference in surprise between remembered and recalled stories. This is consistent with our results from <a href="word-counting.html" class="quarto-xref"><span>Chapter 14</span></a>, where we tested the same hypothesis with the same dictionary, but used word counts rather than embeddings.</p>
<section id="ddr-for-word-by-word-analysis" class="level4" data-number="20.1.1.1"><h4 data-number="20.1.1.1" class="anchored" data-anchor-id="ddr-for-word-by-word-analysis">
<span class="header-section-number">20.1.1.1</span> DDR for Word-by-Word Analysis</h4>
<p>Another advantage of DDR over dictionary-based word counts is that DDR enables word-by-word analysis of text. It is not very informative to count how many surprise words are in each word (it will either be one or zero), but we can compare the embedding of each word to the surprise DDR—how close are they in the vector space? This allows us to see how a construct spreads out within a single text. As an example, let’s take a single story from the Hippocorpus:</p>
<div class="cell">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># full text as string</span></span>
<span><span class="va">story</span> <span class="op">&lt;-</span> <span class="fu">word</span><span class="op">(</span><span class="va">hippocorpus_df</span><span class="op">$</span><span class="va">story</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span>, end <span class="op">=</span> <span class="fl">140L</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="va">story</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="r-output"><code>#&gt; It seems just like yesterday but today makes five months ago it happened. I had been watching my phone like an owl for the past week. I was waiting for a work related call that my team was waiting for to close a important deal. It wasnt the call I expected though. It was for  my sister was in labor with the twins. My sister is only 7 months pregnant. I got the call shortly after arriving at work. Just as fast I was back out the door and on my way to the hospital. When I arrived my sister had just delivered and I just was in awe. Even though they were a bit small they were mighty. They were the most precious things I had ever seen. I held my niece and nephew and couldnt stop crying.
</code></pre>
</div>
<p>To visualize surprise within this text, we can separate it into words and find the embedding of each word. Rather than averaging all of these embeddings together to get the embedding of the full text, we can compute a rolling average, averaging each word’s embedding with those of its neighbors.</p>
<div class="cell">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># separate into vector of tokens</span></span>
<span><span class="va">story</span> <span class="op">&lt;-</span> <span class="fu">word</span><span class="op">(</span><span class="va">hippocorpus_df</span><span class="op">$</span><span class="va">story</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span>, end <span class="op">=</span> <span class="fl">140L</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">tokens</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/base/character.html">as.character</a></span><span class="op">(</span><span class="op">)</span></span>
<span>  </span>
<span><span class="co"># rolling average of embeddings</span></span>
<span><span class="va">story_surprise</span> <span class="op">&lt;-</span> <span class="fu">as_tibble</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">word2vec_mod</span>, <span class="va">story</span>, type <span class="op">=</span> <span class="st">"embedding"</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span></span>
<span>    <span class="fu">across</span><span class="op">(</span></span>
<span>      <span class="va">V1</span><span class="op">:</span><span class="va">V300</span>, </span>
<span>      <span class="op">~</span><span class="fu">zoo</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/zoo/man/rollapply.html">rollapply</a></span><span class="op">(</span></span>
<span>        <span class="va">.x</span>, <span class="fl">4</span>, <span class="va">mean</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span>, </span>
<span>        align <span class="op">=</span> <span class="st">"center"</span>,</span>
<span>        fill <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">.x</span>, <span class="fl">1</span><span class="op">)</span>, <span class="cn">NA</span>, <span class="fu"><a href="https://rdrr.io/r/utils/head.html">tail</a></span><span class="op">(</span><span class="va">.x</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span>        <span class="op">)</span></span>
<span>      <span class="op">)</span></span>
<span>    <span class="op">)</span></span>
<span>  </span>
<span><span class="co"># vector of computed surprise (cosine similarity)</span></span>
<span><span class="va">story_surprise</span> <span class="op">&lt;-</span> <span class="va">story_surprise</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">rowwise</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>surprise <span class="op">=</span> <span class="fu">cos_sim</span><span class="op">(</span><span class="fu">c_across</span><span class="op">(</span><span class="va">V1</span><span class="op">:</span><span class="va">V300</span><span class="op">)</span>, <span class="va">surprise_ddr</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">pull</span><span class="op">(</span><span class="va">surprise</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now visualize the surprise in each word of the text. Since <code>ggplot2</code> makes it difficult to plot dynamically colored text in one continuous chunk, we will use ANSI color codes to print the text directly to the console.</p>
<div class="cell">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># (see https://www.hackitu.de/termcolor256/ for info on ANSI colors)</span></span>
<span><span class="co"># blue-red heat scale</span></span>
<span><span class="va">ansi_scale</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span></span>
<span>  <span class="fl">063</span>, <span class="fl">105</span>, <span class="fl">147</span>, <span class="fl">189</span>, <span class="fl">188</span>, <span class="fl">230</span>, <span class="fl">223</span>, </span>
<span>  <span class="fl">224</span>, <span class="fl">217</span>, <span class="fl">210</span>, <span class="fl">203</span>, <span class="fl">196</span>, <span class="fl">160</span>, <span class="fl">124</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># turn scale value into ANSI color code</span></span>
<span><span class="va">map_to_ansi</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">ansi_scale</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">x_new</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">x</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span><span class="op">*</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">ansi_scale</span><span class="op">)</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/diff.html">diff</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/range.html">range</a></span><span class="op">(</span><span class="va">x</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="fl">1</span></span>
<span>  <span class="va">x_new</span></span>
<span>  <span class="va">ansi_scale</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">x_new</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="va">story_surprise</span> <span class="op">&lt;-</span> <span class="fu">map_to_ansi</span><span class="op">(</span><span class="va">story_surprise</span>, <span class="va">ansi_scale</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># print</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">story_surprise</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="kw">if</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">story_surprise</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="va">story</span><span class="op">[</span><span class="va">i</span><span class="op">]</span>, <span class="st">" "</span><span class="op">)</span></span>
<span>  <span class="op">}</span><span class="kw">else</span><span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"\033[48;5;"</span>, <span class="va">story_surprise</span><span class="op">[</span><span class="va">i</span><span class="op">]</span>, <span class="st">"m"</span>, <span class="va">story</span><span class="op">[</span><span class="va">i</span><span class="op">]</span>, <span class="st">" \033[0m"</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="r-output"><code>#&gt; <span style="background-color: #D7D7FF;">It </span><span style="background-color: #AF0000;">seems just </span>like  <span style="background-color: #FF5F5F;">yesterday </span><span style="background-color: #FFD7D7;">but </span><span style="background-color: #FF8787;">today </span><span style="background-color: #D7D7D7;">makes </span><span style="background-color: #D7D7FF;">five </span><span style="background-color: #FFFFD7;">months </span><span style="background-color: #FFAFAF;">ago it </span><span style="background-color: #FF0000;">happened </span><span style="background-color: #FF8787;">. </span><span style="background-color: #FFD7AF;">I </span><span style="background-color: #FF8787;">had been watching </span><span style="background-color: #FF0000;">my </span><span style="background-color: #FF8787;">phone </span><span style="background-color: #FFD7D7;">like </span><span style="background-color: #FFAFAF;">an </span><span style="background-color: #D7D7D7;">owl </span><span style="background-color: #FFFFD7;">for the </span><span style="background-color: #D7D7D7;">past </span><span style="background-color: #FFD7D7;">week </span><span style="background-color: #FFAFAF;">. </span><span style="background-color: #FF0000;">I was </span><span style="background-color: #FFD7D7;">waiting </span><span style="background-color: #FF8787;">for </span><span style="background-color: #D7D7FF;">a </span><span style="background-color: #AFAFFF;">work </span><span style="background-color: #FFD7AF;">related </span><span style="background-color: #FFAFAF;">call </span><span style="background-color: #D70000;">that </span><span style="background-color: #FF0000;">my </span><span style="background-color: #D70000;">team </span><span style="background-color: #FF8787;">was </span><span style="background-color: #FFD7D7;">waiting </span><span style="background-color: #FF8787;">for </span><span style="background-color: #FFFFD7;">to </span><span style="background-color: #FFD7D7;">close </span><span style="background-color: #FF8787;">a </span><span style="background-color: #FFD7D7;">important </span><span style="background-color: #FF8787;">deal </span><span style="background-color: #FFFFD7;">. </span><span style="background-color: #FFD7D7;">It wasnt </span><span style="background-color: #FF8787;">the </span><span style="background-color: #FF5F5F;">call </span><span style="background-color: #FF0000;">I </span><span style="background-color: #FF5F5F;">expected </span><span style="background-color: #FFAFAF;">though . </span><span style="background-color: #FFD7AF;">It </span><span style="background-color: #FF8787;">was </span><span style="background-color: #FFAFAF;">for my sister </span><span style="background-color: #FFFFD7;">was </span><span style="background-color: #D7D7D7;">in labor </span><span style="background-color: #FFD7AF;">with the </span><span style="background-color: #FFD7D7;">twins </span><span style="background-color: #D7D7D7;">. My </span><span style="background-color: #FFD7D7;">sister </span><span style="background-color: #FFFFD7;">is only 7 </span><span style="background-color: #5F5FFF;">months </span><span style="background-color: #FFD7AF;">pregnant </span><span style="background-color: #FF0000;">. </span><span style="background-color: #D70000;">I got </span><span style="background-color: #FF5F5F;">the </span><span style="background-color: #FFD7AF;">call shortly </span><span style="background-color: #FFFFD7;">after </span><span style="background-color: #FFAFAF;">arriving </span><span style="background-color: #FFD7AF;">at </span><span style="background-color: #FFD7D7;">work . </span><span style="background-color: #FFD7AF;">Just </span><span style="background-color: #FF8787;">as fast </span><span style="background-color: #FF0000;">I </span><span style="background-color: #FF5F5F;">was </span><span style="background-color: #FF8787;">back </span><span style="background-color: #FFD7D7;">out </span><span style="background-color: #FFFFD7;">the </span><span style="background-color: #D7D7FF;">door </span><span style="background-color: #FFFFD7;">and </span><span style="background-color: #FF8787;">on my </span><span style="background-color: #FF5F5F;">way </span><span style="background-color: #FF8787;">to </span><span style="background-color: #D7D7D7;">the </span><span style="background-color: #FFD7AF;">hospital </span><span style="background-color: #FFAFAF;">. </span><span style="background-color: #FF5F5F;">When </span><span style="background-color: #FF0000;">I </span><span style="background-color: #FF5F5F;">arrived </span><span style="background-color: #FF8787;">my </span><span style="background-color: #D70000;">sister </span><span style="background-color: #FF5F5F;">had </span><span style="background-color: #FF8787;">just </span><span style="background-color: #FF0000;">delivered and </span><span style="background-color: #D70000;">I just was </span><span style="background-color: #FFAFAF;">in awe . Even </span><span style="background-color: #FFD7D7;">though they </span><span style="background-color: #FFAFAF;">were </span><span style="background-color: #FFD7D7;">a </span><span style="background-color: #FF5F5F;">bit </span><span style="background-color: #FF8787;">small </span><span style="background-color: #FFD7D7;">they were </span><span style="background-color: #FFFFD7;">mighty . </span><span style="background-color: #D7D7D7;">They </span><span style="background-color: #FFFFD7;">were </span><span style="background-color: #FFD7AF;">the </span><span style="background-color: #FF5F5F;">most </span><span style="background-color: #FF0000;">precious </span><span style="background-color: #AF0000;">things </span><span style="background-color: #FF0000;">I </span><span style="background-color: #FFAFAF;">had </span><span style="background-color: #FFFFD7;">ever </span><span style="background-color: #FFD7D7;">seen </span><span style="background-color: #FFD7AF;">. </span><span style="background-color: #FFD7D7;">I </span><span style="background-color: #FF8787;">held </span><span style="background-color: #FFD7AF;">my </span><span style="background-color: #FFFFD7;">niece </span><span style="background-color: #8787FF;">and </span><span style="background-color: #D7D7D7;">nephew </span><span style="background-color: #FFD7AF;">and </span><span style="background-color: #FFAFAF;">couldnt stop </span>crying  .
</code></pre>
</div>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advantages of DDR
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Richer, More Robust Construct Representation Than Word Counting</strong></li>
<li><strong>Avoids Statistical Problems With Word Count Distributions</strong></li>
<li><strong>Enables Word-by-Word Analysis</strong></li>
<li>
<strong>Works Well With Short Dictionaries:</strong> DDR only needs a dictionary that captures the essence of the construct being measured. For many constructs, this could be only a few words. You can even ask an LLM chatbot to generate a list of words that people high or low in a certain construct might use.</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Disadvantages of DDR
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>
<strong>Can Implicitly Encode Associated Constructs:</strong> For example, if surprised texts tend to have positive valence in the data used to train the word embedding model, the DDR for surprise may embed some positive valence as well. This can be remedied by constructing a DDR for positive valence too, and using it as a statistical control when testing hypotheses.</li>
<li>
<strong>May Not Work With Contextualized Embeddings:</strong> Even if we assume that contextualized embeddings (<a href="contextualized-embeddings.html" class="quarto-xref"><span>Chapter 19</span></a>) conform to the geometrical properties associated with word embeddings, LLMs are not designed to embed single words, which is required for DDR.</li>
<li>
<strong>Not Appropriate for Linguistic Measures:</strong> Word embeddings encode the general gist of a text, whereas constructs like passive voice or pronoun use refer to specific words.</li>
</ul>
</div>
</div>
</section></section><section id="sec-ccr" class="level3" data-number="20.1.2"><h3 data-number="20.1.2" class="anchored" data-anchor-id="sec-ccr">
<span class="header-section-number">20.1.2</span> Contextualized Construct Representation (CCR)</h3>
<p>Dictionaries are not the only validated psychological measures that we can apply using embeddings. With contextualized embeddings, we can extract the gist of any text and compare it to that of any other text (<a href="contextualized-embeddings.html" class="quarto-xref"><span>Chapter 19</span></a>). <span class="citation" data-cites="atari_etal_2023">Atari et al. (<a href="#ref-atari_etal_2023" role="doc-biblioref">2023</a>)</span> propose to do this with the most popular form of psychometric scale: the questionnaire. Psychologists have been using questionnaires to measure things for over a century, and tens of thousands of validated questionnaires are now available <a href="https://www.apa.org/pubs/databases/psyctests">online</a>. The LLM embedding of a questionnaire is referred to as a Contextualized Construct Representation (CCR).</p>
<p>We can use CCR to measure surprise in the Hippocorpus texts. For our questionnaire, we will use an adapted version of the surprise scale used by <span class="citation" data-cites="choi_choi_2010">D. Choi &amp; Choi (<a href="#ref-choi_choi_2010" role="doc-biblioref">2010</a>)</span> and <span class="citation" data-cites="choi_nisbett_2000">I. Choi &amp; Nisbett (<a href="#ref-choi_nisbett_2000" role="doc-biblioref">2000</a>)</span>.</p>
<div class="cell">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">surprise_items</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span></span>
<span>  <span class="st">"I was extremely surprised by the outcome of the event."</span>,</span>
<span>  <span class="st">"The outcome of the event was extremely interesting."</span>,</span>
<span>  <span class="st">"The outcome of the event was extremely new."</span></span>
<span>  <span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-style-simple callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Beware of Reverse Coding!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Many questionnaires include reverse-coded items (e.g.&nbsp;“I often feel happy” on a depression questionnaire). The easiest way to deal with these is to manually add negations to flip their meaning (e.g.&nbsp;“I <em>do not</em> often feel happy”).</p>
</div>
</div>
<p>The first step in using CCR is to compute contextualized embeddings for the texts in the Hippocorpus dataset. We already did this in <a href="contextualized-embeddings.html" class="quarto-xref"><span>Chapter 19</span></a>. The next step is to compute contextualized embeddings for the items in the questionnaire, and average them to produce a CCR.</p>
<div class="cell">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># embed items (using the same model as we used before)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-text.org/">text</a></span><span class="op">)</span></span>
<span>  </span>
<span><span class="va">surprise_sbert</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://r-text.org/reference/textEmbed.html">textEmbed</a></span><span class="op">(</span></span>
<span>  <span class="va">surprise_items</span>,</span>
<span>  model <span class="op">=</span> <span class="st">"sentence-transformers/all-MiniLM-L12-v2"</span>, <span class="co"># model name</span></span>
<span>  layers <span class="op">=</span> <span class="op">-</span><span class="fl">2</span>,  <span class="co"># second to last layer (default)</span></span>
<span>  tokens_select <span class="op">=</span> <span class="st">"[CLS]"</span>, <span class="co"># use only [CLS] token</span></span>
<span>  dim_name <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  keep_token_embeddings <span class="op">=</span> <span class="cn">FALSE</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># compute CCR by averaging item embeddings</span></span>
<span><span class="va">surprise_ccr</span> <span class="op">&lt;-</span> <span class="va">surprise_sbert</span><span class="op">$</span><span class="va">texts</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">summarise</span><span class="op">(</span><span class="fu">across</span><span class="op">(</span><span class="fu">everything</span><span class="op">(</span><span class="op">)</span>, <span class="va">mean</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/unlist.html">unlist</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now measure surprise in the Hippocorpus texts by computing the cosine similarity between their embeddings and the surprise CCR.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<div class="cell">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># score documents by surprise</span></span>
<span><span class="va">hippocorpus_surprise_ccr</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_sbert</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">rowwise</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span></span>
<span>    surprise <span class="op">=</span> <span class="fu">cos_sim</span><span class="op">(</span><span class="fu">c_across</span><span class="op">(</span><span class="va">Dim1</span><span class="op">:</span><span class="va">Dim384</span><span class="op">)</span>, <span class="va">surprise_ccr</span><span class="op">)</span>,</span>
<span>    <span class="co"># transform cosine similarity to stay between 0 and 1</span></span>
<span>    surprise <span class="op">=</span> <span class="va">surprise</span><span class="op">/</span><span class="fl">2</span> <span class="op">+</span> <span class="fl">1</span><span class="op">/</span><span class="fl">2</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">ungroup</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">Dim1</span><span class="op">:</span><span class="va">Dim384</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># logistic regression</span></span>
<span><span class="va">surprise_mod_ccr</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span></span>
<span>  <span class="va">surprise</span> <span class="op">~</span> <span class="va">memType</span>, </span>
<span>  family <span class="op">=</span> <span class="va">binomial</span>,</span>
<span>  <span class="va">hippocorpus_surprise_ccr</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">surprise_mod_ccr</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="r-output"><code>#&gt; 
#&gt; Call:
#&gt; glm(formula = surprise ~ memType, family = binomial, data = hippocorpus_surprise_ccr)
#&gt; 
#&gt; Coefficients:
#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)    
#&gt; (Intercept)      5.63709    0.32025  17.602    memTyperecalled -0.02168    0.44955  -0.048    0.962    
#&gt; memTyperetold   -0.02781    0.55768  -0.050    0.960    
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; (Dispersion parameter for binomial family taken to be 1)
#&gt; 
#&gt;     Null deviance: 0.57625  on 6853  degrees of freedom
#&gt; Residual deviance: 0.57283  on 6851  degrees of freedom
#&gt; AIC: 55.455
#&gt; 
#&gt; Number of Fisher Scoring iterations: 9
</code></pre>
</div>
<p>Once again, we find no significant difference in surprise between remembered and recalled stories. However, CCR has a fundamental problem that needs to be addressed.</p>
<p>Embeddings capture the overall “vibes” of a text, including its tone and dialect. With CCR, we are comparing the “vibes” of a questionnaire written by academics to the “vibes” of narratives written by Hippocorpus participants. By comparing these vectors, we are not just measuring how much surprise is in each text—we are also measuring the extent to which each text is in the style of a questionnaire written by academics. This introduces a confounding variable into our analysis—questionnaire-ness.</p>
<p>The questionnaire-ness problem means that CCR is most effective for analyzing texts that bear a strong similarity to the questionnaire itself. For example, if you are analyzing participant descriptions of their own values, and your questionnaire items are statements about values in the first person (as any questionnaires are), CCR is likely to work well, especially with the improvement described in <a href="#sec-dimension-projection" class="quarto-xref"><span>Section 20.2.2</span></a> and <a href="#sec-ccr-improvement" class="quarto-xref"><span>Section 20.2.2.2</span></a>. With this method, you can compare participant responses to the questionnaire without actually administering the questionnaire itself; participants can answer in their own words, which CCR will compare to the wording of the questionnaire.</p>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advantages of CCR
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Can Apply Existing Questionnaires</strong></li>
<li><strong>Effectively Uses Contextualized Embeddings</strong></li>
<li>
<strong>Allows Free Response Items:</strong> Compares free-written participant responses with questionnaire wording.</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Disadvantages of CCR
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>
<strong>Limited Applicability:</strong> Less effective on texts that do not contain similar content and wording to the questionnaires</li>
<li>
<strong>Risks Measuring Questionnaire-ness:</strong> This risk can be mitigated by using an anchored vector (<a href="#sec-ccr-improvement" class="quarto-xref"><span>Section 20.2.2.2</span></a>)</li>
</ul>
</div>
</div>
</section></section><section id="reasoning-in-vector-space-beyond-cosine-similarity-and-dot-products" class="level2" data-number="20.2"><h2 data-number="20.2" class="anchored" data-anchor-id="reasoning-in-vector-space-beyond-cosine-similarity-and-dot-products">
<span class="header-section-number">20.2</span> Reasoning in Vector Space: Beyond Cosine Similarity and Dot Products</h2>
<section id="sec-parallelograms" class="level3" data-number="20.2.1"><h3 data-number="20.2.1" class="anchored" data-anchor-id="sec-parallelograms">
<span class="header-section-number">20.2.1</span> Additive Analogies</h3>
<p>Nearly every introduction to word embeddings opens with their analogical property. This is for good reason: it is extremely cool. Embeddings can be added to each other in order to arrive at new concepts. Here’s an example, using word2vec embeddings reduced to two dimensions with PCA:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="navigating-vectorspace_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid figure-img" width="768"></p>
</figure>
</div>
</div>
</div>
<p>If we subtract the embedding of “man” from the embedding of “woman”, we get the vector shown in blue. This vector represents the move from male to female gender. A vector between two embeddings is called an <strong>anchored vector</strong>. So when we add the man-woman anchored vector to the embedding of “aunt”, we get very close to the embedding of “uncle”. This property was first noted in word2vec <span class="citation" data-cites="mikolov_etal_2013">(<a href="#ref-mikolov_etal_2013" role="doc-biblioref">Mikolov et al., 2013</a>)</span>, and GloVe <span class="citation" data-cites="pennington_etal_2014">(<a href="#ref-pennington_etal_2014" role="doc-biblioref">Pennington et al., 2014</a>)</span> was specifically designed with it in mind.</p>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Additive Analogies in Contextualized Embeddings
</div>
</div>
<div class="callout-body-container callout-body">
<p>Notice that the analogical property relies on the magnitude of the vectors—if some vectors were shorter or longer than necessary, the parallelogram would not fit. This means that analogical reasoning may not be applicable to LLM embeddings, which are often organized in nonlinear patterns <span class="citation" data-cites="cai_etal_2021 ethayarajh_2019 gao_etal_2019">(<a href="#ref-cai_etal_2021" role="doc-biblioref">Cai et al., 2021</a>; <a href="#ref-ethayarajh_2019" role="doc-biblioref">Ethayarajh, 2019</a>; <a href="#ref-gao_etal_2019" role="doc-biblioref">Gao et al., 2019</a>)</span>. Even specialized models like SBERT are generally not designed with the additive analogical property in mind <span class="citation" data-cites="reimers_gurevych_2019">(<a href="#ref-reimers_gurevych_2019" role="doc-biblioref">Reimers &amp; Gurevych, 2019</a>)</span>. Even though some geometrically motivated methods work fairly well in LLM embeddings, as we will see in <a href="#sec-ccr-improvement" class="quarto-xref"><span>Section 20.2.2.2</span></a>, there is lots of room for improvement in this area.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
</div>
</div>
<p>The simplest application of the analogical property is to complete analogies like “telescope is to astronomy as ________ is to psychology.” You can find word2vec’s answer to this puzzle by subtracting the embedding of “telescope” from the embedding of “astronomy”, adding the result to the embedding of “psychology”, and finding the embedding with the lowest Euclidean distance to that vector.</p>
</section><section id="sec-dimension-projection" class="level3" data-number="20.2.2"><h3 data-number="20.2.2" class="anchored" data-anchor-id="sec-dimension-projection">
<span class="header-section-number">20.2.2</span> Anchored Vectors For Better Construct Representations</h3>
<p>There is a fundamental problem with all embeddings that additive analogical reasoning can help us solve. Consider the embeddings for “happy” and “sad”. These may seem like opposites, but actually they are likely to be very close to each other in vector space because they both relate to emotional valence. This means that if we try to measure the happiness of words by comparing their embeddings to the embedding for “happy”, we will actually be measuring the extent to which the words relate to emotion in general. The word “depression” might seem happier than the word “table”, since depression is more emotion-related. This problem can be solved by using <strong>anchored vectors</strong>. Just like we created an anchored vector between “man” and “woman” to represent masculinity (as opposed to femininity), we can create an anchored vector between “happy” and “sad” to represent happiness (as opposed to sadness). As we saw in <a href="#sec-parallelograms" class="quarto-xref"><span>Section 20.2.1</span></a>, anchored vectors can be applied wherever necessary in embedding space.</p>
<div class="cell">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">happy_vec</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">word2vec_mod</span>, <span class="st">"happy"</span>, type <span class="op">=</span> <span class="st">"embedding"</span><span class="op">)</span></span>
<span><span class="va">sad_vec</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">word2vec_mod</span>, <span class="st">"sad"</span>, type <span class="op">=</span> <span class="st">"embedding"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">happiness_anchor</span> <span class="op">&lt;-</span> <span class="va">happy_vec</span> <span class="op">-</span> <span class="va">sad_vec</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To measure constructs with an anchored vector, take the dot product of your text embeddings with the anchored vector. This is the equivalent of “projecting” the embeddings down onto the scale between one end of the anchored vector and the other.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="navigating-vectorspace_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid figure-img" width="768"></p>
</figure>
</div>
</div>
</div>
<p>By projecting each embedding down onto the anchored vector between happy and sad, we create a scale from happy to sad.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> This is sometimes referred to as <strong>semantic projection</strong> <span class="citation" data-cites="grand_etal_2022">(<a href="#ref-grand_etal_2022" role="doc-biblioref">Grand et al., 2022</a>)</span>.</p>
<section id="sec-ddr-improvement" class="level4" data-number="20.2.2.1"><h4 data-number="20.2.2.1" class="anchored" data-anchor-id="sec-ddr-improvement">
<span class="header-section-number">20.2.2.1</span> Improving DDR With Anchored Vectors</h4>
<p>In <a href="word-counting.html#sec-polarity" class="quarto-xref"><span>Section 14.4</span></a>, we used two dictionaries to measure surprise <em>as opposed to anticipation</em> with word counts. By creating an anchored vector between surprise and anticipation, we can now replicate that analysis using DDR. The first step is to create a DDR for each dictionary. Since we already have one for surprise from <a href="#sec-ddr" class="quarto-xref"><span>Section 20.1.1</span></a>, we just need to replicate the process for anticipation.</p>
<div class="cell">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># get dictionary</span></span>
<span><span class="va">anticipation_dict</span> <span class="op">&lt;-</span> <span class="fu">quanteda.sentiment</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/quanteda.sentiment/man/data_dictionary_NRC.html">data_dictionary_NRC</a></span><span class="op">$</span><span class="va">anticipation</span></span>
<span></span>
<span><span class="co"># estimate frequency of dictionary words</span></span>
<span><span class="va">anticipation_dict_freqs</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_dfm</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">dfm_keep</span><span class="op">(</span><span class="va">anticipation_dict</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">quanteda.textstats</span><span class="fu">::</span><span class="fu"><a href="https://quanteda.io/reference/textstat_frequency.html">textstat_frequency</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="va">feature</span>, <span class="va">frequency</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># word2vec embeddings of dictionary words</span></span>
<span><span class="va">anticipation_ddr</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">word2vec_mod</span>, <span class="va">anticipation_dict</span>, type <span class="op">=</span> <span class="st">"embedding"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">as_tibble</span><span class="op">(</span>rownames <span class="op">=</span> <span class="st">"feature"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">left_join</span><span class="op">(</span><span class="va">anticipation_dict_freqs</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">replace_na</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>frequency <span class="op">=</span> <span class="fl">0</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># average dictionary embedding (weighted by frequency)</span></span>
<span><span class="va">anticipation_ddr</span> <span class="op">&lt;-</span> <span class="va">anticipation_ddr</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">summarise</span><span class="op">(</span><span class="fu">across</span><span class="op">(</span><span class="va">V1</span><span class="op">:</span><span class="va">V300</span>, <span class="op">~</span><span class="fu"><a href="https://rdrr.io/r/stats/weighted.mean.html">weighted.mean</a></span><span class="op">(</span><span class="va">.x</span>, <span class="va">frequency</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="va">V1</span><span class="op">:</span><span class="va">V300</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/unlist.html">unlist</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now that we have DDRs for both surprise and anticipation, we can create an anchored vector between them:</p>
<div class="cell">
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">surprise_ddr_anchored</span> <span class="op">&lt;-</span> <span class="va">surprise_ddr</span> <span class="op">-</span> <span class="va">anticipation_ddr</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now score the Hippocorpus texts by the dot product between their word2vec embeddings and the anchored vector, effectively projecting each one onto a scale between anticipation and surprise.</p>
<div class="cell">
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># score documents by surprise</span></span>
<span><span class="va">hippocorpus_surprise_ddr_anchored</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_word2vec</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">rowwise</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>surprise <span class="op">=</span> <span class="fu">dot_prod</span><span class="op">(</span><span class="fu">c_across</span><span class="op">(</span><span class="va">V1</span><span class="op">:</span><span class="va">V300</span><span class="op">)</span>, <span class="va">surprise_ddr_anchored</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">ungroup</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">V1</span><span class="op">:</span><span class="va">V300</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># rejoin docvars</span></span>
<span><span class="va">hippocorpus_surprise_ddr_anchored</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_surprise_ddr_anchored</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">bind_cols</span><span class="op">(</span><span class="fu">docvars</span><span class="op">(</span><span class="va">hippocorpus_corp</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Since the scale is theoretically infinite (a text could have more surprise than the average dictionary embedding for surprise), we can analyze it with a standard linear regression.</p>
<div class="cell">
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">surprise_mod_ddr_anchored</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span></span>
<span>  <span class="va">surprise</span> <span class="op">~</span> <span class="va">memType</span>,</span>
<span>  data <span class="op">=</span> <span class="va">hippocorpus_surprise_ddr_anchored</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">surprise_mod_ddr_anchored</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="r-output"><code>#&gt; 
#&gt; Call:
#&gt; lm(formula = surprise ~ memType, data = hippocorpus_surprise_ddr_anchored)
#&gt; 
#&gt; Residuals:
#&gt;      Min       1Q   Median       3Q      Max 
#&gt; -2.65062 -0.36894 -0.00133  0.37938  2.64127 
#&gt; 
#&gt; Coefficients:
#&gt;                 Estimate Std. Error  t value Pr(&gt;|t|)    
#&gt; (Intercept)     -4.19667    0.01084 -387.242    memTyperecalled  0.01157    0.01529    0.757   0.4493    
#&gt; memTyperetold   -0.04791    0.01905   -2.515   0.0119 *  
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; Residual standard error: 0.5689 on 6851 degrees of freedom
#&gt; Multiple R-squared:  0.001468,   Adjusted R-squared:  0.001176 
#&gt; F-statistic: 5.035 on 2 and 6851 DF,  p-value: 0.006531
</code></pre>
</div>
<p>We found no significant difference between imagined and recalled stories, but we did find a significant difference between imagined and retold stories such that retold stories had slightly less surprise as opposed to anticipation.</p>
</section><section id="sec-ccr-improvement" class="level4" data-number="20.2.2.2"><h4 data-number="20.2.2.2" class="anchored" data-anchor-id="sec-ccr-improvement">
<span class="header-section-number">20.2.2.2</span> Improving CCR With Anchored Vectors</h4>
<p>Remember the questionnaire-ness problem with CCR from <a href="#sec-ccr" class="quarto-xref"><span>Section 20.1.2</span></a>? Anchored vectors can help us solve this problem. This time, let’s just negate each item from the surprise questionnaire, like this:</p>
<div class="cell">
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">surprise_items_pos</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span></span>
<span>  <span class="st">"I was extremely surprised by the outcome of the event."</span>,</span>
<span>  <span class="st">"The outcome of the event was extremely interesting."</span>,</span>
<span>  <span class="st">"The outcome of the event was extremely new."</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="va">surprise_items_neg</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span></span>
<span>  <span class="st">"I was not surprised at all by the outcome of the event."</span>,</span>
<span>  <span class="st">"The outcome of the event was not interesting at all."</span>,</span>
<span>  <span class="st">"The outcome of the event was not new at all."</span></span>
<span>  <span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This approach has the advantage of maintaining most of the original wording. By creating an anchored vector between the positive and negative CCRs, we can disregard this questionnaire-y wording, focusing only on the direction between lots of surprise and no surprise at all. Even though this approach makes big assumptions about the linearity of the contextualized embedding space (<a href="#sec-parallelograms" class="quarto-xref"><span>Section 20.2.1</span></a>), it has been shown to work fairly well for a variety of constructs and models <span class="citation" data-cites="grand_etal_2022">(<a href="#ref-grand_etal_2022" role="doc-biblioref">Grand et al., 2022</a>)</span>. It is particularly applicable to the Hippocorpus data, since the texts are first-person narratives about an event, just like the questionnaire items.</p>
<p>Let’s create the new anchored CCR and use it to reanalyze the Hippocorpus data.</p>
<div class="cell">
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># embed items (using the same model as we used before)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-text.org/">text</a></span><span class="op">)</span></span>
<span>  </span>
<span><span class="va">surprise_neg_sbert</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://r-text.org/reference/textEmbed.html">textEmbed</a></span><span class="op">(</span></span>
<span>  <span class="va">surprise_items_neg</span>,</span>
<span>  model <span class="op">=</span> <span class="st">"sentence-transformers/all-MiniLM-L12-v2"</span>, <span class="co"># model name</span></span>
<span>  layers <span class="op">=</span> <span class="op">-</span><span class="fl">2</span>,  <span class="co"># second to last layer (default)</span></span>
<span>  tokens_select <span class="op">=</span> <span class="st">"[CLS]"</span>, <span class="co"># use only [CLS] token</span></span>
<span>  dim_name <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  keep_token_embeddings <span class="op">=</span> <span class="cn">FALSE</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># compute negative CCR by averaging item embeddings</span></span>
<span><span class="va">surprise_neg_ccr</span> <span class="op">&lt;-</span> <span class="va">surprise_neg_sbert</span><span class="op">$</span><span class="va">texts</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">summarise</span><span class="op">(</span><span class="fu">across</span><span class="op">(</span><span class="fu">everything</span><span class="op">(</span><span class="op">)</span>, <span class="va">mean</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/unlist.html">unlist</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">surprise_ccr_anchored</span> <span class="op">&lt;-</span> <span class="va">surprise_ccr</span> <span class="op">-</span> <span class="va">surprise_neg_ccr</span></span>
<span></span>
<span><span class="co"># score documents by surprise</span></span>
<span><span class="va">hippocorpus_surprise_ccr_anchored</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_sbert</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">rowwise</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>surprise <span class="op">=</span> <span class="fu">dot_prod</span><span class="op">(</span><span class="fu">c_across</span><span class="op">(</span><span class="va">Dim1</span><span class="op">:</span><span class="va">Dim384</span><span class="op">)</span>, <span class="va">surprise_ccr_anchored</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">ungroup</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">Dim1</span><span class="op">:</span><span class="va">Dim384</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># linear regression</span></span>
<span><span class="va">surprise_mod_ccr_anchored</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span></span>
<span>  <span class="va">surprise</span> <span class="op">~</span> <span class="va">memType</span>, </span>
<span>  <span class="va">hippocorpus_surprise_ccr_anchored</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">surprise_mod_ccr_anchored</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="r-output"><code>#&gt; 
#&gt; Call:
#&gt; lm(formula = surprise ~ memType, data = hippocorpus_surprise_ccr_anchored)
#&gt; 
#&gt; Residuals:
#&gt;      Min       1Q   Median       3Q      Max 
#&gt; -0.39157 -0.06969 -0.00115  0.06810  0.41258 
#&gt; 
#&gt; Coefficients:
#&gt;                 Estimate Std. Error t value Pr(&gt;|t|)    
#&gt; (Intercept)     1.061703   0.001922 552.448   memTyperecalled 0.010240   0.002712   3.775 0.000161 ***
#&gt; memTyperetold   0.012183   0.003378   3.607 0.000312 ***
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; Residual standard error: 0.1009 on 6851 degrees of freedom
#&gt; Multiple R-squared:  0.00283,    Adjusted R-squared:  0.002539 
#&gt; F-statistic: 9.721 on 2 and 6851 DF,  p-value: 6.083e-05
</code></pre>
</div>
<p>We found a significant difference between imagined and recalled stories such that recalled stories had more surprising content (p &lt; .001)! We also found that retold stories had more surprising content than imagined stories (p &lt; .001). These results support Sap et al.’s hypothesis that true autobiographical stories would include more surprising events than imagined stories.</p>
<p><strong>An example of using anchored vectors and CCR in research:</strong> <span class="citation" data-cites="simchon_etal_2023">Simchon et al. (<a href="#ref-simchon_etal_2023" role="doc-biblioref">2023</a>)</span> collected 10,000 posts from the <a href="https://www.reddit.com/r/depression">r/depression</a> subreddit, along with a control group of 100 posts each from 100 randomly selected subreddits. They then used a variant of SBERT, <code>all-MiniLM-L6-v2</code> (see <a href="contextualized-embeddings.html" class="quarto-xref"><span>Chapter 19</span></a>), to compute CCR embeddings of a psychological questionnaire measuring “locus of control,” the feeling that you have control over your own life. The questionnaire included items measuring an internal locus of control (“I have control”), and items measuring an external locus of control (“External forces have control”). Simchon et al.&nbsp;constructed an anchored vector to capture the direction between internal and external locus of control, and projected embeddings of the Reddit posts onto that vector to measure how much each post reflected an internal vs.&nbsp;an external locus of control. They found that posts in r/depression exhibited a more external locus of control than posts in the control group.</p>
</section></section><section id="sec-correlational-anchors" class="level3" data-number="20.2.3"><h3 data-number="20.2.3" class="anchored" data-anchor-id="sec-correlational-anchors">
<span class="header-section-number">20.2.3</span> Correlational Anchored Vectors</h3>
<p>In <a href="dla.html#sec-generating-dictionaries" class="quarto-xref"><span>Section 15.4</span></a>, we used the <a href="https://data.world/crowdflower/sentiment-analysis-in-text">Crowdflower Emotion in Text dataset</a> to generate a new dictionary for the emotion of surprise. We can use a similar approach to generate an anchored vector. Remember that the anchored vector for surprise is simply a direction in the embedding space. Rather than finding this direction by subtracting a negative construct embedding from a positive one (as we did in <a href="#sec-ccr-improvement" class="quarto-xref"><span>Section 20.2.2.2</span></a> and <a href="#sec-ccr-improvement" class="quarto-xref"><span>Section 20.2.2.2</span></a>), we can use machine learning to find the direction that best represents surprise in a training dataset.</p>
<p>To train an anchored vector on the Crowdflower dataset, we will first need to embed its 40,000 Twitter posts. We will do this just as we did for the Hippocorpus texts in <a href="decontextualized-embeddings.html#sec-word2vec" class="quarto-xref"><span>Section 18.3.1</span></a>.</p>
<div class="cell">
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># data from https://data.world/crowdflower/sentiment-analysis-in-text</span></span>
<span><span class="va">crowdflower</span> <span class="op">&lt;-</span> <span class="fu">read_csv</span><span class="op">(</span><span class="st">"data/text_emotion.csv"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">rename</span><span class="op">(</span>text <span class="op">=</span> <span class="va">content</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span></span>
<span>    doc_id <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/character.html">as.character</a></span><span class="op">(</span><span class="va">tweet_id</span><span class="op">)</span>,</span>
<span>    surprise <span class="op">=</span> <span class="fu">if_else</span><span class="op">(</span><span class="va">sentiment</span> <span class="op">==</span> <span class="st">"surprise"</span>, <span class="st">"surprise"</span>, <span class="st">"no surprise"</span><span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="va">crowdflower_dfm</span> <span class="op">&lt;-</span> <span class="va">crowdflower</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">corpus</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">tokens</span><span class="op">(</span>remove_punct <span class="op">=</span> <span class="cn">TRUE</span>, remove_url <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">dfm</span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># word2vec document embeddings</span></span>
<span><span class="va">crowdflower_word2vec</span> <span class="op">&lt;-</span> <span class="va">crowdflower_dfm</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">textstat_embedding</span><span class="op">(</span><span class="va">word2vec_mod</span><span class="op">)</span></span>
<span></span>
<span><span class="va">crowdflower</span> <span class="op">&lt;-</span> <span class="va">crowdflower</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">left_join</span><span class="op">(</span><span class="va">crowdflower_word2vec</span>, by <span class="op">=</span> <span class="st">"doc_id"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>With Partial Least Squares (PLS) regression, which finds directions in the feature space that best correlate with the dependent variable (in this case, surprise), we can create a <strong>correlational anchored vector</strong>.</p>
<div class="cell">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/topepo/caret/">caret</a></span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="r-output"><code>#&gt; Loading required package: lattice
</code></pre>
<pre class="r-output"><code>#&gt; 
#&gt; Attaching package: 'caret'
</code></pre>
<pre class="r-output"><code>#&gt; The following object is masked from 'package:purrr':
#&gt; 
#&gt;     lift
</code></pre>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">2024</span><span class="op">)</span></span>
<span><span class="va">pls_surprise</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/train.html">train</a></span><span class="op">(</span></span>
<span>  <span class="va">surprise</span> <span class="op">~</span> <span class="va">.</span>, </span>
<span>  data <span class="op">=</span> <span class="fu">select</span><span class="op">(</span><span class="va">crowdflower</span>, <span class="va">surprise</span>, <span class="va">V1</span><span class="op">:</span><span class="va">V300</span><span class="op">)</span>, </span>
<span>  method <span class="op">=</span> <span class="st">"pls"</span>,</span>
<span>  scale <span class="op">=</span> <span class="cn">FALSE</span>,  <span class="co"># keep original embedding dimensions</span></span>
<span>  trControl <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/trainControl.html">trainControl</a></span><span class="op">(</span><span class="st">"cv"</span>, number <span class="op">=</span> <span class="fl">10</span><span class="op">)</span>,  <span class="co"># cross-validation</span></span>
<span>  tuneLength <span class="op">=</span> <span class="fl">1</span>  <span class="co"># only 1 component (our anchored vector)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">surprise_anchored_pls</span> <span class="op">&lt;-</span> <span class="va">pls_surprise</span><span class="op">$</span><span class="va">finalModel</span><span class="op">$</span><span class="va">projection</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>With the new correlational anchored vector, we can redo our analysis from <a href="#sec-ddr" class="quarto-xref"><span>Section 20.1.1</span></a>.</p>
<div class="cell">
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># score documents by surprise</span></span>
<span><span class="va">hippocorpus_surprise_anchored_pls</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_word2vec</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">rowwise</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>surprise <span class="op">=</span> <span class="fu">dot_prod</span><span class="op">(</span><span class="fu">c_across</span><span class="op">(</span><span class="va">V1</span><span class="op">:</span><span class="va">V300</span><span class="op">)</span>, <span class="va">surprise_anchored_pls</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">ungroup</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">V1</span><span class="op">:</span><span class="va">V300</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># rejoin docvars</span></span>
<span><span class="va">hippocorpus_surprise_anchored_pls</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_surprise_anchored_pls</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">bind_cols</span><span class="op">(</span><span class="fu">docvars</span><span class="op">(</span><span class="va">hippocorpus_corp</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">surprise_mod_anchored_pls</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span></span>
<span>  <span class="va">surprise</span> <span class="op">~</span> <span class="va">memType</span>,</span>
<span>  data <span class="op">=</span> <span class="va">hippocorpus_surprise_anchored_pls</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">surprise_mod_anchored_pls</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="r-output"><code>#&gt; 
#&gt; Call:
#&gt; lm(formula = surprise ~ memType, data = hippocorpus_surprise_anchored_pls)
#&gt; 
#&gt; Residuals:
#&gt;      Min       1Q   Median       3Q      Max 
#&gt; -1.28741 -0.20195  0.00297  0.21233  1.03080 
#&gt; 
#&gt; Coefficients:
#&gt;                  Estimate Std. Error  t value Pr(&gt;|t|)    
#&gt; (Intercept)     -1.506064   0.005941 -253.519    memTyperecalled  0.105878   0.008384   12.629    memTyperetold    0.093503   0.010442    8.955    ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; Residual standard error: 0.3119 on 6851 degrees of freedom
#&gt; Multiple R-squared:  0.02523,    Adjusted R-squared:  0.02494 
#&gt; F-statistic: 88.65 on 2 and 6851 DF,  p-value: </code></pre>
</div>
<p>Once again we find significant results in support of <span class="citation" data-cites="sap_etal_2022">Sap et al. (<a href="#ref-sap_etal_2022" role="doc-biblioref">2022</a>)</span>!</p>
</section><section id="sec-machine-learning-methods" class="level3" data-number="20.2.4"><h3 data-number="20.2.4" class="anchored" data-anchor-id="sec-machine-learning-methods">
<span class="header-section-number">20.2.4</span> Machine Learning Methods</h3>
<p>After <a href="#sec-correlational-anchors" class="quarto-xref"><span>Section 20.2.3</span></a>, you may wonder why we stopped at a single direction in embedding space. Why not go all out with the machine learning? If you wondered this, great job! Psychologists are increasingly training machine learning algorithms on text embeddings to quantify relevant constructs <span class="citation" data-cites="kjell_etal_2022">(<a href="#ref-kjell_etal_2022" role="doc-biblioref">Kjell et al., 2022</a>)</span>. Indeed, this is the approach used to generate <a href="https://github.com/rimonim/ds4psych/blob/main/cover.R">the cover of this book</a>.</p>
<p>With machine learning approaches, the nonlinearity of contextualized embedding spaces becomes less of a problem.</p>
<p>The <code>text</code> package provides</p>
<p>A few pretrained embedding-based valence models: https://osf.io/v4jb5</p>
<p><span class="citation" data-cites="chersoni_etal_2021">Chersoni et al. (<a href="#ref-chersoni_etal_2021" role="doc-biblioref">2021</a>)</span> used partial least squares regression to map distributional embeddings to human-rated semantic features derived from research in cognitive psychology.</p>
<p>Some research advises using both the <code>[CLS]</code> token and an aggregation of the other token embeddings <span class="citation" data-cites="lee_etal_2023">(<a href="#ref-lee_etal_2023" role="doc-biblioref">Lee et al., 2023</a>)</span></p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-atari_etal_2023" class="csl-entry" role="listitem">
Atari, M., Omrani, A., &amp; Dehghani, M. (2023). <em>Contextualized construct representation: Leveraging psychometric scales to advance theory-driven text analysis</em>. PsyArXiv. <a href="https://doi.org/10.31234/osf.io/m93pd">https://doi.org/10.31234/osf.io/m93pd</a>
</div>
<div id="ref-cai_etal_2021" class="csl-entry" role="listitem">
Cai, X., Huang, J., Bian, Y., &amp; Church, K. (2021). Isotropy in the contextual embedding space: Clusters and manifolds. <em>International Conference on Learning Representations</em>. <a href="https://openreview.net/forum?id=xYGNO86OWDH">https://openreview.net/forum?id=xYGNO86OWDH</a>
</div>
<div id="ref-chersoni_etal_2021" class="csl-entry" role="listitem">
Chersoni, E., Santus, E., Huang, C.-R., &amp; Lenci, A. (2021). Decoding word embeddings with brain-based semantic features. <em>Computational Linguistics</em>, <em>47</em>(3), 663–698. <a href="https://doi.org/10.1162/coli_a_00412">https://doi.org/10.1162/coli_a_00412</a>
</div>
<div id="ref-choi_choi_2010" class="csl-entry" role="listitem">
Choi, D., &amp; Choi, I. (2010). A comparison of hindsight bias in groups and individuals: The moderating role of plausibility. <em>Journal of Applied Social Psychology</em>, <em>40</em>(2), 325–343. <a href="https://search.ebscohost.com/login.aspx?direct=true&amp;amp;db=sxi&amp;amp;AN=48116256&amp;amp;site=ehost-live">https://search.ebscohost.com/login.aspx?direct=true&amp;amp;db=sxi&amp;amp;AN=48116256&amp;amp;site=ehost-live</a>
</div>
<div id="ref-choi_nisbett_2000" class="csl-entry" role="listitem">
Choi, I., &amp; Nisbett, R. E. (2000). Cultural psychology of surprise: Holistic theories and recognition of contradiction. <em>Journal of Personality and Social Psychology</em>, <em>79</em>(6), 890–905.
</div>
<div id="ref-ethayarajh_2019" class="csl-entry" role="listitem">
Ethayarajh, K. (2019). <em>How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings</em>. <a href="https://arxiv.org/abs/1909.00512">https://arxiv.org/abs/1909.00512</a>
</div>
<div id="ref-gao_etal_2019" class="csl-entry" role="listitem">
Gao, J., He, D., Tan, X., Qin, T., Wang, L., &amp; Liu, T.-Y. (2019). <em>Representation degeneration problem in training natural language generation models</em>. <a href="https://arxiv.org/abs/1907.12009">https://arxiv.org/abs/1907.12009</a>
</div>
<div id="ref-garten_etal_2018" class="csl-entry" role="listitem">
Garten, J., Hoover, J., Johnson, K. M., Boghrati, R., Iskiwitch, C., &amp; Dehghani, M. (2018). Dictionaries and distributions: Combining expert knowledge and large scale textual data content analysis: Distributed dictionary representation. <em>Behavior Research Methods</em>, <em>50</em>, 344–361.
</div>
<div id="ref-grand_etal_2022" class="csl-entry" role="listitem">
Grand, G., Blank, I. A., Pereira, F., &amp; Fedorenko, E. (2022). Semantic projection recovers rich human knowledge of multiple object features from word embeddings. <em>Nature Human Behaviour</em>, <em>6</em>(7), 975–987. <a href="https://doi.org/10.1038/s41562-022-01316-8">https://doi.org/10.1038/s41562-022-01316-8</a>
</div>
<div id="ref-kjell_etal_2022" class="csl-entry" role="listitem">
Kjell, O., Sikström, S., Kjell, K., &amp; Schwartz, H. (2022). Natural language analyzed with AI-based transformers predict traditional subjective well-being measures approaching the theoretical upper limits in accuracy. <em>Scientific Reports</em>, <em>12</em>, 3918. <a href="https://doi.org/10.1038/s41598-022-07520-w">https://doi.org/10.1038/s41598-022-07520-w</a>
</div>
<div id="ref-lee_etal_2023" class="csl-entry" role="listitem">
Lee, K., Choi, G., &amp; Choi, C. (2023). Use all tokens method to improve semantic relationship learning. <em>Expert Systems with Applications</em>, <em>233</em>, 120911. https://doi.org/<a href="https://doi.org/10.1016/j.eswa.2023.120911">https://doi.org/10.1016/j.eswa.2023.120911</a>
</div>
<div id="ref-li_etal_2020" class="csl-entry" role="listitem">
Li, B., Zhou, H., He, J., Wang, M., Yang, Y., &amp; Li, L. (2020). <em>On the sentence embeddings from pre-trained language models</em>. <a href="https://arxiv.org/abs/2011.05864">https://arxiv.org/abs/2011.05864</a>
</div>
<div id="ref-mikolov_etal_2013" class="csl-entry" role="listitem">
Mikolov, T., Yih, W., &amp; Zweig, G. (2013). Linguistic regularities in continuous space word representations. In L. Vanderwende, H. Daumé III, &amp; K. Kirchhoff (Eds.), <em>Proceedings of the 2013 conference of the north <span>A</span>merican chapter of the association for computational linguistics: Human language technologies</em> (pp. 746–751). Association for Computational Linguistics. <a href="https://aclanthology.org/N13-1090">https://aclanthology.org/N13-1090</a>
</div>
<div id="ref-mohammad_turney_2013" class="csl-entry" role="listitem">
Mohammad, S. M., &amp; Turney, P. D. (2013). Crowdsourcing a word-emotion association lexicon. <em>Computational Intelligence</em>, <em>29</em>(3), 436–465.
</div>
<div id="ref-mohammad_turney_2010" class="csl-entry" role="listitem">
Mohammad, S., &amp; Turney, P. (2010). Emotions evoked by common words and phrases: Using <span>M</span>echanical <span>T</span>urk to create an emotion lexicon. <em>Proceedings of the <span>NAACL</span> <span>HLT</span> 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text</em>, 26–34. <a href="https://aclanthology.org/W10-0204">https://aclanthology.org/W10-0204</a>
</div>
<div id="ref-pennington_etal_2014" class="csl-entry" role="listitem">
Pennington, J., Socher, R., &amp; Manning, C. D. (2014). GloVe: Global vectors for word representation. <em>Empirical Methods in Natural Language Processing (EMNLP)</em>, 1532–1543. <a href="http://www.aclweb.org/anthology/D14-1162">http://www.aclweb.org/anthology/D14-1162</a>
</div>
<div id="ref-reimers_gurevych_2019" class="csl-entry" role="listitem">
Reimers, N., &amp; Gurevych, I. (2019). Sentence-<span>BERT</span>: Sentence embeddings using <span>S</span>iamese <span>BERT</span>-networks. In K. Inui, J. Jiang, V. Ng, &amp; X. Wan (Eds.), <em>Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)</em> (pp. 3982–3992). Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D19-1410">https://doi.org/10.18653/v1/D19-1410</a>
</div>
<div id="ref-sap_etal_2022" class="csl-entry" role="listitem">
Sap, M., Jafarpour, A., Choi, Y., Smith, N. A., Pennebaker, J. W., &amp; Horvitz, E. (2022). Quantifying the narrative flow of imagined versus autobiographical stories. <em>Proceedings of the National Academy of Sciences</em>, <em>119</em>(45), e2211715119. <a href="https://doi.org/10.1073/pnas.2211715119">https://doi.org/10.1073/pnas.2211715119</a>
</div>
<div id="ref-simchon_etal_2023" class="csl-entry" role="listitem">
Simchon, A., Hadar, B., &amp; Gilead, M. (2023). A computational text analysis investigation of the relation between personal and linguistic agency. <em>Communications Psychology</em>, 1–9. <a href="https://doi.org/10.1038/s44271-023-00020-1">https://doi.org/10.1038/s44271-023-00020-1</a>
</div>
</div>
</section></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr>
<ol>
<li id="fn1"><p>Cosine similarity is appropriate here because our contextualized embeddings were generated by an SBERT model, which was designed to be used with cosine similarity. If we had used another model such as RoBERTa, Euclidean distance might be more appropriate.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>There are some promising methods for getting more geometrically regular embeddings out of LLMs. For example, averaging the last two layers of the model seems to help <span class="citation" data-cites="li_etal_2020">(<a href="#ref-li_etal_2020" role="doc-biblioref">Li et al., 2020</a>)</span>. Taking a different approach, <span class="citation" data-cites="ethayarajh_2019">Ethayarajh (<a href="#ref-ethayarajh_2019" role="doc-biblioref">2019</a>)</span> created static word embeddings from an LLM by running it on a large corpus and taking the set of each word’s contextualized representations from all the places it appears in the corpus. The loadings of the first principal component of this set represent the dimensions along which the meaning of the word changes across different contexts. These loadings can themselves be used as a vector embedding which can out-perform GloVe and FastText embeddings on many word vector benchmarks, including analogy solving. This approach worked best for embeddings from the early layers of the LLM.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>For an intuitive explanation of why the dot product is equivalent to a projection, see <a href="https://youtu.be/LyGKycYT2v0?si=86cfrN9DP9xw5HUx">3blue1brown’s video on the subject.</a>. Incidentally, the dot product with the anchored vector is also equivalent to the dot product with the positive embedding (e.g.&nbsp;“happy”) minus the dot product with the negative vector (e.g.&nbsp;“sad”).<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Taking the dot product with an anchored vector yields an unstandardized version of this scale. If you want “sad” to be 0 and “happy” to be 1 on the scale, use the <code>anchored_sim()</code> function included in <a href="https://github.com/rimonim/ds4psych/blob/main/vector_scripts.R">our Github repo</a>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/ds4psych\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./contextualized-embeddings.html" class="pagination-link" aria-label="Contextualization With Large Language Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Contextualization With Large Language Models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./lda.html" class="pagination-link" aria-label="Topic Modeling">
        <span class="nav-page-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Topic Modeling</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>Data Science for Psychology was written by <a href="https://rimonim.github.io">Louis Teitelbaum</a> and <a href="https://almogsi.com">Almog Simchon</a>.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/rimonim/ds4psych/blob/main/navigating-vectorspace.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/rimonim/ds4psych/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a> and is powered by <a href="https://www.netlify.com/">Netlify</a></p>
</div>
  </div>
</footer>


</body></html>