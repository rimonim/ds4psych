<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Louis Teitelbaum and Almog Simchon">
<title>Data Science for Psychology: Natural Language - 18&nbsp; Word Embeddings</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./contextualized-embeddings.html" rel="next">
<link href="./vectorspace-intro.html" rel="prev">
<link href="./images/favicon.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><link href="site_libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet">
<script src="site_libs/htmlwidgets-1.6.4/htmlwidgets.js"></script><script src="site_libs/plotly-binding-4.10.4/plotly.js"></script><script src="site_libs/typedarray-0.1/typedarray.min.js"></script><script src="site_libs/jquery-3.5.1/jquery.min.js"></script><link href="site_libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet">
<script src="site_libs/crosstalk-1.2.1/js/crosstalk.min.js"></script><link href="site_libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet">
<script src="site_libs/plotly-main-2.11.1/plotly-latest.min.js"></script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./quantification.html">Quantifying Psychological Properties of Text</a></li><li class="breadcrumb-item"><a href="./decontextualized-embeddings.html"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Word Embeddings</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Data Science for Psychology: Natural Language</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/rimonim/ds4psych" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
<li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=%7Curl%7C">
              <i class="bi bi-bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=%7Curl%7C">
              <i class="bi bi-bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=%7Curl%7C">
              <i class="bi bi-bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
      </ul>
</div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction: Why Does Psychology Need Natural Language?</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ethics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Ethics of Data Science in Psychology</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./data-viz.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Visualization</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./aesthetics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Why Aesthetic Choices are Important</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./telling-a-story.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Don’t Distract From the Story</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./word-viz.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Visualizing Distributions of Words</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data-viz-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Additional Resources for Data Visualization</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./data-sources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sources of Data</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./experiments.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Experiments</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./corpora.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Corpus Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./apis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Web APIs</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./scraping.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Web Scraping</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./quantification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quantifying Psychological Properties of Text</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./look-at-your-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Look at Your Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./quanteda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Introduction to Quanteda</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tokenization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Tokenization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./word-counting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Dictionary-Based Word Counts</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dla.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Open Vocabulary Word Counting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./word-counting-improvements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Transforming Word Counts</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./vectorspace-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Introduction to Vector Space</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./decontextualized-embeddings.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Word Embeddings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./contextualized-embeddings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Contextualization With Large Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./navigating-vectorspace.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Navigating Vector Space</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./lda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Topic Modeling</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linguistic-complexity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Measures of Linguistic Complexity</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./querying_llms.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Just Ask an LLM</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./audio-video-image.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Audio, Video, and Image Data</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./transcription.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Transcribing Audio</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Additional Resources</span></span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#the-distributional-hypothesis" id="toc-the-distributional-hypothesis" class="nav-link active" data-scroll-target="#the-distributional-hypothesis"><span class="header-section-number">18.1</span> The Distributional Hypothesis</a></li>
  <li>
<a href="#sec-lsa" id="toc-sec-lsa" class="nav-link" data-scroll-target="#sec-lsa"><span class="header-section-number">18.2</span> LSA</a>
  <ul class="collapse">
<li><a href="#sec-lsa-variations" id="toc-sec-lsa-variations" class="nav-link" data-scroll-target="#sec-lsa-variations"><span class="header-section-number">18.2.1</span> Variations on LSA</a></li>
  </ul>
</li>
  <li>
<a href="#sec-word-embeddings" id="toc-sec-word-embeddings" class="nav-link" data-scroll-target="#sec-word-embeddings"><span class="header-section-number">18.3</span> Advanced Word Embeddings</a>
  <ul class="collapse">
<li><a href="#sec-word2vec" id="toc-sec-word2vec" class="nav-link" data-scroll-target="#sec-word2vec"><span class="header-section-number">18.3.1</span> Word2vec</a></li>
  <li><a href="#sec-glove" id="toc-sec-glove" class="nav-link" data-scroll-target="#sec-glove"><span class="header-section-number">18.3.2</span> GloVe</a></li>
  <li><a href="#sec-fasttext" id="toc-sec-fasttext" class="nav-link" data-scroll-target="#sec-fasttext"><span class="header-section-number">18.3.3</span> FastText</a></li>
  <li><a href="#sec-embedding-magnitude" id="toc-sec-embedding-magnitude" class="nav-link" data-scroll-target="#sec-embedding-magnitude"><span class="header-section-number">18.3.4</span> Interpreting Advanced Word Embeddings</a></li>
  </ul>
</li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/rimonim/ds4psych/blob/main/decontextualized-embeddings.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/rimonim/ds4psych/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./quantification.html">Quantifying Psychological Properties of Text</a></li><li class="breadcrumb-item"><a href="./decontextualized-embeddings.html"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Word Embeddings</span></a></li></ol></nav><div class="quarto-title">
<h1 class="title"><span id="sec-decontextualized-embeddings" class="quarto-section-identifier"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Word Embeddings</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><section id="the-distributional-hypothesis" class="level2" data-number="18.1"><h2 data-number="18.1" class="anchored" data-anchor-id="the-distributional-hypothesis">
<span class="header-section-number">18.1</span> The Distributional Hypothesis</h2>
<p>This textbook assumes that words have psychologically interesting content. For example, certain words are associated with surprise, while others may be associated with concrete thought. But what does it mean for a word to be associated with an emotion or a cognitive process? How do words come to have any meaning at all? One answer: People associate a word with surprise because they often hear it in surprising situations. Because people associate surprise-related words with surprising situations, they use those words more when they are thinking about surprising situations.</p>
<p>So teaching a computer to recognize surprise-related words should be simple, right? We’ll just tell the computer to look for words that tend to appear in surprising situations! But there’s a problem: Computers don’t get surprised, and they have no idea what a surprising situation is.</p>
<p>According to <strong>the distributional hypothesis</strong>, our problem is actually not a problem at all. The computer might not know what surprise is, but it doesn’t need to. It doesn’t need to know what anything <em>is</em>—it just needs to know how everything is related to everything else. To do this, it just needs to notice what appears next to what. <strong>Similar words appear in similar contexts</strong>. For example, consider the following two sentences from the paper that introduced the distributional hypothesis, <span class="citation" data-cites="harris_1954">Harris (<a href="#ref-harris_1954" role="doc-biblioref">1954</a>, emphasis added)</span>.</p>
<blockquote class="blockquote">
<p>“The formation of new <em>utterances</em> in the <em>language</em> is therefore based on the distributional relations as changeably perceived by the <em>speakers</em>-among the parts of the previously heard <em>utterances</em>.”</p>
</blockquote>
<blockquote class="blockquote">
<p>“The correlation between <em>language</em> and <em>meaning</em> is much greater when we consider connected discourse.”</p>
</blockquote>
<p>Even if we have no idea what “utterances” or “meaning” are, we can learn from these sentences that they must be related somehow, since they both appear together with the word “language.” The more sentences we observe, the more sure we can be about the distributional patterns (i.e.&nbsp;which words tend to have similar words nearby). Words that tend to have very similar words nearby are likely to be similar in meaning, while words that have very different contexts are probably unrelated. Algorithms that learn the meanings of tokens (or at least the relations between their meanings) from these patterns of co-occurrence are called <strong>Distributional Semantic Models (DSMs)</strong>.</p>
<div class="callout callout-style-simple callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
A Common Misconception
</div>
</div>
<div class="callout-body-container callout-body">
<p>Two words are NOT considered similar based on whether they appear together often. Words are similar when they tend to appear in similar <em>contexts</em>. For example, “fridge” and “refrigerator” almost never appear together in the same sentence, but they do tend to appear next to similar groupings of other words (e.g.&nbsp;“food,” “cold,” etc.). LSA, the first DSM we will cover, does not fully address this difficulty.</p>
</div>
</div>
<p>When DSMs learn how different meanings are related, they <em>embed</em> those meanings as vectors in a vector space, like this:</p>
<div class="cell">
<div class="cell-output-display">
<div class="plotly html-widget html-fill-item" id="htmlwidget-74cfae68c7f2192b61f0" style="width:100%;height:406px;"></div>
<script type="application/json" data-for="htmlwidget-74cfae68c7f2192b61f0">{"x":{"visdat":{"10b94313d6a1b":["function () ","plotlyVisDat"]},"cur_data":"10b94313d6a1b","attrs":{"10b94313d6a1b":{"x":{},"y":{},"z":{},"split":{},"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"markers","inherit":true},"10b94313d6a1b.1":{"x":{},"y":{},"z":{},"split":{},"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"text":{},"type":"scatter3d","mode":"text","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"xaxis":{"zerolinecolor":"black","zerolinewidth":10},"showlegend":false,"scene":{"xaxis":{"title":"V1"},"yaxis":{"title":"V2"},"zaxis":{"title":"V3"}},"hovermode":"closest"},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"x":[-0.36244642737218208],"y":[0.10336302253996849],"z":[0.25597161573536165],"type":"scatter3d","mode":"markers","name":"anxious","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"x":[-0.20048952208304843],"y":[-0.47953728220530528],"z":[0.17349679604121063],"type":"scatter3d","mode":"markers","name":"bipolar","marker":{"color":"rgba(255,127,14,1)","line":{"color":"rgba(255,127,14,1)"}},"error_y":{"color":"rgba(255,127,14,1)"},"error_x":{"color":"rgba(255,127,14,1)"},"line":{"color":"rgba(255,127,14,1)"},"frame":null},{"x":[-0.2941103607640555],"y":[-0.32239881317784785],"z":[-0.048332237519831373],"type":"scatter3d","mode":"markers","name":"depressed","marker":{"color":"rgba(44,160,44,1)","line":{"color":"rgba(44,160,44,1)"}},"error_y":{"color":"rgba(44,160,44,1)"},"error_x":{"color":"rgba(44,160,44,1)"},"line":{"color":"rgba(44,160,44,1)"},"frame":null},{"x":[-0.17512175552802359],"y":[-0.52372372216682617],"z":[0.00144812308620342],"type":"scatter3d","mode":"markers","name":"depression","marker":{"color":"rgba(214,39,40,1)","line":{"color":"rgba(214,39,40,1)"}},"error_y":{"color":"rgba(214,39,40,1)"},"error_x":{"color":"rgba(214,39,40,1)"},"line":{"color":"rgba(214,39,40,1)"},"frame":null},{"x":[-0.33556477252597477],"y":[0.28466715970390993],"z":[0.11705857260170072],"type":"scatter3d","mode":"markers","name":"ecstatic","marker":{"color":"rgba(148,103,189,1)","line":{"color":"rgba(148,103,189,1)"}},"error_y":{"color":"rgba(148,103,189,1)"},"error_x":{"color":"rgba(148,103,189,1)"},"line":{"color":"rgba(148,103,189,1)"},"frame":null},{"x":[-0.25800397430043442],"y":[0.17085988251017903],"z":[0.018085744349629361],"type":"scatter3d","mode":"markers","name":"furious","marker":{"color":"rgba(140,86,75,1)","line":{"color":"rgba(140,86,75,1)"}},"error_y":{"color":"rgba(140,86,75,1)"},"error_x":{"color":"rgba(140,86,75,1)"},"line":{"color":"rgba(140,86,75,1)"},"frame":null},{"x":[-0.35662778297930486],"y":[0.24864288333445073],"z":[-0.15556890791135303],"type":"scatter3d","mode":"markers","name":"happy","marker":{"color":"rgba(227,119,194,1)","line":{"color":"rgba(227,119,194,1)"}},"error_y":{"color":"rgba(227,119,194,1)"},"error_x":{"color":"rgba(227,119,194,1)"},"line":{"color":"rgba(227,119,194,1)"},"frame":null},{"x":[-0.23824421903029769],"y":[-0.33801464236487944],"z":[0.098017196040503476],"type":"scatter3d","mode":"markers","name":"manic","marker":{"color":"rgba(127,127,127,1)","line":{"color":"rgba(127,127,127,1)"}},"error_y":{"color":"rgba(127,127,127,1)"},"error_x":{"color":"rgba(127,127,127,1)"},"line":{"color":"rgba(127,127,127,1)"},"frame":null},{"x":[-0.25833691215071303],"y":[-0.082676817325885887],"z":[-0.61130043718229221],"type":"scatter3d","mode":"markers","name":"miserable","marker":{"color":"rgba(188,189,34,1)","line":{"color":"rgba(188,189,34,1)"}},"error_y":{"color":"rgba(188,189,34,1)"},"error_x":{"color":"rgba(188,189,34,1)"},"line":{"color":"rgba(188,189,34,1)"},"frame":null},{"x":[-0.31811862929474299],"y":[0.029525786135739088],"z":[0.40714832020454939],"type":"scatter3d","mode":"markers","name":"nervous","marker":{"color":"rgba(23,190,207,1)","line":{"color":"rgba(23,190,207,1)"}},"error_y":{"color":"rgba(23,190,207,1)"},"error_x":{"color":"rgba(23,190,207,1)"},"line":{"color":"rgba(23,190,207,1)"},"frame":null},{"x":[-0.29495531382524232],"y":[0.075698027439194446],"z":[-0.52727865357457815],"type":"scatter3d","mode":"markers","name":"sad","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"x":[-0.30531703982103131],"y":[0.28528837334936841],"z":[0.19162378349372311],"type":"scatter3d","mode":"markers","name":"surprised","marker":{"color":"rgba(255,127,14,1)","line":{"color":"rgba(255,127,14,1)"}},"error_y":{"color":"rgba(255,127,14,1)"},"error_x":{"color":"rgba(255,127,14,1)"},"line":{"color":"rgba(255,127,14,1)"},"frame":null},{"x":[-0.36244642737218208],"y":[0.10336302253996849],"z":[0.25597161573536165],"text":"anxious","type":"scatter3d","mode":"text","name":"anxious","marker":{"color":"rgba(44,160,44,1)","line":{"color":"rgba(44,160,44,1)"}},"error_y":{"color":"rgba(44,160,44,1)"},"error_x":{"color":"rgba(44,160,44,1)"},"line":{"color":"rgba(44,160,44,1)"},"frame":null},{"x":[-0.20048952208304843],"y":[-0.47953728220530528],"z":[0.17349679604121063],"text":"bipolar","type":"scatter3d","mode":"text","name":"bipolar","marker":{"color":"rgba(214,39,40,1)","line":{"color":"rgba(214,39,40,1)"}},"error_y":{"color":"rgba(214,39,40,1)"},"error_x":{"color":"rgba(214,39,40,1)"},"line":{"color":"rgba(214,39,40,1)"},"frame":null},{"x":[-0.2941103607640555],"y":[-0.32239881317784785],"z":[-0.048332237519831373],"text":"depressed","type":"scatter3d","mode":"text","name":"depressed","marker":{"color":"rgba(148,103,189,1)","line":{"color":"rgba(148,103,189,1)"}},"error_y":{"color":"rgba(148,103,189,1)"},"error_x":{"color":"rgba(148,103,189,1)"},"line":{"color":"rgba(148,103,189,1)"},"frame":null},{"x":[-0.17512175552802359],"y":[-0.52372372216682617],"z":[0.00144812308620342],"text":"depression","type":"scatter3d","mode":"text","name":"depression","marker":{"color":"rgba(140,86,75,1)","line":{"color":"rgba(140,86,75,1)"}},"error_y":{"color":"rgba(140,86,75,1)"},"error_x":{"color":"rgba(140,86,75,1)"},"line":{"color":"rgba(140,86,75,1)"},"frame":null},{"x":[-0.33556477252597477],"y":[0.28466715970390993],"z":[0.11705857260170072],"text":"ecstatic","type":"scatter3d","mode":"text","name":"ecstatic","marker":{"color":"rgba(227,119,194,1)","line":{"color":"rgba(227,119,194,1)"}},"error_y":{"color":"rgba(227,119,194,1)"},"error_x":{"color":"rgba(227,119,194,1)"},"line":{"color":"rgba(227,119,194,1)"},"frame":null},{"x":[-0.25800397430043442],"y":[0.17085988251017903],"z":[0.018085744349629361],"text":"furious","type":"scatter3d","mode":"text","name":"furious","marker":{"color":"rgba(127,127,127,1)","line":{"color":"rgba(127,127,127,1)"}},"error_y":{"color":"rgba(127,127,127,1)"},"error_x":{"color":"rgba(127,127,127,1)"},"line":{"color":"rgba(127,127,127,1)"},"frame":null},{"x":[-0.35662778297930486],"y":[0.24864288333445073],"z":[-0.15556890791135303],"text":"happy","type":"scatter3d","mode":"text","name":"happy","marker":{"color":"rgba(188,189,34,1)","line":{"color":"rgba(188,189,34,1)"}},"error_y":{"color":"rgba(188,189,34,1)"},"error_x":{"color":"rgba(188,189,34,1)"},"line":{"color":"rgba(188,189,34,1)"},"frame":null},{"x":[-0.23824421903029769],"y":[-0.33801464236487944],"z":[0.098017196040503476],"text":"manic","type":"scatter3d","mode":"text","name":"manic","marker":{"color":"rgba(23,190,207,1)","line":{"color":"rgba(23,190,207,1)"}},"error_y":{"color":"rgba(23,190,207,1)"},"error_x":{"color":"rgba(23,190,207,1)"},"line":{"color":"rgba(23,190,207,1)"},"frame":null},{"x":[-0.25833691215071303],"y":[-0.082676817325885887],"z":[-0.61130043718229221],"text":"miserable","type":"scatter3d","mode":"text","name":"miserable","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"x":[-0.31811862929474299],"y":[0.029525786135739088],"z":[0.40714832020454939],"text":"nervous","type":"scatter3d","mode":"text","name":"nervous","marker":{"color":"rgba(255,127,14,1)","line":{"color":"rgba(255,127,14,1)"}},"error_y":{"color":"rgba(255,127,14,1)"},"error_x":{"color":"rgba(255,127,14,1)"},"line":{"color":"rgba(255,127,14,1)"},"frame":null},{"x":[-0.29495531382524232],"y":[0.075698027439194446],"z":[-0.52727865357457815],"text":"sad","type":"scatter3d","mode":"text","name":"sad","marker":{"color":"rgba(44,160,44,1)","line":{"color":"rgba(44,160,44,1)"}},"error_y":{"color":"rgba(44,160,44,1)"},"error_x":{"color":"rgba(44,160,44,1)"},"line":{"color":"rgba(44,160,44,1)"},"frame":null},{"x":[-0.30531703982103131],"y":[0.28528837334936841],"z":[0.19162378349372311],"text":"surprised","type":"scatter3d","mode":"text","name":"surprised","marker":{"color":"rgba(214,39,40,1)","line":{"color":"rgba(214,39,40,1)"}},"error_y":{"color":"rgba(214,39,40,1)"},"error_x":{"color":"rgba(214,39,40,1)"},"line":{"color":"rgba(214,39,40,1)"},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
</div>
</div>
<p>Now that you are comfortable with the concept of a word vector space (<a href="vectorspace-intro.html" class="quarto-xref"><span>Chapter 17</span></a>), let’s look at how different DSMs embed words and documents into them.</p>
</section><section id="sec-lsa" class="level2" data-number="18.2"><h2 data-number="18.2" class="anchored" data-anchor-id="sec-lsa">
<span class="header-section-number">18.2</span> LSA</h2>
<p>Latent Semantic Analysis (LSA) is the simplest sort of DSM.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> You can think of it as the linear regression of the embeddings world. In its standard form, LSA is a simple dimensionality reduction technique—singular-value decomposition (SVD)—applied to the DFM. To illustrate what this means, let’s start with a DFM describing the 862 posts on Reddit’s r/relationship_advice that are featured on the cover of this book. For the sake of illustration, we’ll only consider two features of this DFM: the words “I” and “me”.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; Document-feature matrix of: 868 documents, 2 features (3.17% sparse) and 7 docvars.
#&gt;        features
#&gt; docs     i me
#&gt;   post1 38  7
#&gt;   post2 28 18
#&gt;   post3 17  7
#&gt;   post4  9  5
#&gt;   post5 17  2
#&gt;   post6 24 13
#&gt; [ reached max_ndoc ... 862 more documents ]</code></pre>
</div>
</div>
<p>Since we’re only considering two features, we can visualize this DFM in two dimensions:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="decontextualized-embeddings_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="768"></p>
</figure>
</div>
</div>
</div>
<p>The terms “I” and “me” are strongly correlated with each other. LSA will recognize this and summarize the I/me-ness of each document with a single number, a combination of the two variables. How does it find this single number?</p>
<p>It finds the line of best fit that goes through the origin (0,0)—the line along which the variables stretch out the most. The direction of that I/me summary line becomes the first dimension of the new embedding space. So each post’s score for this new dimension (i.e.&nbsp;the first number in its embedding vector) represents how far each point is along the direction of the summary line. In the following visualization, each point is projected down onto the summary line. You can see how this squishes the two dimensions of “I” and “me” so that each post can be measured simply by how far it is along the summary line:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="decontextualized-embeddings_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="768"></p>
</figure>
</div>
</div>
</div>
<p>In this simple example, we started with two dimensions (“I” and “me”) and reduced them to one. Our new one-dimensional document embeddings measure how far the document is along the line drawn by the LSA.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>If we wanted a second dimension for the embedding space (which would be silly in this example since we only have two dimensions overall), LSA would draw a second line orthogonal (i.e.&nbsp;perpendicular) to the first—the line of best fit to the spread not accounted for by the first line. In real applications, of course, we’ll want to use tens of thousands of features, not two. And the embedding vectors we make might be hundreds of dimensions, not one. Nevertheless, the concept remains the same: Lines through the origin are found that best explain the variance between the documents (note that these lines will all be orthogonal to each other). These lines become the new dimensions: The embedding of a document describes the projection of that document point onto each line.</p>
<p>As part of the process of producing an embedding for each document, LSA also produces an embedding for each <em>word</em>. In the simple example above, the new one-dimensional <em>word embeddings</em> of “I” and “me” measure how much influence each word has on the summary line from Step 2. In other words, the embedding of “me” is the rise of the line along the y axis, and the embedding of “I” is the run of the line along the x axis. This is equivalent to asking how much I/me-ness is in “I” or how much I/me-ness is in “me.” In a real application with higher dimensional embeddings, the concept remains the same: The LSA embedding of a word describes the weights of that word count on each successive line of best fit.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>Performing LSA on a training dataset is made easy with the <code><a href="https://rdrr.io/pkg/quanteda.textmodels/man/textmodel_lsa.html">textmodel_lsa()</a></code> function from the <a href="https://github.com/quanteda/quanteda.textmodels"><code>quanteda.textmodels</code></a> package. Let’s try it on the full DFM from the r/relationship_advice posts, with all 14,897 features instead of just “I” and “me”.</p>
<p>We’ll use <code>nd = 100</code> to reduce these 14,897 dimensions into an embedding space of just 100 dimensions. Why 100 dimensions? Choosing the right dimensionality for LSA can be tricky—too many dimensions make the vector space noisy, but too few dimensions can miss important nuances in meaning. Of course, the larger the training dataset, the more dimensions you can use without overfitting. Notice that the process of finding the line of best fit, and then the next best line orthogonal to the first (and then the next best line orthogonal to both, etc.) guarantees that the first dimensions of the LSA vectors will be the most important ones, and the later ones will be more likely to reflect random noise in the data. 100 or 150 dimensions are popular choices for sufficiently large training sets.</p>
<div class="cell">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/quanteda/quanteda.textmodels">quanteda.textmodels</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">ra_posts_lsa</span> <span class="op">&lt;-</span> <span class="va">ra_posts_dfm</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/quanteda.textmodels/man/textmodel_lsa.html">textmodel_lsa</a></span><span class="op">(</span>nd <span class="op">=</span> <span class="fl">100</span>, margin <span class="op">=</span> <span class="st">"both"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now that the model is set up, we can access the word embeddings with <code>ra_posts_lsa$features</code> and the document embeddings with <code>ra_posts_lsa$docs</code>. For example, the embedding of the word “surprised” would be <code>ra_posts_lsa$features["surprised",]</code> and the embedding of the first post in the dataset would be <code>ra_posts_lsa$docs["post1",]</code>. We could also look for the words closest in meaning to the word “surprised” by measuring their cosine similarity with the “surprised” embedding (see <a href="vectorspace-intro.html#sec-cosine-similarity" class="quarto-xref"><span>Section 17.1.2</span></a>).</p>
<div class="cell">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">surprise_embedding</span> <span class="op">&lt;-</span> <span class="va">ra_posts_lsa</span><span class="op">$</span><span class="va">features</span><span class="op">[</span><span class="st">"surprised"</span>,<span class="op">]</span></span>
<span></span>
<span><span class="co"># cosine similarity function</span></span>
<span><span class="va">cos_sim</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">dot</span> <span class="op">&lt;-</span> <span class="va">x</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">y</span></span>
<span>  <span class="va">normx</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">x</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">normy</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">y</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/vector.html">as.vector</a></span><span class="op">(</span> <span class="va">dot</span> <span class="op">/</span> <span class="op">(</span><span class="va">normx</span><span class="op">*</span><span class="va">normy</span><span class="op">)</span> <span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># measure cosine similarity of each vector to "surprised"</span></span>
<span><span class="va">surprise_words</span> <span class="op">&lt;-</span> <span class="va">ra_posts_lsa</span><span class="op">$</span><span class="va">features</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">as_tibble</span><span class="op">(</span>rownames <span class="op">=</span> <span class="cn">NA</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">rownames_to_column</span><span class="op">(</span><span class="st">"token"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">rowwise</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span></span>
<span>    surprise <span class="op">=</span> <span class="fu">cos_sim</span><span class="op">(</span><span class="fu">c_across</span><span class="op">(</span><span class="va">V1</span><span class="op">:</span><span class="va">V100</span><span class="op">)</span>, <span class="va">surprise_embedding</span><span class="op">)</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">ungroup</span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># find the ten closest words to "surprised"</span></span>
<span><span class="va">surprise_words</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">arrange</span><span class="op">(</span><span class="fu">desc</span><span class="op">(</span><span class="va">surprise</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">slice_head</span><span class="op">(</span>n <span class="op">=</span> <span class="fl">10</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">pull</span><span class="op">(</span><span class="va">token</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;  [1] "surprised" "peanut"    "besides"   "child's"   "woah"      "brick"    
#&gt;  [7] "cases"     "oil"       "insulin"   "trolling"</code></pre>
</div>
</div>
<p>Some of these are a bit strange—probably we would get better results with a larger dataset—but “woah” and “trolling” do sound pretty surprising. It seems likely that “peanut”, “cases”, “oil”, and “insulin” were learned from posts about surprising allergy incidents.</p>
<p>We can also apply the embeddings learned on the r/relationship_advice posts to the now-familiar Hippocorpus data, and use the new surprise scores to retest the hypothesis that true autobiographical stories include more surprise than imagined stories.</p>
<div class="cell">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># reformat hippocorpus_dfm to match ra_posts_dfm</span></span>
<span><span class="va">hippocorpus_dfm</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_dfm</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">dfm_match</span><span class="op">(</span><span class="fu">featnames</span><span class="op">(</span><span class="va">ra_posts_dfm</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># apply LSA model to Hippocorpus data</span></span>
<span><span class="va">hippocorpus_lsa</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">ra_posts_lsa</span>, <span class="va">hippocorpus_dfm</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># measure surprise in Hippocorpus </span></span>
<span><span class="co"># (similarity to the word "surprised")</span></span>
<span><span class="va">hippocorpus_surprise</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_lsa</span><span class="op">$</span><span class="va">docs</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">as_tibble</span><span class="op">(</span>rownames <span class="op">=</span> <span class="cn">NA</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">rownames_to_column</span><span class="op">(</span><span class="st">"doc_id"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">rowwise</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span></span>
<span>    surprise <span class="op">=</span> <span class="fu">cos_sim</span><span class="op">(</span><span class="fu">c_across</span><span class="op">(</span><span class="va">V1</span><span class="op">:</span><span class="va">V100</span><span class="op">)</span>, <span class="va">surprise_embedding</span><span class="op">)</span></span>
<span>    <span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">ungroup</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Since cosine similarity can only be between -1 and 1 (<a href="vectorspace-intro.html#sec-cosine-similarity" class="quarto-xref"><span>Section 17.1.2</span></a>), we will use beta regression<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>. This requires us to transform the cosine similarity to range between 0 and 1 before modeling.</p>
<div class="cell">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># transform cosine similarity to stay between 0 and 1</span></span>
<span><span class="va">hippocorpus_surprise</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_surprise</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>surprise <span class="op">=</span> <span class="va">surprise</span><span class="op">/</span><span class="fl">2</span> <span class="op">+</span> <span class="fl">1</span><span class="op">/</span><span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># rejoin docvars</span></span>
<span><span class="va">hippocorpus_surprise</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_surprise</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">bind_cols</span><span class="op">(</span><span class="fu">docvars</span><span class="op">(</span><span class="va">hippocorpus_corp</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># beta regression</span></span>
<span><span class="va">surprise_mod_lsa</span> <span class="op">&lt;-</span> <span class="fu">betareg</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/betareg/man/betareg.html">betareg</a></span><span class="op">(</span></span>
<span>  <span class="va">surprise</span> <span class="op">~</span> <span class="va">memType</span>, </span>
<span>  <span class="va">hippocorpus_surprise</span>, </span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">surprise_mod_lsa</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; 
#&gt; Call:
#&gt; betareg::betareg(formula = surprise ~ memType, data = hippocorpus_surprise)
#&gt; 
#&gt; Standardized weighted residuals 2:
#&gt;     Min      1Q  Median      3Q     Max 
#&gt; -4.0907 -0.6880 -0.0095  0.6722  3.5857 
#&gt; 
#&gt; Coefficients (mean model with logit link):
#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)    
#&gt; (Intercept)     0.078968   0.003483  22.673  &lt; 2e-16 ***
#&gt; memTyperecalled 0.023277   0.004917   4.734 2.20e-06 ***
#&gt; memTyperetold   0.030599   0.006125   4.996 5.85e-07 ***
#&gt; 
#&gt; Phi coefficients (precision model with identity link):
#&gt;       Estimate Std. Error z value Pr(&gt;|z|)    
#&gt; (phi)  118.824      2.021   58.79   &lt;2e-16 ***
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
#&gt; 
#&gt; Type of estimator: ML (maximum likelihood)
#&gt; Log-likelihood: 1.144e+04 on 4 Df
#&gt; Pseudo R-squared: 0.004906
#&gt; Number of iterations: 9 (BFGS) + 2 (Fisher scoring)</code></pre>
</div>
</div>
<p>We found a significant difference between recalled and imagined stories, such that recalled stories had more surprise (p &lt; .001) and a significant difference between retold and imagined stories such that retold stories had more surprise (p &lt; .001).</p>
<section id="sec-lsa-variations" class="level3" data-number="18.2.1"><h3 data-number="18.2.1" class="anchored" data-anchor-id="sec-lsa-variations">
<span class="header-section-number">18.2.1</span> Variations on LSA</h3>
<p>Even in the simplified example shown above using word counts for only “I” and “me”, it is easy to see some problems with the standard LSA procedure. First, there is no guarantee that the line of best fit for describing the relationships between word counts will go through the origin. How can we fix this?</p>
<p>Standard LSA is singular-value decomposition (SVD) applied to a DFM. If you are familiar with principle components analysis (PCA), the explanation of this process above may have sounded familiar. Indeed, PCA is almost the same as SVD, but with one added step at the beginning: centering all the variables at zero. This centering can make a big difference when the line of best fit to your data does not go through the origin. To center a DFM before performing LSA, you can use this function:</p>
<div class="cell">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">dfm_center</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">dfm</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/methods/new.html">new</a></span><span class="op">(</span><span class="st">"dfmSparse"</span>, <span class="fu"><a href="https://rdrr.io/r/methods/as.html">as</a></span><span class="op">(</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">dfm</span>, <span class="fl">1</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>, <span class="st">"dgCMatrix"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Another potential problem is that standard LSA gives more weight to common tokens, since common tokens tend to have more variance in their counts (remember that the line of best fit is the one along which the variables spread out the most). This can be remedied by normalizing the DFM before performing the LSA (i.e.&nbsp;transforming all of the counts to z-scores). To do this, you can use this function on your DFM:</p>
<div class="cell">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">dfm_scale</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">dfm</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/methods/new.html">new</a></span><span class="op">(</span><span class="st">"dfmSparse"</span>, <span class="fu"><a href="https://rdrr.io/r/methods/as.html">as</a></span><span class="op">(</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">dfm</span>, <span class="fl">1</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>, <span class="st">"dgCMatrix"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To gain an appreciation for these variations, let’s see what LSA looks like on our “I” and “me” features with centering and normalization:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="decontextualized-embeddings_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid figure-img" width="768"></p>
</figure>
</div>
</div>
</div>
<p>Other problems with LSA are familiar from <a href="word-counting-improvements.html" class="quarto-xref"><span>Chapter 16</span></a>: For example, LSA can only model linear relationships, but the relationships between word counts are not necessarily linear. In fact, the scatterplots above make it fairly clear that the relationship between the number of “I”s and the number of “we”s in a Reddit post is curved. Similarly, LSA (and SVD in general) works best with normally distributed data <span class="citation" data-cites="rosario_2001">(see <a href="#ref-rosario_2001" role="doc-biblioref">Rosario, 2001</a>)</span>, and word counts are anything but normally distributed. Also, standard LSA is sensitive to text length and may not generalize well to a dataset with texts that are much shorter or much longer than the training set. All of these problems can be remedied using the methods discussed in <a href="word-counting-improvements.html" class="quarto-xref"><span>Chapter 16</span></a>. For example, one might calculate TF-IDF scores (<a href="word-counting-improvements.html#sec-tfidf" class="quarto-xref"><span>Section 16.5</span></a>) before performing LSA to emphasize topical content. Alternatively, one might perform smoothing (<a href="word-counting-improvements.html#sec-smoothing" class="quarto-xref"><span>Section 16.6</span></a>) followed by relative tokenization (<a href="word-counting-improvements.html#sec-relative-tokenization" class="quarto-xref"><span>Section 16.3</span></a>) and the Anscombe transform (<a href="word-counting-improvements.html#sec-anscombe" class="quarto-xref"><span>Section 16.4</span></a>) to standardize word counts across text length and get them closer to a normal distribution. The original designers of LSA advocated for a transformation similar to TF-IDF which they justified in cognitive terms<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> <span class="citation" data-cites="landauer_dumais_1997">(<a href="#ref-landauer_dumais_1997" role="doc-biblioref">Landauer &amp; Dumais, 1997</a>)</span>.</p>
<p>Another difficulty with LSA is that it relies on documents to define the context of words. This works well if each document only deals with one topic (or emotion), but not so well with documents that include multiple topics. One solution to this (if you have reasonably long texts) is to use a moving <em>context window</em>: Extract all segments of, say, 10 words, and use each one as a separate document for training the LSA. This can be accomplished in R by applying the following code to your texts before tokenization:</p>
<div class="cell">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">example_texts</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span></span>
<span>  <span class="st">"One solution to this is to use a moving context window"</span>,</span>
<span>  <span class="st">"extracting all segments of, say, 10 words, and using each one as a separate document for training the LSA."</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="co"># function to split text with moving window</span></span>
<span><span class="va">str_split_window</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">string</span>, <span class="va">window_size</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">nwords</span> <span class="op">&lt;-</span> <span class="fu">str_count</span><span class="op">(</span><span class="va">string</span>, <span class="st">" "</span><span class="op">)</span> <span class="op">+</span> <span class="fl">1L</span></span>
<span>  <span class="va">out</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">lapply</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">string</span><span class="op">)</span>, <span class="kw">function</span><span class="op">(</span><span class="va">s</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="op">(</span><span class="va">window_size</span> <span class="op">+</span> <span class="fl">1L</span><span class="op">)</span><span class="op">:</span><span class="va">nwords</span><span class="op">[</span><span class="va">s</span><span class="op">]</span>, <span class="kw">function</span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="fu">word</span><span class="op">(</span><span class="va">string</span><span class="op">[</span><span class="va">s</span><span class="op">]</span>, <span class="va">i</span><span class="op">-</span><span class="va">window_size</span>, <span class="va">i</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="op">}</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/unlist.html">unlist</a></span><span class="op">(</span><span class="va">out</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="fu">str_split_window</span><span class="op">(</span><span class="va">example_texts</span>, <span class="fl">10</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt;  [1] "One solution to this is to use a moving context window"       
#&gt;  [2] "extracting all segments of, say, 10 words, and using each one"
#&gt;  [3] "all segments of, say, 10 words, and using each one as"        
#&gt;  [4] "segments of, say, 10 words, and using each one as a"          
#&gt;  [5] "of, say, 10 words, and using each one as a separate"          
#&gt;  [6] "say, 10 words, and using each one as a separate document"     
#&gt;  [7] "10 words, and using each one as a separate document for"      
#&gt;  [8] "words, and using each one as a separate document for training"
#&gt;  [9] "and using each one as a separate document for training the"   
#&gt; [10] "using each one as a separate document for training the LSA."</code></pre>
</div>
</div>
<p><strong>An example of LSA in research:</strong> <span class="citation" data-cites="moss_etal_2006">Moss et al. (<a href="#ref-moss_etal_2006" role="doc-biblioref">2006</a>)</span> asked mechanical engineering students to write brief descriptions of devices that were presented in diagrams. They then performed LSA on these descriptions, reducing them to a 100 dimensional embedding space. They then found the embeddings of an existing dictionary of function-related words (e.g.&nbsp;“actuate”, “adjust”, “control”), and averaged them to produce a vector representing the function of devices. Finally, they computed cosine similarity between this vector and that of each document. They found that fourth-year engineering students used more functional language than first-year students.</p>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advantages of LSA
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>
<strong>Context-Based Model:</strong> LSA captures the correlations between tokens in texts. This is an improvement on simple word counting methods that can miss subtle patterns in language use.</li>
<li>
<strong>Simplicity:</strong> Since many psychology researchers are familiar with PCA, LSA may feel like less of a black box than more modern methods.</li>
<li>
<strong>Easy Integration With Transformations:</strong> Since LSA is so straightforward, it is easy to integrate with methods from <a href="word-counting-improvements.html" class="quarto-xref"><span>Chapter 16</span></a>.</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Disadvantages of LSA
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Assumes Linearity</strong></li>
<li><strong>Works Best With Normal Distributions</strong></li>
<li>
<strong>Relies on Documents as Context:</strong> LSA works best when documents have only one topic each.</li>
<li>
<strong>Prioritizes Common Words:</strong> This can be fixed by adding a normalization step.</li>
</ul>
</div>
</div>
</section></section><section id="sec-word-embeddings" class="level2" data-number="18.3"><h2 data-number="18.3" class="anchored" data-anchor-id="sec-word-embeddings">
<span class="header-section-number">18.3</span> Advanced Word Embeddings</h2>
<p>LSA is a good baseline for word embeddings, but as we have seen, it suffers from many of the familiar problems associated with word counts: difficulties with nonlinear relationships, non-normal distributions, etc.</p>
<p>LSA also suffers from an even more fundamental problem. Recall the warning from the beginning of this chapter: Two words are NOT considered similar based on whether they appear together often. Words are similar when they tend to appear in similar <em>contexts</em>. LSA is fundamentally based on global patterns of covariance in the DFM. Because synonyms rarely appear together in the same document (i.e.&nbsp;their counts are likely to be negatively correlated), their embeddings will be further apart in the vector space than they really should be. More modern techniques for embedding words fix this problem as well as the others with model architectures that are carefully tailored for capturing meaning.</p>
<section id="sec-word2vec" class="level3" data-number="18.3.1"><h3 data-number="18.3.1" class="anchored" data-anchor-id="sec-word2vec">
<span class="header-section-number">18.3.1</span> Word2vec</h3>
<p>Word2vec was first introduced by <span class="citation" data-cites="mikolov_etal_2013b">Mikolov, Chen, et al. (<a href="#ref-mikolov_etal_2013b" role="doc-biblioref">2013</a>)</span> and was refined by <span class="citation" data-cites="mikolov_etal_2013c">Mikolov, Sutskever, et al. (<a href="#ref-mikolov_etal_2013c" role="doc-biblioref">2013</a>)</span>. They proposed a few variations on a simple neural network<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> that learns the relationships between words and contexts. Here we describe the most commonly used variation—continuous Skip-gram with negative sampling.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> Imagine training the model on the following sentence:</p>
<blockquote class="blockquote">
<p>Coding can be frustrating.</p>
</blockquote>
<p>Our Skip-gram training dataset would have one column for the input word, and another column for words from its immediate context. It is called “continuous” because it slides a context window along the training text (<a href="#sec-lsa-variations" class="quarto-xref"><span>Section 18.2.1</span></a>), considering each word as input, and the words immediately around it (e.g.&nbsp;10 before and 10 after) as context, like this:</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; # A tibble: 12 × 2
#&gt;    word        context    
#&gt;    &lt;chr&gt;       &lt;chr&gt;      
#&gt;  1 coding      can        
#&gt;  2 coding      be         
#&gt;  3 coding      frustrating
#&gt;  4 can         coding     
#&gt;  5 can         be         
#&gt;  6 can         frustrating
#&gt;  7 be          coding     
#&gt;  8 be          can        
#&gt;  9 be          frustrating
#&gt; 10 frustrating coding     
#&gt; 11 frustrating can        
#&gt; 12 frustrating be</code></pre>
</div>
</div>
<p>The negative sampling method adds more rows to the training set, this time from words and contexts that do not go together, drawn at random from other parts of the corpus. A third column indicates whether the pair of words are really neighbors or not:</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; # A tibble: 12 × 3
#&gt;    word   context     neighbors
#&gt;    &lt;chr&gt;  &lt;chr&gt;           &lt;dbl&gt;
#&gt;  1 coding can                 1
#&gt;  2 coding be                  1
#&gt;  3 coding frustrating         1
#&gt;  4 can    coding              1
#&gt;  5 can    be                  1
#&gt;  6 can    frustrating         1
#&gt;  7 coding happy               0
#&gt;  8 coding olive               0
#&gt;  9 coding jump                0
#&gt; 10 can    happy               0
#&gt; 11 can    olive               0
#&gt; 12 can    jump                0</code></pre>
</div>
</div>
<p>The word2vec model takes the first two columns as input and tries to predict whether the two words are neighbors or not. It does this by learning two separate sets of embeddings: word embeddings and context embeddings.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="decontextualized-embeddings_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid figure-img" width="768"></p>
</figure>
</div>
</div>
</div>
<p>For each row of the training set, the model looks up the embedding for the target word and the embedding for the context word, and computes the dot product between the two vectors. The dot product is closely related to the cosine similarity, which we discussed in <a href="vectorspace-intro.html#sec-cosine-similarity" class="quarto-xref"><span>Section 17.1.2</span></a>—it measures how similar the two embeddings are. If the dot product is large (i.e.&nbsp;the word embedding and the context embedding are very similar), the model predicts that the two words are likely to be real neighbors. If the dot product is small, the model predicts that the two words were probably sampled at random.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> During training, the model learns which word embeddings and context embeddings will do best at this binary prediction task.</p>
<p>Notice that word2vec (and fastText and GloVe) give each word two embeddings: one for when the word is the target and another for when it is the context <span class="citation" data-cites="goldberg_levy_2014">(<a href="#ref-goldberg_levy_2014" role="doc-biblioref">Goldberg &amp; Levy, 2014</a>)</span>. This may seem strange, but it actually solves two important problems with LSA:</p>
<ol type="1">
<li>
<strong>A Nuance of the Distributional Hypothesis.</strong> Recall the case of “fridge” and “refrigerator”, which almost never appear together in the same sentence, but do tend to appear next to similar groupings of other words. Because LSA is based directly on broad patterns of covariance in word frequencies, it will pick up on the fact that “fridge” and “refrigerator” are negatively correlated and push them further apart than they should be. Word2vec, on the other hand, can learn a <em>context embedding</em> for “refrigerator” that is not so close to the <em>word embedding</em> for “fridge”, even when the word embeddings of the two words are very close. This allows word2vec to recognize that “refrigerator” and “fridge” tend to appear in similar contexts, but are unlikely to appear together. In this way, word2vec is truer to the distributional hypothesis than LSA.</li>
<li>
<strong>Associative Asymmetry.</strong> The cosine similarity between two word embeddings gives the best estimate of <em>conceptual similarity</em> <span class="citation" data-cites="torabi-asr_etal_2018">(<a href="#ref-torabi-asr_etal_2018" role="doc-biblioref">Torabi Asr et al., 2018</a>)</span>. This is because conceptual similarity is not the same as association in language (or in the mind). In fact, psycholinguists have long known that human associations between two words are asymmetric. For example, people prompted with “leopard” are much more likely to think of “tiger” than people prompted with “tiger” are to think of “leopard” <span class="citation" data-cites="tversky_gati_1982">(<a href="#ref-tversky_gati_1982" role="doc-biblioref">Tversky &amp; Gati, 1982</a>)</span>. These sorts of associative connections are closely tied to probabilities of co-occurrence in language and are therefore much better represented by the cosine similarity (or even the dot product) between a word embedding and a context embedding <span class="citation" data-cites="torabi-asr_etal_2018">(<a href="#ref-torabi-asr_etal_2018" role="doc-biblioref">Torabi Asr et al., 2018</a>)</span>. Thus the association between “leopard” and “tiger” would be represented by the similarity between the <em>word embedding</em> of “leopard” and the <em>context embedding</em> of “tiger”, allowing for the asymmetry observed in mental associations.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> Since LSA only produces one embedding per word, it cannot capture this asymmetry.</li>
</ol>
<p>Word2vec was revolutionary when it came out. The main reason for this is the efficiency of the training process. This efficiency means that the model can be trained on massive datasets. Larger and more diverse datasets mean more reliable embeddings. A few pretrained models can be easily downloaded from the Internet (e.g.&nbsp;from <a href="https://github.com/maxoodf/word2vec?tab=readme-ov-file#basic-usage">here</a> or <a href="https://www.kaggle.com/datasets/pkugoodspeed/nlpword2vecembeddingspretrained">here</a>). Because these models are trained on very large datasets and are already known to perform well, it almost never makes sense to train your own word2vec from scratch.</p>
<p>Once you’ve downloaded a pretrained model (generally as a .bin file), you can open it in R with the <a href="https://cran.r-project.org/web/packages/word2vec/readme/README.html"><code>word2vec</code> package</a>. Here we’ll be using a model trained on the entirety of Google news, downloaded from <a href="https://www.kaggle.com/datasets/pkugoodspeed/nlpword2vecembeddingspretrained/download?datasetVersionNumber=1">here</a>, which uses 300-dimensional embeddings.</p>
<div class="cell">
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/bnosac/word2vec">word2vec</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># model file path</span></span>
<span><span class="va">word2vec_mod</span> <span class="op">&lt;-</span> <span class="st">"data/GoogleNews-vectors-negative300.bin"</span></span>
<span></span>
<span><span class="co"># open model</span></span>
<span><span class="va">word2vec_mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/word2vec/man/read.word2vec.html">read.word2vec</a></span><span class="op">(</span>file <span class="op">=</span> <span class="va">word2vec_mod</span>, normalize <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To find embeddings of specific words, use <code>predict(word2vec_mod, c("word1", "word2"), type = "embedding")</code>. To get embeddings for full documents, average the embeddings of the words in the document. Here we provide a function to compute document embeddings directly from a DFM.</p>
<div class="cell">
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">textstat_embedding</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">dfm</span>, <span class="va">model</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">feats</span> <span class="op">&lt;-</span> <span class="fu">featnames</span><span class="op">(</span><span class="va">dfm</span><span class="op">)</span></span>
<span>  <span class="co"># find word embeddings</span></span>
<span>  <span class="va">feat_embeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">model</span>, <span class="va">feats</span>, type <span class="op">=</span> <span class="st">"embedding"</span><span class="op">)</span></span>
<span>  <span class="va">feat_embeddings</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">feat_embeddings</span><span class="op">)</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span>  <span class="co"># average word embeddings of each document</span></span>
<span>  <span class="va">out_mat</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">dfm</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">feat_embeddings</span><span class="op">)</span><span class="op">/</span><span class="fu">ntoken</span><span class="op">(</span><span class="va">dfm</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">out_mat</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"V"</span>, <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">ncol</a></span><span class="op">(</span><span class="va">out_mat</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="fu">as_tibble</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="va">out_mat</span><span class="op">)</span>, rownames <span class="op">=</span> <span class="st">"doc_id"</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s use word2vec embeddings and cosine similarity to reanalyze the Hippocorpus data.</p>
<div class="cell">
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># embedding of the word "surprised"</span></span>
<span><span class="va">surprise_embedding</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">word2vec_mod</span>, <span class="st">"surprised"</span>, type <span class="op">=</span> <span class="st">"embedding"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/vector.html">as.vector</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># document embeddings</span></span>
<span><span class="va">hippocorpus_word2vec</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_dfm</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">textstat_embedding</span><span class="op">(</span><span class="va">word2vec_mod</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># score documents by surprise</span></span>
<span><span class="va">hippocorpus_surprise_word2vec</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_word2vec</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">rowwise</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span></span>
<span>    surprise <span class="op">=</span> <span class="fu">cos_sim</span><span class="op">(</span><span class="fu">c_across</span><span class="op">(</span><span class="va">V1</span><span class="op">:</span><span class="va">V300</span><span class="op">)</span>, <span class="va">surprise_embedding</span><span class="op">)</span>,</span>
<span>    <span class="co"># transform cosine similarity to stay between 0 and 1</span></span>
<span>    surprise <span class="op">=</span> <span class="va">surprise</span><span class="op">/</span><span class="fl">2</span> <span class="op">+</span> <span class="fl">1</span><span class="op">/</span><span class="fl">2</span></span>
<span>    <span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">ungroup</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">V1</span><span class="op">:</span><span class="va">V300</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># rejoin docvars</span></span>
<span><span class="va">hippocorpus_surprise_word2vec</span> <span class="op">&lt;-</span> <span class="va">hippocorpus_surprise_word2vec</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">bind_cols</span><span class="op">(</span><span class="fu">docvars</span><span class="op">(</span><span class="va">hippocorpus_corp</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># beta regression</span></span>
<span><span class="va">surprise_mod_word2vec</span> <span class="op">&lt;-</span> <span class="fu">betareg</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/betareg/man/betareg.html">betareg</a></span><span class="op">(</span></span>
<span>  <span class="va">surprise</span> <span class="op">~</span> <span class="va">memType</span>, </span>
<span>  <span class="va">hippocorpus_surprise_word2vec</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">surprise_mod_word2vec</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>#&gt; 
#&gt; Call:
#&gt; betareg::betareg(formula = surprise ~ memType, data = hippocorpus_surprise_word2vec)
#&gt; 
#&gt; Standardized weighted residuals 2:
#&gt;     Min      1Q  Median      3Q     Max 
#&gt; -5.5628 -0.6548  0.0385  0.6634  3.9802 
#&gt; 
#&gt; Coefficients (mean model with logit link):
#&gt;                   Estimate Std. Error z value Pr(&gt;|z|)    
#&gt; (Intercept)      0.6532469  0.0009524 685.890  &lt; 2e-16 ***
#&gt; memTyperecalled -0.0101895  0.0013430  -7.587 3.27e-14 ***
#&gt; memTyperetold   -0.0077017  0.0016726  -4.605 4.13e-06 ***
#&gt; 
#&gt; Phi coefficients (precision model with identity link):
#&gt;       Estimate Std. Error z value Pr(&gt;|z|)    
#&gt; (phi)  1775.78      30.33   58.56   &lt;2e-16 ***
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
#&gt; 
#&gt; Type of estimator: ML (maximum likelihood)
#&gt; Log-likelihood: 2.102e+04 on 4 Df
#&gt; Pseudo R-squared: 0.00875
#&gt; Number of iterations: 21 (BFGS) + 3 (Fisher scoring)</code></pre>
</div>
</div>
<p>Once again we found a significant difference between recalled and imagined stories, this time in the opposite direction (though see <a href="navigating-vectorspace.html" class="quarto-xref"><span>Chapter 20</span></a> for some ways in which this may be misleading).</p>
<p><strong>An example of word2vec in research:</strong> <span class="citation" data-cites="chatterjee_etal_2023">Chatterjee et al. (<a href="#ref-chatterjee_etal_2023" role="doc-biblioref">2023</a>)</span> used word2vec to study the phenomenon of nominative determinism—the purported tendency to chose a profession or city with a first letter that matches the first letter of one’s name (e.g.&nbsp;someone named Louis might choose to be a language researcher). They first used a word2vec model trained on Google News to obtain embeddings for 3,410 first names, 508 professions, and 14,856 US cities. They then averaged the embeddings of all names/professions/cities that begin with the same letter to obtain a vector representing names that begin with the letter “A”, a vector representing professions that begin with the letter “A”, etc. Using cosine similarity, they found that same-letter names and professions (e.g.&nbsp;Albert and Actuary) tend to be more similar than different-letter names and professions (e.g.&nbsp;Albert and Dentist), even when controlling for gender, ethnicity, and frequency. They found a similar pattern for names and cities.</p>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advantages of Word2vec
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>
<strong>Accurately Represents Meaning:</strong> By distinguishing between target and context words, word2vec stays true to the distributional hypothesis. Since it is not based on counts, it also avoids problems with non-linear relationships.</li>
<li>
<strong>Efficient for Large Datasets:</strong> This means that models can be trained on enormous amounts of text. Some such models are available for download on the Internet.</li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Disadvantages of Word2vec
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>
<strong>Relies on Word-Level Meaning:</strong> Word2vec assumes that each word has only one meaning. This means that it has trouble with words that can mean more than one thing (e.g.&nbsp;deep learning <em>model</em> vs.&nbsp;fashion <em>model</em>). Word2vec will learn the average of these meanings.</li>
<li>
<strong>Works Best in English:</strong> English words are generally spelled the same no matter where they are in a sentence. Word2vec doesn’t work as well for languages that have more prefixes, suffixes, conjugations, etc., since it has to relearn the meaning for each form of the word.</li>
<li><strong>Not Many Pretrained Models Available</strong></li>
</ul>
</div>
</div>
</section><section id="sec-glove" class="level3" data-number="18.3.2"><h3 data-number="18.3.2" class="anchored" data-anchor-id="sec-glove">
<span class="header-section-number">18.3.2</span> GloVe</h3>
<p>Word2vec produces spectacularly rich and reliable vector embeddings, but their reliance on randomly sampled pairs of words and contexts makes them somewhat noisy and overly sensitive to frequent tokens. The developers of word2vec managed to fix these problems by strategically filtering the training dataset, but <span class="citation" data-cites="pennington_etal_2014">Pennington et al. (<a href="#ref-pennington_etal_2014" role="doc-biblioref">2014</a>)</span> came up with a more elegant solution: Global Vectors (GloVe) is designed on the same principles of word2vec, but it is computed from global patterns of co-occurrence rather than individual examples.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<p>Even though GloVe uses a different method of training, the embeddings it generates are very similar to those generated by word2vec. Because GloVe embeddings are so similar to word2vec embeddings, we will not go into detail here about the way the GloVe algorithm works. Nevertheless, GloVe does have one very important advantage over word2vec: Better pretrained models are available online. Whereas the most easily available word2vec model is trained on news, the <a href="https://nlp.stanford.edu/projects/glove/">GloVe website</a> offers models trained on social media (<code>glove.twitter.27B.zip</code>) and on large portions of the Internet (Common Crawl). These models generalize better to social media texts (since they were trained on similar texts) and are likely to have richer representations of emotional or social content, since more examples of that content appear on social media than in the news or on Wikipedia.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a></p>
<p>Since the pretrained GloVe models are available in .txt format, you don’t need a wrapper package to use them in R. Simply download the pretrained model, input the path to the file as <code>path_to_glove</code>, and run the following code:</p>
<div class="cell">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">path_to_glove</span> <span class="op">&lt;-</span> <span class="st">"data/glove/glove.twitter.27B.100d.txt"</span></span>
<span><span class="va">dimensions</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span><span class="op">(</span><span class="fu">str_extract</span><span class="op">(</span><span class="va">path_to_glove</span>, <span class="st">"[:digit:]+(?=d\\.txt)"</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># matrix with token embeddings</span></span>
<span><span class="va">glove_pretrained</span> <span class="op">&lt;-</span> <span class="fu">data.table</span><span class="fu">::</span><span class="fu"><a href="https://rdatatable.gitlab.io/data.table/reference/fread.html">fread</a></span><span class="op">(</span></span>
<span>  <span class="va">path_to_glove</span>, </span>
<span>  quote <span class="op">=</span> <span class="st">""</span>,</span>
<span>  col.names <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"token"</span>, <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"dim_"</span>, <span class="fl">1</span><span class="op">:</span><span class="va">dimensions</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">distinct</span><span class="op">(</span><span class="va">token</span>, .keep_all <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">remove_rownames</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">column_to_rownames</span><span class="op">(</span><span class="st">"token"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># update class to "embeddings" (required for `predict.embeddings` function)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/class.html">class</a></span><span class="op">(</span><span class="va">glove_pretrained</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="st">"embeddings"</span></span>
<span></span>
<span><span class="co"># function to retrieve embeddings</span></span>
<span><span class="co">#   `object`: an "embeddings" object (matrix with character rownames)</span></span>
<span><span class="co">#   `newdata`: a character vector of tokens</span></span>
<span><span class="co">#   `type`: 'embedding' gives the embeddings of newdata. </span></span>
<span><span class="co">#           'nearest' gives nearest embeddings by cosine similarity </span></span>
<span><span class="co">#           (requires the cos_sim function)</span></span>
<span><span class="co">#   `top_n`: for `type = 'nearest'`, how many nearest neighbors to output?</span></span>
<span><span class="va">predict.embeddings</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">object</span>, <span class="va">newdata</span>, </span>
<span>                               <span class="va">type</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"embedding"</span>, <span class="st">"nearest"</span><span class="op">)</span>, </span>
<span>                               <span class="va">top_n</span> <span class="op">=</span> <span class="fl">10L</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">embeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span><span class="op">(</span><span class="va">object</span><span class="op">)</span></span>
<span>  <span class="va">embeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span></span>
<span>    <span class="va">embeddings</span>, </span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span>ncol <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">ncol</a></span><span class="op">(</span><span class="va">embeddings</span><span class="op">)</span>, dimnames <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="st">"NOT_IN_DICT"</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="va">newdata</span><span class="op">[</span><span class="op">!</span><span class="op">(</span><span class="va">newdata</span> <span class="op"><a href="https://rdrr.io/r/base/match.html">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/colnames.html">rownames</a></span><span class="op">(</span><span class="va">embeddings</span><span class="op">)</span><span class="op">)</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="st">"NOT_IN_DICT"</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="va">type</span> <span class="op">==</span> <span class="st">"embedding"</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">embeddings</span><span class="op">[</span><span class="va">newdata</span>,<span class="op">]</span></span>
<span>  <span class="op">}</span><span class="kw">else</span><span class="op">{</span></span>
<span>    <span class="kw">if</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">newdata</span><span class="op">)</span> <span class="op">&gt;</span> <span class="fl">1</span><span class="op">)</span><span class="op">{</span></span>
<span>      <span class="va">target</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/vector.html">as.vector</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">embeddings</span><span class="op">[</span><span class="va">newdata</span>,<span class="op">]</span>, <span class="fl">2</span>, <span class="va">mean</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="op">}</span><span class="kw">else</span><span class="op">{</span></span>
<span>      <span class="va">target</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/vector.html">as.vector</a></span><span class="op">(</span><span class="va">embeddings</span><span class="op">[</span><span class="va">newdata</span>,<span class="op">]</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>    <span class="va">sims</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">object</span>, <span class="fl">1</span>, <span class="va">cos_sim</span>, <span class="va">target</span><span class="op">)</span></span>
<span>    <span class="va">embeddings</span> <span class="op">&lt;-</span> <span class="va">embeddings</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/rev.html">rev</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/order.html">order</a></span><span class="op">(</span><span class="va">sims</span><span class="op">)</span><span class="op">)</span>,<span class="op">]</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">embeddings</span>, <span class="va">top_n</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>You can then proceed just as we did for word2vec, using the <code>textstat_embedding()</code> function provided in that section to compute document embeddings directly from a DFM.</p>
<section id="sec-glove-training" class="level4" data-number="18.3.2.1"><h4 data-number="18.3.2.1" class="anchored" data-anchor-id="sec-glove-training">
<span class="header-section-number">18.3.2.1</span> Training a Custom GloVe Model</h4>
<p>Since excellent pretrained GloVe embeddings are available online, it rarely makes sense to train your own model. Nevertheless, GloVe’s elegant training procedure makes for easy integration with Quanteda. A tutorial on training a custom GloVe model in Quanteda can be found <a href="https://quanteda.io/articles/pkgdown/replication/text2vec.html">here</a>.</p>
<p>Why might you want to train a custom word embeddings model? Maybe you are interested in quantifying differences in individual word use between multiple large groups of text. For example, you might train a GloVe model on texts written by conservatives and another on texts written by liberals, and demonstrate that the word “skirt” is closer to the word “woman” in conservative language than it is in liberal language.</p>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advantages of GloVe
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Elegant Training Procedure</strong></li>
<li><strong>Psychologically Sensitive Pretrained Models</strong></li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Disadvantages of GloVe
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Requires Large Training Sets</strong></li>
<li><strong>Relies on Word-Level Meaning</strong></li>
<li><strong>Works Best in English</strong></li>
</ul>
</div>
</div>
</section></section><section id="sec-fasttext" class="level3" data-number="18.3.3"><h3 data-number="18.3.3" class="anchored" data-anchor-id="sec-fasttext">
<span class="header-section-number">18.3.3</span> FastText</h3>
<p>FastText <span class="citation" data-cites="bojanowski_etal_2017">(<a href="#ref-bojanowski_etal_2017" role="doc-biblioref">Bojanowski et al., 2017</a>)</span> is a specialized version of word2vec, designed to work with languages in which words take different forms depending on their grammatical place. Rather than learning a word embedding and a context embedding for each full word (e.g.&nbsp;“quantify” and “quantification” each get their own embedding), fastText learns a vector for each shingle within a word (see <a href="tokenization.html#sec-shingles" class="quarto-xref"><span>Section 13.1.5</span></a>). For example, “quantify” might be broken up into “quant”, “uanti”, “antif”, and “ntify”. But it doesn’t treat each shingle as its own word. Rather, it trains on words just like word2vec and GloVe, but makes sure that the embedding of a word is equal to the <em>sum</em> of all of the shingle vectors inside it.</p>
<p>This approach is mostly unnecessary for English, where words are generally spelled the same wherever they appear. But for more morphologically rich languages like Hebrew, Arabic, French, or Finnish, fastText works much better than word2vec and GloVe. This is because there might not be enough data for word2vec and GloVe to learn reliable representations of every form of every word, especially rare forms. FastText, on the other hand, can focus on the important subcomponents of the words that stay the same across different forms. This way it can learn rich representations even of rare forms of a word that don’t appear in the training dataset (e.g.&nbsp;it could quantify the meaning of מחשבותייך even if it were only trained on מחשבה, מחשבות, חבר, and חברייך).</p>
<p>After downloading a pretrained model from <a href="https://fasttext.cc/docs/en/crawl-vectors.html">this page</a> <span class="citation" data-cites="grave_etal_2018">(<a href="#ref-grave_etal_2018" role="doc-biblioref">Grave et al., 2018</a>)</span>, you can use fastText in R through the <a href="https://cran.r-project.org/web/packages/fastTextR/vignettes/Word_representations.html"><code>fastTextR</code> package</a>. Conveniently, <code>fastTextR</code> includes a dedicated function for obtaining full text embeddings, <code><a href="https://rdrr.io/pkg/fastTextR/man/ft_sentence_vectors.html">ft_sentence_vectors()</a></code>.</p>
<div class="cell">
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/EmilHvitfeldt/fastTextR">fastTextR</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># example texts</span></span>
<span><span class="va">heb_words</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"מחשבותייך"</span>, <span class="st">"מחשבה"</span><span class="op">)</span></span>
<span><span class="va">heb_texts</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"הדבור מיחד את האדם מן החי, הדומם והצומח, הלשון היא – נפש חיה – רוח ממללה"</span>, <span class="st">"לשון היא המבדלת בין אומה אחת לחברתה, והיא החוט, שעליו נחרזות תמורות הנפש הרבות"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># load pretrained model from file</span></span>
<span><span class="va">heb_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/fastTextR/man/ft_load.html">ft_load</a></span><span class="op">(</span><span class="st">"data/cc.he.300.bin"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># get word embeddings</span></span>
<span><span class="va">word_vecs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/fastTextR/man/ft_word_vectors.html">ft_word_vectors</a></span><span class="op">(</span><span class="va">heb_model</span>, <span class="va">heb_words</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># get text embeddings</span></span>
<span><span class="va">text_vecs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/fastTextR/man/ft_sentence_vectors.html">ft_sentence_vectors</a></span><span class="op">(</span><span class="va">heb_model</span>, <span class="va">heb_texts</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advantages of FastText
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Better for Morphologically Rich Languages</strong></li>
<li><strong>Better for Rare Words</strong></li>
<li><strong>Can Infer Embeddings for Words That Were Not in Training Data</strong></li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Disadvantages of FastText
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>
<strong>More Complex:</strong> This means larger files to download when using pretrained models. It also increases the risk of overfitting.</li>
</ul>
</div>
</div>
</section><section id="sec-embedding-magnitude" class="level3" data-number="18.3.4"><h3 data-number="18.3.4" class="anchored" data-anchor-id="sec-embedding-magnitude">
<span class="header-section-number">18.3.4</span> Interpreting Advanced Word Embeddings</h3>
<p>Advanced word embedding algorithms like word2vec, GloVe, and fastText use the dot product of embeddings to measure how likely two words are to appear together. The dot product is the same as cosine similarity, except that it gets larger as the vectors get farther away from the origin (i.e.&nbsp;cosine similarity is the dot product of two normalized vectors).</p>
<div class="cell">
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">dot_prod</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">dot</span> <span class="op">&lt;-</span> <span class="va">x</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html">%*%</a></span> <span class="va">y</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/vector.html">as.vector</a></span><span class="op">(</span><span class="va">dot</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="decontextualized-embeddings_files/figure-html/unnamed-chunk-24-1.png" class="img-fluid figure-img" width="768"></p>
</figure>
</div>
</div>
</div>
<p>Recall that in models like word2vec and GloVe, the dot product corresponds to the probability that two words occur together. Vectors that are farther away from the origin will result in very positive or very negative dot products, making the model more confident in the pair of words either being neighbors or not. This means that the distance of a word embedding from the origin (also called the norm or magnitude) is proportional to the informativeness of the word <span class="citation" data-cites="schakel_wilson_2015 oyama_etal_2023">(<a href="#ref-oyama_etal_2023" role="doc-biblioref">Oyama et al., 2023</a>; <a href="#ref-schakel_wilson_2015" role="doc-biblioref">Schakel &amp; Wilson, 2015</a>)</span>. For example, the word “the” has a very low magnitude because it does not indicate a specific context, while the word “psychology” has a very high magnitude because its use is associated with a very specific context. Therefore, the magnitude of the embedding measures how representative it is of certain contexts as opposed to others, similar to averaging the TF-IDF of a word across a corpus (<a href="word-counting-improvements.html#sec-tfidf" class="quarto-xref"><span>Section 16.5</span></a>).</p>
<p>This is the reason why an accurate embedding of a full text can be obtained by averaging the embeddings of each of its words. You might think that averaging word embeddings will lead to overvaluing common words, like “the” and “I”, which appear more frequently but are not very informative about the text’s meaning. Don’t worry, because the magnitude of a word embedding is smaller for common words, which means that common words have less impact on the average <span class="citation" data-cites="ethayarajh_etal_2019">(<a href="#ref-ethayarajh_etal_2019" role="doc-biblioref">Ethayarajh et al., 2019</a>)</span>.</p>
<p>Once average embeddings are computed, we almost always use cosine similarity to assess the relationships between embeddings. <strong>The cosine similarity measures only the meanings of the two embeddings, while ignoring how specific they are to those meanings.</strong> If the specificity of texts to your construct of interest is important to your analysis, consider using the dot product instead of cosine similarity. Despite its unpopularity as a similarity metric, the dot product may sometimes be optimal for analyzing texts with decontextualized embeddings (<span class="quarto-unresolved-ref">?sec-ccr-validation</span>). For more applications of word embedding magnitude, see <a href="navigating-vectorspace.html" class="quarto-xref"><span>Chapter 20</span></a> and <a href="linguistic-complexity.html" class="quarto-xref"><span>Chapter 22</span></a>.</p>
<hr>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-alammar_2019" class="csl-entry" role="listitem">
Alammar, J. (2019). The illustrated Word2vec. In <em>Jay Alammar – Visualizing machine learning one concept at a time</em>. <a href="http://jalammar.github.io/illustrated-word2vec/">http://jalammar.github.io/illustrated-word2vec/</a>
</div>
<div id="ref-bojanowski_etal_2017" class="csl-entry" role="listitem">
Bojanowski, P., Grave, E., Joulin, A., &amp; Mikolov, T. (2017). <em>Enriching word vectors with subword information</em>. <a href="https://arxiv.org/abs/1607.04606">https://arxiv.org/abs/1607.04606</a>
</div>
<div id="ref-chatterjee_etal_2023" class="csl-entry" role="listitem">
Chatterjee, P., Mishra, H., &amp; Mishra, A. (2023). Does the first letter of one’s name affect life decisions? A natural language processing examination of nominative determinism. <em>Journal of Personality and Social Psychology</em>, <em>125</em>. <a href="https://doi.org/10.1037/pspa0000347">https://doi.org/10.1037/pspa0000347</a>
</div>
<div id="ref-deerwester_etal_1990" class="csl-entry" role="listitem">
Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., &amp; Harshman, R. A. (1990). Indexing by latent semantic analysis. <em>Journal of the Association for Information Science and Technology</em>, <em>41</em>(6), 391–407. <a href="https://doi.org/10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9">https://doi.org/10.1002/(SICI)1097-4571(199009)41:6&lt;391::AID-ASI1&gt;3.0.CO;2-9</a>
</div>
<div id="ref-ethayarajh_etal_2019" class="csl-entry" role="listitem">
Ethayarajh, K., Duvenaud, D., &amp; Hirst, G. (2019). <em>Towards understanding linear word analogies</em>. <a href="https://arxiv.org/abs/1810.04882">https://arxiv.org/abs/1810.04882</a>
</div>
<div id="ref-goldberg_levy_2014" class="csl-entry" role="listitem">
Goldberg, Y., &amp; Levy, O. (2014). <em>word2vec explained: Deriving mikolov et al.’s negative-sampling word-embedding method</em>. <a href="https://arxiv.org/abs/1402.3722">https://arxiv.org/abs/1402.3722</a>
</div>
<div id="ref-grave_etal_2018" class="csl-entry" role="listitem">
Grave, E., Bojanowski, P., Gupta, P., Joulin, A., &amp; Mikolov, T. (2018). Learning word vectors for 157 languages. <em>Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018)</em>.
</div>
<div id="ref-harris_1954" class="csl-entry" role="listitem">
Harris, Z. S. (1954). Distributional <span>Structure</span>. <em>WORD</em>, <em>10</em>(2-3), 146–162. <a href="https://doi.org/10.1080/00437956.1954.11659520">https://doi.org/10.1080/00437956.1954.11659520</a>
</div>
<div id="ref-kauf_etal_2024" class="csl-entry" role="listitem">
Kauf, C., Tuckute, G., Levy, R., Andreas, J., &amp; Fedorenko, E. (2024). <span class="nocase">Lexical-Semantic Content, Not Syntactic Structure, Is the Main Contributor to ANN-Brain Similarity of fMRI Responses in the Language Network</span>. <em>Neurobiology of Language</em>, <em>5</em>(1), 7–42. <a href="https://doi.org/10.1162/nol_a_00116">https://doi.org/10.1162/nol_a_00116</a>
</div>
<div id="ref-knief_forstmeier_2021" class="csl-entry" role="listitem">
Knief, U., &amp; Forstmeier, W. (2021). Violating the normality assumption may be the lesser of two evils. <em>Behavior Research Methods</em>, <em>53</em>. <a href="https://doi.org/10.3758/s13428-021-01587-5">https://doi.org/10.3758/s13428-021-01587-5</a>
</div>
<div id="ref-landauer_dumais_1997" class="csl-entry" role="listitem">
Landauer, T. K., &amp; Dumais, S. T. (1997). A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. <em>Psychological Review</em>, <em>104</em>, 211–240. <a href="https://api.semanticscholar.org/CorpusID:1144461">https://api.semanticscholar.org/CorpusID:1144461</a>
</div>
<div id="ref-mikolov_etal_2013b" class="csl-entry" role="listitem">
Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). <em>Efficient estimation of word representations in vector space</em>. <a href="https://arxiv.org/abs/1301.3781">https://arxiv.org/abs/1301.3781</a>
</div>
<div id="ref-mikolov_etal_2013c" class="csl-entry" role="listitem">
Mikolov, T., Sutskever, I., Chen, K., Corrado, G., &amp; Dean, J. (2013). <em>Distributed representations of words and phrases and their compositionality</em>. <a href="https://arxiv.org/abs/1310.4546">https://arxiv.org/abs/1310.4546</a>
</div>
<div id="ref-moss_etal_2006" class="csl-entry" role="listitem">
Moss, J., Kotovsky, K., &amp; Cagan, J. (2006). The <span>Role</span> of <span>Functionality</span> in the <span>Mental</span> <span>Representations</span> of <span>Engineering</span> <span>Students</span>: <span>Some</span> <span>Differences</span> in the <span>Early</span> <span>Stages</span> of <span>Expertise</span>. <em>Cognitive Science</em>, <em>30</em>(1), 65–93. <a href="https://doi.org/10.1207/s15516709cog0000_45">https://doi.org/10.1207/s15516709cog0000_45</a>
</div>
<div id="ref-oyama_etal_2023" class="csl-entry" role="listitem">
Oyama, M., Yokoi, S., &amp; Shimodaira, H. (2023). <em>Norm of word embedding encodes information gain</em>. <a href="https://arxiv.org/abs/2212.09663">https://arxiv.org/abs/2212.09663</a>
</div>
<div id="ref-pennington_etal_2014" class="csl-entry" role="listitem">
Pennington, J., Socher, R., &amp; Manning, C. D. (2014). GloVe: Global vectors for word representation. <em>Empirical Methods in Natural Language Processing (EMNLP)</em>, 1532–1543. <a href="http://www.aclweb.org/anthology/D14-1162">http://www.aclweb.org/anthology/D14-1162</a>
</div>
<div id="ref-rosario_2001" class="csl-entry" role="listitem">
Rosario, B. (2001). <em>Latent <span>Semantic</span> <span>Indexing</span> : <span>An</span> <span>Overview</span> 1 <span>Latent</span> <span>Semantic</span> <span>Indexing</span> : <span>An</span> overview <span>INFOSYS</span> 240 <span>Spring</span> 2000 <span>Final</span> <span>Paper</span></em>. <a href="https://www.semanticscholar.org/paper/Latent-Semantic-Indexing-%3A-An-Overview-1-Latent-%3A-Rosario/95981f057cb76a24329fcf2b572f75d8c2b1613e#citing-papers">https://www.semanticscholar.org/paper/Latent-Semantic-Indexing-%3A-An-Overview-1-Latent-%3A-Rosario/95981f057cb76a24329fcf2b572f75d8c2b1613e#citing-papers</a>
</div>
<div id="ref-schakel_wilson_2015" class="csl-entry" role="listitem">
Schakel, A. M. J., &amp; Wilson, B. J. (2015). <em>Measuring word significance using distributed representations of words</em>. <a href="https://arxiv.org/abs/1508.02297">https://arxiv.org/abs/1508.02297</a>
</div>
<div id="ref-torabi-asr_etal_2018" class="csl-entry" role="listitem">
Torabi Asr, F., Zinkov, R., &amp; Jones, M. (2018). Querying word embeddings for similarity and relatedness. In M. Walker, H. Ji, &amp; A. Stent (Eds.), <em>Proceedings of the 2018 conference of the north <span>A</span>merican chapter of the association for computational linguistics: Human language technologies, volume 1 (long papers)</em> (pp. 675–684). Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N18-1062">https://doi.org/10.18653/v1/N18-1062</a>
</div>
<div id="ref-tversky_gati_1982" class="csl-entry" role="listitem">
Tversky, A., &amp; Gati, I. (1982). Similarity, separability, and the triangle inequality. <em>Psychological Review</em>, <em>89</em>, 123–154. <a href="https://doi.org/10.1037/0033-295X.89.2.123">https://doi.org/10.1037/0033-295X.89.2.123</a>
</div>
</div>
</section></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr>
<ol>
<li id="fn1"><p>LSA was first introduced by <span class="citation" data-cites="deerwester_etal_1990">Deerwester et al. (<a href="#ref-deerwester_etal_1990" role="doc-biblioref">1990</a>)</span>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>The scale in Step 3 above has been modified for clarity. In reality, the values of LSA embeddings may be scaled down or flipped relative to the original word counts. Since reasoning in vector space relies on relative distances and angles, this change of scale has no effect on measurements.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>LSA document embeddings are sometimes explained as the sum of LSA word embeddings for each document. Mathematically, this is equivalent to the explanation provided here.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Many researchers use linear regression in situations like this, despite the violation of the linearity assumption. Generally this is fine, since linear regression is surprisingly robust <span class="citation" data-cites="knief_forstmeier_2021">(<a href="#ref-knief_forstmeier_2021" role="doc-biblioref">Knief &amp; Forstmeier, 2021</a>)</span>.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Cognitive scientists have long debated the extent to which the way DSMs learn meaning is similar to the way humans learn meaning. For an interesting recent paper in this field, see <span class="citation" data-cites="kauf_etal_2024">Kauf et al. (<a href="#ref-kauf_etal_2024" role="doc-biblioref">2024</a>)</span>.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Some people think word2vec is too simple to be called a neural network. If you are one of these people, you are welcome to think of word2vec as a fancy sort of logistic regression instead.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>This section is partially adapted from <span class="citation" data-cites="alammar_2019">Alammar (<a href="#ref-alammar_2019" role="doc-biblioref">2019</a>)</span><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>To learn why models like word2vec use dot products instead of cosine similarity, see <a href="#sec-embedding-magnitude" class="quarto-xref"><span>Section 18.3.4</span></a> below.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>To the best of our knowledge, pretrained context embeddings are not available online. So if you are interested in associative (rather than conceptual) relationships between words, we recommend training your own model (see <a href="#sec-glove-training" class="quarto-xref"><span>Section 18.3.2.1</span></a>).<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>GloVe is built on the same metric that we used in <a href="dla.html" class="quarto-xref"><span>Chapter 15</span></a>: relative frequency ratios. Rather than comparing two word frequencies in two groups of texts as we did in that chapter, it instead compares co-occurrence with one word to co-occurrence with another.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>Another notable difference between GloVe and word2vec is that the GLoVe averages the word embeddings and context embeddings rather than using only the word embeddings as word2vec does. This makes GloVe embeddings slightly better at representing overall meaning, but may blur the distinction between conceptual similarity and mental/linguistic association <span class="citation" data-cites="torabi-asr_etal_2018">(<a href="#ref-torabi-asr_etal_2018" role="doc-biblioref">Torabi Asr et al., 2018</a>)</span>.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/ds4psych\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./vectorspace-intro.html" class="pagination-link" aria-label="Introduction to Vector Space">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Introduction to Vector Space</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./contextualized-embeddings.html" class="pagination-link" aria-label="Contextualization With Large Language Models">
        <span class="nav-page-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Contextualization With Large Language Models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>Data Science for Psychology was written by <a href="https://rimonim.github.io">Louis Teitelbaum</a> and <a href="https://almogsi.com">Almog Simchon</a>.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/rimonim/ds4psych/blob/main/decontextualized-embeddings.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/rimonim/ds4psych/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a> and is powered by <a href="https://www.netlify.com/">Netlify</a></p>
</div>
  </div>
</footer>


</body></html>