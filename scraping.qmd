# Web Scraping {#sec-scraping}

```{r setup}
#| echo: false
#| include: false

source("_common.R")
```

In some cases, data are available for free on the internet, but the host does not provide an API. In such cases, we can write code to retrieve the data directly from the web page. This is called web scraping.

## Be Polite

Often, scraping requires the code to visit many web pages. For example, say we wanted a list of poems by English poets, along with the birth dates of their authors. We might have the computer scrape the [list of English poets from the Poetry Foundation](https://www.poetryfoundation.org/poets/browse#page=1&sort_by=recently_added&region=175), which requires clicking through 17 pages of search results, and retrieve the author's birth date and the URL for each author's full list of poems. We could then have the computer visit each of those URLs and retrieve the URL for each individual poem by each author. Finally, the computer would visit each poem URL and retrieve the text of the poem. In all, this process might require visiting hundreds or even thousands of web pages.

For the same reasons that APIs generally have rate limits [@sec-apis], web scraping algorithms should be polite---not, for example, requesting thousands of pages in the space of a few seconds. If you are not polite, you are likely to get banned as a bot.

To read more about web scraping etiquette and easy ways to implement it, see [the relevant chapter in R for Data Science](https://r4ds.hadley.nz/webscraping), and the homepage of the  [polite](https://dmi3kno.github.io/polite/) package. 

## A Simple Example

To give you a taste for what web scraping is like, we will give a simple example of scraping a single page.

The page in this case will be [the blog reel of one the authors of this book, Louis](https://rimonim.github.io/blog.html).

![](images/scraping1.png)

We will scrape the name and date of each blog post using the [rvest](https://rvest.tidyverse.org) package. First, we retrieve the raw html code of the webpage. 

```{r}
#| output: false
library(rvest)

html <- read_html("https://rimonim.github.io/blog.html")
```

Then, we have to identify the name of the particular objects we are looking for. In most cases, this can be done by right clicking and pressing "Inspect Element" in the web browser.^[Using "Inspect Element" on more complicated web pages can be difficult. For finding 
element names more easily, we recommend [SelectorGadget](https://rvest.tidyverse.org/articles/selectorgadget.html)]

![](images/scraping2.png)

Now that we know that blog titles are called "h3.no-anchor.listing-title", we can extract those objects from the raw html using `html_elements()` and convert them to regular text using `html_text2()`.

```{r}
post_titles <- html |> 
  html_elements("h3.no-anchor.listing-title") |> 
  html_text2()

head(post_titles)
```

To build a dataset with multiple variables, we repeat this process for each variable.

```{r}
posts <- tibble(
  date = html |> 
    html_elements("div.listing-date") |> 
    html_text2(),
  title = post_titles
)

head(posts)
```

This has been a very basic example. For a more in-depth tutorial on web scraping (including more complex examples), see [the relevant chapter in R for Data Science](https://r4ds.hadley.nz/webscraping).
