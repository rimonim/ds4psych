# Open Vocabulary Word Counting {#sec-dla}

```{r setup}
#| echo: false
#| include: false

source("_common.R")
library(quanteda)

hippocorpus_corp <- read_csv("data/hippocorpus-u20220112/hcV3-stories.csv") |> 
  select(AssignmentId, story, memType, summary, WorkerId, 
         annotatorGender, openness, timeSinceEvent) |> 
  corpus(docid_field = "AssignmentId", 
         text_field = "story")

hippocorpus_dfm <- hippocorpus_corp |> 
  tokens(remove_punct = TRUE) |> 
  dfm() |> 
  dfm_remove("~")
```

In @sec-word-counting, we used a preexisting list of surprise-related words to test whether true autobiographical stories are more surprise-related than imagined stories. Sometimes though, it can be informative to take a theory-free approach to the difference between two groups. We might ask: **How are the words used in true autobiographical stories different from the words used in imagined stories?** We call the investigation of questions like this open vocabulary analyses, a term coined by @schwartz_etal_2013.

Both open vocabulary analyses and dictionary-based methods use word counts, but they come from opposite directions: Dictionary-based methods start with a construct and use it to examine the difference between groups (or levels of a continuous variable). The open vocabulary approach, on the other hand, starts with the difference between groups (or levels of a continuous variable) and generates a list of words that can then be identified with one or more constructs after the fact.

Open vocabulary analyses are useful for three types of applications:

- Open vocabulary analyses are sometimes a good second step in EDA, after looking at your data directly (see @sec-look-at-your-data). If you are planning an analysis of token counts (e.g. using dictionary-based methods), open vocabulary analyses are a good way to look for overall patterns in the way that tokens are distributed across your groups, or to look for individual tokens that are particularly representative of one group or another.
- When training a machine learning model on token counts (@sec-machine-learning-word-counts), you will need to decide which variables (AKA features) to include in the model. You can use an open vocabulary approach to find the tokens that carry the most information about your outcome variable.
- Open vocabulary analyses can be a final product! In some cases, researchers want to characterize difference between the language use of two groups, without being constrained by particular dictionaries. @schwartz_etal_2013 provide many elegant examples of this approach.

## Frequency Ratios {#sec-freq-ratios}

The most intuitive way to compare texts from two groups is one we already explored in the context of data visualization in @sec-word-viz: frequency ratios. To get frequency ratios from a Quanteda DFM, we can use the `textstat_frequency()` function from the [`quanteda.textstats`](https://github.com/quanteda/quanteda.textstats) package, with the `groups` parameter set to the categorical variable of interest. Let's compare true stories from fictional ones in the _Hippocorpus_ data.

```{r}
library(quanteda.textstats)

imagined_vs_recalled <- hippocorpus_dfm |> 
  textstat_frequency(groups = memType)

head(imagined_vs_recalled)
```

In the resulting dataframe, each row represents one feature within each category. `frequency` is the number of times the feature appears in the group, `rank` is the ordering from highest to lowest frequency within each group, and `docfreq` is the number of documents in the group in which the feature appears at least once. To compare imagined stories to recalled ones, we can calculate frequency ratios. 

```{r}
imagined_vs_recalled <- imagined_vs_recalled |> 
  filter(group %in% c("imagined", "recalled")) |> 
  pivot_wider(id_cols = "feature", 
              names_from = "group", 
              values_from = "frequency",
              names_prefix = "count_") |> 
  mutate(freq_imagined = count_imagined/sum(count_imagined, na.rm = TRUE),
         freq_recalled = count_recalled/sum(count_recalled, na.rm = TRUE),
         imagined_freq_ratio = freq_imagined/freq_recalled)

head(imagined_vs_recalled)
```

We can now plot a rotated F/F plot, as in @sec-word-viz.

```{r}
#| warning: false
library(ggiraph, verbose = FALSE)
library(ggrepel)

set.seed(2023)
p <- imagined_vs_recalled |> 
  mutate(
    # calculate total frequency
    common = (freq_imagined + freq_recalled)/2,
    # remove single quotes (for html)
    feature = str_replace_all(feature, "'", "`")) |> 
  ggplot(aes(imagined_freq_ratio, common, 
             label = feature,
             color = imagined_freq_ratio,
             tooltip = feature, 
             data_id = feature
             )) +
    geom_point_interactive() +
    geom_text_repel_interactive(size = 2) +
    scale_y_continuous(
      trans = "log2", breaks = ~.x,
      minor_breaks = ~2^(seq(0, log2(.x[2]))),
      labels = c("Rare", "Common")
      ) +   
    scale_x_continuous(
      trans = "log10", limits = c(1/10,10),
      breaks = c(1/10, 1, 10),
      labels = c("10x More Common\nin Recalled Stories",
                 "Equal Proportion",
                 "10x More Common\nin Imagined Stories")
      ) +
    scale_color_gradientn(
      colors = c("#023903", 
                 "#318232",
                 "#E2E2E2", 
                 "#9B59A7",
                 "#492050"), 
      trans = "log2", # log scale for ratios
      guide = "none"
      ) +
    labs(
      title = "Words in Imagined and Recalled Stories",
      x = "",
      y = "Total Frequency",
      color = ""
    ) +
    # fixed coordinates since x and y use the same units
    coord_fixed(ratio = 1/8) + 
    theme_minimal()

girafe_options(
  girafe(ggobj = p),
  opts_tooltip(css = "font-family:sans-serif;font-size:1em;color:Black;")
  )
```

This view allows us to explore individual words that are characteristic of one or the other group. It also shows the overall shape of the distribution---the slight skew to the left suggests that recalled stories tend to have a slightly richer vocabulary than imagined ones.

## Keyness {#sec-keyness}

Working with frequency ratios has an intuitive appeal that is perfect for data visualization; "10 times more common in group A than group B" is a statement that anyone can understand. Even so, **frequency ratios are statistically misleading**, since they do not account for random sampling error. For example, a word that appears 1000 times in group A and 100 times in group B is much more convincingly representative of group A than a word that appears 10 times in group A and 1 time in group B, even though the frequency ratio is identical. Furthermore, simple frequency ratios do not account for base rates---ratios can be skewed to one side simply because texts in one group are longer than those in another. The F/F plot in @sec-freq-ratios solved this problem by working with relative frequencies (the count divided by total words in the group; @sec-relative-tokenization), but these problems can be solved more elegantly by using statistically motivated methods for group comparisons, sometimes referred to as _keyness_ statistics.

Quanteda's default keyness statistic is none other than the chi-squared value, which measures how different the two groups are relative to what might be expected by random chance^[More precisely, the chi-squared statistic is the sum of the differences between observed frequencies and the frequencies that would be expected if there were no difference between the groups, divided by the expected frequency.]. We can compute the chi-squared statistic directly from the DFM using the `textstat_keyness()` function from  [`quanteda.textstats`](https://github.com/quanteda/quanteda.textstats), with the "target" parameter set to one of the two groups of documents, in this case imagined ones, or `docvars(imagined_vs_recalled_dfm, "memType") == "imagined"`. This group is referred to as the target group, while everything else becomes the reference group. The function keeps a token's chi-squared statistic positive if it is more common than expected in the target group, or flips it to negative if it is less common than expected in the target group. Though chi-squared is the default, we recommend using a likelihood ratio test (`measure = "lr"`) for most applications. This setting produces the G-squared statistic, a close relative of chi-squared. For smaller samples, we recommend Fisher's exact test (`measure = "exact"`), which is more reliable for hypothesis testing but takes much longer to compute.

```{r}
# Filtered DFM
imagined_vs_recalled_dfm <- hippocorpus_dfm |> 
  # only keep features that appear in at least 30 documents
  dfm_trim(min_docfreq = 30) |>
  # only keep imagined and recalled stories (not retold)
  dfm_subset(memType %in% c("imagined", "recalled"))

# Calculate Keyness
imagined_keyness <- imagined_vs_recalled_dfm |> 
  textstat_keyness(docvars(imagined_vs_recalled_dfm, "memType") == "imagined",
                   measure = "lr")

head(imagined_keyness)
```

We can use Quanteda to generate a simple plot of the most extreme values of our keyness statistic, using the `textplot_keyness()` function, from the [`quanteda.textplots`](https://quanteda.io/articles/pkgdown/examples/plotting.html) package. 

```{r}
imagined_keyness |> 
  quanteda.textplots::textplot_keyness() +
    labs(title = "Words in Imagined (target) and Recalled (reference) Stories")
```

We can also represent the keyness values as a word cloud, as we did for frequency ratios in @sec-word-viz.

```{r}
#| warning: false
#| fig-height: 8
library(ggwordcloud)
set.seed(2)

imagined_keyness |> 
  # only words with significant difference to p < .001
  filter(p < .001) |> 
  # arrange in descending order
  arrange(desc(abs(G2))) |> 
  # plot
  ggplot(aes(label = feature, 
             size = G2, 
             color = G2 > 0,
             angle_group = G2 > 0)) +
    geom_text_wordcloud_area(eccentricity = 1, show.legend = TRUE) + 
    scale_size_area(max_size = 30, guide = "none") +
    scale_color_discrete(
      name = "",
      breaks = c(FALSE, TRUE),
      labels = c("More in Recalled", 
                 "More in Imagined")
      ) +
    labs(caption = "Only words with a significant difference between the groups (p < .001) were included.") +
    theme_void() # blank background
```

Keyness plots like this are often a good second step in EDA, after looking at your data directly (see @sec-look-at-your-data). For an even better chance at noticing interesting patterns, we recommend generating keyness plots for n-grams and shingles in addition to words (see @sec-tokenization). 

## Continuous Covariates

Open vocabulary analysis does not require two distinct groups. For example, we may want to investigate the differences in language associated with openness to experience. A simple way to do this is to measure the correlation between word frequencies in documents and the documents' authors' `openness` score. 

```{r}
# openness score of the author of each document
openness_scores <- docvars(hippocorpus_corp)$openness

hippocorpus_dfm
openness_cor <- hippocorpus_dfm |> 
  convert(to = "data.frame") |> 
  # compute rank-based correlation
  summarise(
    across(
      -doc_id, 
      ~ cor(.x, openness_scores, method = "spearman")
      )
    ) |> 
  pivot_longer(everything(), 
               names_to = "feature", 
               values_to = "cor")

head(openness_cor)
```

Now that we have correlation coefficients, we can once again make a word cloud.

```{r}
#| warning: false
#| fig-height: 8
set.seed(2)

openness_cor |> 
  # arrange in descending order
  arrange(desc(abs(cor))) |> 
  # only top correlating words
  slice_head(n = 150) |> 
  # plot
  ggplot(aes(label = feature, 
             size = abs(cor), 
             color = cor,
             angle_group = cor < 0)) +
    geom_text_wordcloud_area(eccentricity = 1, show.legend = TRUE) + 
    scale_size_area(max_size = 10, guide = "none") +
    scale_color_gradient2(
      name = "",
      low = "red3", mid = "grey", high = "blue3",
      breaks = c(-.05, 0, .05),
      labels = c("Negatively Correlated\nwith Openness", "",
                 "Positively Correlated\nwith Openness")
      ) +
    theme_void() # blank background
```

## Generating Dictionaries With Open Vocabulary Analysis {#sec-generating-dictionaries}

In @sec-dictionary-sources we saw that dictionaries for word counting can be generated in many ways. One of these ways is to start with a corpus and proceed with open vocabulary analysis. As an example, let's use the [Crowdflower Emotion in Text dataset](https://data.world/crowdflower/sentiment-analysis-in-text) to generate a new dictionary for the emotion of surprise. The Crowdflower dataset includes 40,000 Twitter posts, each labeled with one of 13 sentiments. Of these, 2187 are labeled as reflecting surprise.

```{r}
#| echo: false
#| output: false

# data from https://data.world/crowdflower/sentiment-analysis-in-text
crowdflower <- read_csv("data/text_emotion.csv") |> 
  rename(text = content)
```

```{r}
glimpse(crowdflower)
```

Generating a dictionary is a subtly different goal than discovering the differences between groups. In @sec-keyness above, we recommended the likelihood ratio test as a way of looking for statistically significant differences. When generating a dictionary though, we care less about statistical significance and more about information---if I observe this word in a text, how much does it tell me about the text? For this kind of question, we recommend a different keyness statistic: pointwise mutual information (PMI). 

PMI can be computed just like the likelihood ratio test, using `textstat_keyness()`. We will need to be careful though, since PMI is sensitive to rare tokens---if a token appears only once in the corpus, in a text labelled surprise, it will be identified as giving lots of information about whether the text is labeled surprise. This is true enough within our training corpus, but with such a small sample size, the appearance of a rare token in a text is probably due to random variation. Rare tokens are therefore unlikely to generalize to other data sets when we use them in a dictionary. So we'll want to keep tokens with a high PMI, but remove very rare ones.

```{r}
# convert to corpus
crowdflower_corp <- crowdflower |> 
  corpus(docid_field = "tweet_id")

# convert to DFM
crowdflower_dfm <- crowdflower_corp |> 
  tokens(remove_punct = TRUE) |> 
  tokens_ngrams(n = c(1L, 2L)) |>  # 1-grams and 2-grams
  dfm()

# compute PMI
surprise_pmi <- crowdflower_dfm |> 
  textstat_keyness(
    docvars(crowdflower_dfm, "sentiment") == "surprise",
    measure = "pmi"
    )

# filter out rare words and low PMI
surprise_pmi <- surprise_pmi |> 
  filter(n_target + n_reference > 10, 
         pmi > 1.5)

surprise_pmi
```

We are left with a list of the unigrams and bigrams that most indicate that a tweet contains surprise. We can now use this list as a dictionary.

```{r}
tweet_surprise_dict <- dictionary(
  list(surprise = surprise_pmi$feature),
  separator = "_" # in n-grams, tokens are separated by "_"
)
```

Can we use this new surprise dictionary to test our hypothesis that true autobiographical stories include more surprise than imagined stories? Maybe. There is no guarantee that surprise words in Tweets will be the same as surprise words in autobiographical stories. With this in mind, we can proceed cautiously:

```{r}
# count words
hippocorpus_surprise_df <- hippocorpus_dfm |> 
  dfm_lookup(tweet_surprise_dict) |> # count dictionary words
  convert("data.frame") |> # convert to dataframe
  right_join(
    hippocorpus_corp |> 
      convert("data.frame") # convert to dataframe
    ) |> 
  mutate(wc = ntoken(hippocorpus_dfm)) # total word count

# model
tweet_surprise_mod <- MASS::glm.nb(surprise ~ memType + log(wc),
                                   data = hippocorpus_surprise_df)
summary(tweet_surprise_mod)
```

Using our dictionary generated from Tweets with open vocabulary analysis, we find no significant difference between true autobiographical stories and imagined stories in the amount of suprise-related language they contain (p = `r round(summary(tweet_surprise_mod)$coefficients["memTyperecalled","Pr(>|z|)"],3)`).

---
