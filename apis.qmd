# Web APIs {#sec-apis}

```{r setup}
#| echo: false
#| include: false

source("_common.R")
```

In @sec-corpora, we listed numerous placed to find datasets online. The sorts of datasets discussed there are generally formatted as a .csv file---or something like it---which you can download directly to your computer and import into R using `read_csv()`. Lot's of online data, however, are not pre-packaged in this way. When datasets are very large or complex, or are being updated regularly, the hosts of the data will instead provide a **web API**.

You can think of web APIs as little programming languages that are written inside a URL (URLs are the things you write in the top of your web browser to tell it which web page you want to access). When you access your custom-written URL, the host will perform whatever computations are necessary to get you your data, and send it to you in the same way that it would send you a website.

Web APIs are sometimes difficult to use because each one works differently. Learning to use a new API can be like learning a new programming language. For this reason, web APIs sometimes have associated **wrappers**, packages that allow you to communicate with the API in a more familiar format.

In the first part of this chapter, we introduce the [`vosonSML` package](https://github.com/vosonlab/vosonSML), an R-friendly wrapper that provides easy access to the Reddit, Twitter, Mastadon, and YouTube APIs. At the end of the chapter, we will discuss web APIs that do not have a convenient wrapper in R.

## API Basic Concepts

-   **Requests:** Each time you visit a URL associated with an API, you are submitting a *request* for data.
-   **Endpoints:** Every API has at least one *endpoint*, a contact point for particular types of requests.
-   **Rate Limits:** Many APIs set limits on the number of requests you can make per minute (or per second). This is because processing requests costs time and money for the host. If you go beyond the rate limit, the API will return an error like *"429 Too Many Requests"*.
-   **Authentication:** Some APIs are not open to the public, instead requiring users to apply for access or pay for a subscription. When accessing these APIs, you need an *API key* or an *access token*. This is your password for the API.

### vosonSML

```{r}
library(vosonSML)
```

For accessing social media APIs with vosonSML, you only need two functions:

-   `Authenticate()` creates a credential object that contains any keys or access tokens needed to access a particular API. This credential object can be reused as long as your credentials don't change.
-   `Collect()` initiates a series of API requests and stores the results as a dataframe or list of dataframes.

vosonSML also provides tools for working with network data (i.e. the ways in which users or posts are connected to one another), but these will not be covered in this textbook.

## Reddit

Reddit [generated over 3 billion posts and comments in 2022](https://www.statista.com/statistics/1319008/reddit-content-created/). Many of these contain long-form text. And its API is free. These traits make it very useful to researchers.

Reddit content exists on three levels:

- **Communities**, called "subreddits" are spaces for users to post about a specific topic. Individual subreddits are referred to as "r/SUBREDDIT". For example, [r/dataisbeautiful](https://www.reddit.com/r/dataisbeautiful/) is for data visualizations, [r/NaturalLanguage](https://www.reddit.com/r/NaturalLanguage/) is for posts about natural language processing, and [r/SampleSize](https://www.reddit.com/r/SampleSize/) is a place to gather volunteer participants for surveys and polls. Communities are policed by _moderators_, users who can remove posts or ban other users from the community. 
- **Posts** are posted by users to a particular subreddit. Each post has a **title**, which is always text, and **content**, which can contain text, images, and videos.
- **Comments** are responses to posts, responses to responses to posts, responses to responses to responses to posts, etc. These are always text.

### The Reddit Algorithm

Reddit data are not representative samples of the global population. They are not even representative samples of Reddit users. This is partly due to the dynamics of the Reddit ranking algorithm, which gives precedent to viral content. The details of the algorithm are [no longer public](https://medium.com/hacking-and-gonzo/how-reddit-ranking-algorithms-work-ef111e33d0d9), but it is largely based on "upvotes" and "downvotes" from the community, and probably also incorporates the time since posting. The ranking system for comments is almost certainly different from the ranking system for posts. Reddit also has a "Karma" system, by which users who post popular content get subsequent content boosted. This creates an incentive system which is [sometimes exploited by the advertising industry](https://www.reddit.com/r/ModSupport/comments/162p502/why_is_reddit_doing_nothing_to_handle_the_obvious/). The bottom line: **Reddit posts are disproportionately viral**. To partially counteract this when gathering Reddit data, set the API to sort by recency (`sort = "new"`) rather than the default, "best" [@sec-reddit-threads].

### Communities

```{r}
APIAdvocacy_posts <- 
  Authenticate("reddit") |>
  Collect(
    endpoint = "listing", 
    subreddits = "RedditAPIAdvocacy",
    sort = "new",  # newest posts first
    period = "all", # all time
    max = 20, # 20 most recent posts
    verbose = TRUE
    )

head(APIAdvocacy_posts)
```

**An example of using Reddit communities in research:** In a series of experiments, @ashokkumar_pennebaker_2022 had participants write small free-form responses and fill out questionnaires on group identity strength. Using dictionary-based word-counts [@sec-word-counting], they then identified dictionaries that were associated with questionnaire-based measure, and used these to construct a composite measure of "unquestioning affiliation". Among college students, they showed that unquestioning affiliation in writing could predict whether students would drop out from college one year later. Finally, they applied their method to naturalistic data retrieved from the Reddit API, from the political communities r/The_Donald and r/hillaryclinton, and showed that users' unquestioning affiliation predicted the duration that they would stay in the community before leaving.

- r/depression, r/ SuicideWatch, r/bipolarreddit, and r/opiates,
- r/changemyview, r/IAmA, and r/ExplainLikeImFive
- r/politics

### Threads {#sec-reddit-threads}

A post with all of its associated comments is called a _thread_. For example, Hadley Wickham, the founder of [the tidyverse](https://www.tidyverse.org), ran a thread on the r/dataisbeautiful subreddit in 2015, in which he answered commenters' questions. To retrieve that thread, first find its URL. Do this by finding the threat on Reddit and copying the link from your web browser. Then call `Authenticate("reddit")` and `Collect()`, like so:

```{r}
# List of thread urls (in this case only one)
threads <- c("https://www.reddit.com/r/dataisbeautiful/comments/3mp9r7/im_hadley_wickham_chief_scientist_at_rstudio_and/")

# Retrieve the data
## Since the Reddit API is open, we don't need
## to give any passwords to Authenticate()
hadley_threads <- 
  Authenticate("reddit") |>
  Collect(
    threadUrls = threads, 
    sort = "new", # newest comments first
    verbose = TRUE # give updates while running
    )

# Peak at Hadley's responses
hadley_threads |>
  filter(user == "hadley") |> 
  head()
```

The resulting dataframe has many columns. The most useful are the following:

- `comment` is the text of the comment itself.
- `user` is user who posted the comment.
- `structure` is the tree structure leading to the comment. For example, "137_6_2" is the 2nd comment on the 6th comment on the 137th comment on the original post.
- `comm_date` it the date and time of the comment, in UTC. Since this is in character format, it needs to be converted to a datetime with `lubridate::as_datetime()`.

By processing the `structure` values, we can conceptualize the thread as a tree, with the original post (OP) as the root and comments as branches:

```{r}
library(ggraph)

hadley_threads |> 
  mutate(
    level = str_count(structure, "_") + 1L,
    parent = str_remove(structure, "_[[:digit:]]+$"),
    parent = if_else(level == 1, "0", parent)
    ) |> 
  select(parent, structure) |> 
  tidygraph::as_tbl_graph() |> 
  ggraph(layout = 'tree', circular = FALSE) +
    geom_edge_diagonal(alpha = .2) +
    geom_node_point(shape = 21, fill = "orangered") +
    theme_void()
```

**An example of using Reddit threads in research:** @xiao_mensah_2022 collected threads from r/changemyview, a community in which the OP makes a claim, commenters make arguments against that claim, and the OP responds with "delta points" to indicate how much their view has been changed. @xiao_mensah_2022 analyzed the frequency of delta points at each level of the thread tree, and found that the OP's view tended to change most after the 2nd, 4th, 6th, 8th, and 10th levels of comments---in other words, every other level. They then analyzed the semantic similarity between comments using cosine similarity [@sec-cosine-similarity] between simple word counts. The results suggested that every-other-level comments tend to elaborate and refine the comments immediately before them, so that the latter are perceived to be more persuasive.

::: {.callout-tip icon="false"}
## Advantages of Reddit Data

-   **Explicit Communities:** Reddit communities are clearly defined and explicit about their purposes. Reddit includes communities devoted to fictional storytelling, factual storytelling, personal reflection, technical advice, political discussion, joke telling, and much more. This makes it to gather a domain-specific sample.
-   **Long-form Text Responses:** While some social media platforms have word limits for posts or comments, Reddit has many communities devoted to long-form text. Longer documents can make characterization of their content more accurate.
-   **Anonymity:** Reddit users can remain relatively anonymous, which might encourage more honest and open sharing of experiences.
:::

::: {.callout-important icon="false"}
## Disadvantages of Reddit Data

-   **Selection Bias:** Certain subreddits may attract specific demographics, leading to potential selection bias in the data.
-   **Anonymity:** In some cases, anonymity may make user behavior less representative. For example, many users have multiple accounts, which they use for different activities on the platform, making users seem disproportionately narrow in their interests. 
:::

## Twitter

::: {.callout-tip icon="false"}
## Advantages of Twitter Data

-   **Hashtag Tracking:** Researchers can track specific hashtags related to psychological phenomena.
-   **Character Limit:**
:::

::: {.callout-important icon="false"}
## Disadvantages of Twitter Data

-   **Character Limit:** The character limit on tweets may limit the depth of responses and the ability to convey complex psychological experiences.
-   **Sampling Bias:** Twitter users may not be representative of the general population, leading to potential sampling bias.
-   **Limited Context:** Tweets may lack context, making it challenging to fully understand the meaning behind short messages.
-   **No more free API:** While lots of historical Twitter data are available on the internet [see @sec-corpora], [the API has been prohibitively expensive since 2023](https://www.wired.com/story/twitter-data-api-prices-out-nearly-everyone/).
:::

## Mastadon

Many servers have a theme based on a specific interest. It is also common for servers to be based around a particular locality, region, ethnicity, or country.

Twitter and Mastodon use hashtags to categorize and organize content. subreddits on Reddit, instances on Mastodon

On a standard Mastodon instance, these messages can include up to 500 text-based characters, greater than Twitter's 280-character limit.

Mastodon uses community-based moderation, in which each server can limit or filter out undesirable types of content, while Twitter uses a single, global policy on content moderation.

-   organized into servers for topic-specific *users*

```{r}

```

::: {.callout-tip icon="false"}
## Advantages of Mastadon Data

-   **Hashtag Tracking:** Researchers can track specific hashtags related to psychological phenomena.
-   **Character Limit:**
:::

::: {.callout-important icon="false"}
## Disadvantages of Mastadon Data

-   **Character Limit:** The character limit on tweets may limit the depth of responses and the ability to convey complex psychological experiences.
-   **Sampling Bias:** Twitter users may not be representative of the general population, leading to potential sampling bias.
-   **Limited Context:** Tweets may lack context, making it challenging to fully understand the meaning behind short messages.
-   MASTADON **Customization:** Users can choose or create instances with specific rules and communities, allowing for more targeted data collection.
-   MASTADON **Smaller User Base:** Mastodon has a smaller user base compared to major platforms, potentially limiting the diversity of data.
-   MASTADON **Less Visibility:** The decentralized nature may make it harder to discover relevant conversations or trends.
:::

## YouTube

```{r}

```

@rosenbusch_etal_2019

::: {.callout-tip icon="false"}
## Advantages of YouTube Data

-   
:::

::: {.callout-important icon="false"}
## Disadvantages of YouTube Data

-   
:::

## Other Web APIs {#sec-other-apis}

https://en.wikipedia.org/wiki/Comparison_of_microblogging_and_similar_services https://the-federation.info

-   Wikipedia (including user data)
-   Facebook
-   Semantic Scholar
-   [CORE](https://core.ac.uk/services/dataset)
-   StackOverflow

------------------------------------------------------------------------
