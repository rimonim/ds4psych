---
include-in-header:
  - text: |
      <style>
      .r-output code {
        word-break: break-wor !important;
        white-space: pre-wrap !important;
      }
      </style>
---

# Navigating Vector Space {#sec-navigating-vectorspace}

```{r setup}
#| echo: false
#| include: false

source("_common.R")
library(quanteda)
library(word2vec)

source("vector_scripts.R")

# prep word2vec
word2vec_mod <- "data/GoogleNews-vectors-negative300.bin"
word2vec_mod <- read.word2vec(file = word2vec_mod, normalize = TRUE)

# prep Hippocorpus
hippocorpus_df <- read_csv("data/hippocorpus-u20220112/hcV3-stories.csv") |> 
  select(AssignmentId, story, memType, summary, WorkerId, 
         annotatorGender, openness, timeSinceEvent)

hippocorpus_corp <- hippocorpus_df |> 
  corpus(docid_field = "AssignmentId", 
         text_field = "story")

hippocorpus_dfm <- hippocorpus_corp |> 
  tokens(remove_punct = TRUE) |> 
  dfm() |> 
  dfm_remove("~")

# SBERT embeddings
hippocorpus_sbert <- readRDS("data/hippocorpus_sbert.rds")
hippocorpus_sbert <- hippocorpus_df |>
  rename(ID = AssignmentId) |> 
  left_join(hippocorpus_sbert)

surprise_ccr <- readRDS("data/surprise_sbert.rds")

# for ANSI rendering
ansi_aware_handler <- function(x, options) {
  paste0(
    "<pre class=\"r-output\"><code>",
    fansi::sgr_to_html(x = x, warn = FALSE, term.cap = "256"),
    "</code></pre>"
  )
}
knitr::knit_hooks$set(
  output = ansi_aware_handler, 
  message = ansi_aware_handler, 
  warning = ansi_aware_handler,
  error = ansi_aware_handler
)
```

::: {.callout-important}
## This page is still under construction. Come back soon!
:::

## Representing Psychological Constructs

In @sec-decontextualized-embeddings we measured the surprise in texts by comparing their embeddings to that of a single word: "surprised". But does the embedding of the word "surprised" fully capture the concept of surprise as an emotion? Faced with this question of construct validity, we have two options:

1.    **Conduct a Validation Study:** We could find or construct a dataset of texts that were rated by a human (or ideally, multiple humans) on the extent to which they reflect the emotion of surprise. We could then compare our embedding-based surprise scores to the human rating and demonstrate that they correlate strongly. We could further note areas of disagreement between the human and embedding-based measures and investigate whether these reflect a difference between the constructs they are measuring.
2.    **Use an Already-Validated Construct Definition:** Properly validating a new measure is hard work. When possible, psychology researchers often prefer to use an existing measure that has already been carefully validated in the past. But embeddings are very new to the field, so few if any validated vector representations of constructs are available. As it turns out, this is not a problem---any language-based psychological measure can be represented as a vector!

### Distributed Dictionary Representation (DDR)

Let's begin with a straightforward sort of psychological measure---the dictionary. We have already discussed dictionaries extensively in @sec-word-counting and noted that psychology researchers have been constructing, validating, and publicizing dictionaries for decades (@sec-dictionary-sources). But these dictionaries are designed for word counting---How do we apply them to a vector-based analysis? @garten_etal_2018 propose a simple solution: Get word embeddings (@sec-word-embeddings) for each word in the dictionary, and average them together to create a single Distributed Dictionary Representation (DDR). The dictionary construct can then be measured by comparing text embeddings to the DDR.

DDR cannot entirely replace word counts; for linguistic concepts like pronoun use or the passive voice, dictionary-based word counts are still necessary. But DDR is ideal for studies of abstract constructs like emotions, that refer to the general gist of a text rather than particular words. The rich representations of word embeddings allow DDR to capture even the subtlest associations between words and constructs, and to precisely reflect the _extent_ to which the words are associated with that construct. It can do this even for texts that do not contain any dictionary words. Because embeddings are continuous and already calibrated to the probabilities of word use in language, DDR also avoids the difficult statistical problems that arise due to the strange distributions of word counts (@sec-word-counting-improvements).

@garten_etal_2018 found that DDR works best with smaller dictionaries of only the words most directly connected to the construct being measured (around 30 words worked best in their experiments). The diminished effectiveness of longer dictionaries is likely due to the properties of word embeddings---while the _direction_ of a word embedding represents its meaning (i.e. its average context), the _magnitude_ of a word embedding (i.e. how far it would move another vector if the two were added together) represents how specific it is to that context (@sec-embedding-magnitude). This is why an accurate embedding of a full text can be obtained by averaging the embeddings of each of its words---the embeddings automatically devalue frequent and uninformative words, and emphasize the words that are representative of the text's specific meaning [@ethayarajh_etal_2019]. Overvaluing informative words is a desirable property for raw texts, in which uninformative words tend to be very frequent. But dictionaries only include one of each word. In longer dictionaries with more infrequent, tangentially connected words, averaging word embeddings will therefore _overvalue_ those infrequent words and skew the DDR. This can be fixed with Garten et al.'s method of picking out only the most informative words. Alternatively, it could be fixed by measuring the frequency of each dictionary word in a corpus and weighting the average embedding by that frequency. This method is actually more consistent with the way most dictionaries are validated, by counting the frequencies of dictionary words in text (@sec-word-counting).

[^ddr-1]: For more information on this property, see our [footnote](decontextualized-embeddings.html#fn7) in @sec-word2vec. Note that this property emerges naturally from the way decontextualized models like word2vec and GloVe are trained, and therefore may not hold true for contextualized embeddings.

Let's measure surprise in the Hippocorpus texts by computing a DDR of the NRC Word-Emotion Association Lexicon [@mohammad_turney_2010; @mohammad_turney_2013] which we used in @sec-word-counting. To correct for word informativeness, we will weight the dictionary word embeddings by their frequency in the corpus.

```{r}
#| output: false
# load surprise dictionary
surprise_dict <- quanteda.sentiment::data_dictionary_NRC["surprise"]

# estimate frequency of dictionary words
surprise_dict_freqs <- hippocorpus_dfm |> 
  dfm_keep(surprise_dict$surprise) |> 
  quanteda.textstats::textstat_frequency() |> 
  select(feature, frequency)

# word2vec embeddings of dictionary words
surprise_ddr <- predict(word2vec_mod, surprise_dict$surprise, type = "embedding") |> 
  as_tibble(rownames = "feature") |> 
  left_join(surprise_dict_freqs) |> 
  replace_na(list(frequency = 0))

# average dictionary embedding (weighted by frequency)
surprise_ddr <- surprise_ddr |> 
  summarise(across(V1:V300, ~weighted.mean(.x, frequency))) |> 
  select(V1:V300) |> 
  unlist()

# document embeddings
hippocorpus_word2vec <- hippocorpus_dfm |> 
  textstat_embedding(word2vec_mod)

# score documents by surprise
hippocorpus_surprise_ddr <- hippocorpus_word2vec |> 
  rowwise() |> 
  mutate(
    surprise = cos_sim(c_across(V1:V300), surprise_ddr),
    # transform cosine similarity to stay between 0 and 1
    surprise = surprise/2 + 1/2
  ) |> 
  ungroup() |> 
  select(-c(V1:V300))

# rejoin docvars
hippocorpus_surprise_ddr <- hippocorpus_surprise_ddr |> 
  bind_cols(docvars(hippocorpus_corp))
```

With the new measure of surprise, we can retest the hypothesis that true autobiographical stories include more surprise than imagined stories. 

```{r}
#| warning: false
# logistic regression
surprise_mod_ddr <- glm(
  surprise ~ memType, 
  family = binomial,
  hippocorpus_surprise_ddr
  )

summary(surprise_mod_ddr)
```

We again find no significant difference in surprise between remembered and recalled stories. This is consistent with our results from @sec-word-counting, where we tested the same hypothesis with the same dictionary, but used word counts rather than embeddings.

#### DDR For Word-by-Word Analysis

Another advantage of DDR over dictionary-based word counts is that DDR enables word-by-word analysis of text. It is not very informative to count how many surprise words are in each word (it will either be one or zero), but we can compare the embedding of each word to the surprise DDR. This allows us to see how a construct spreads out within a single text. As an example, let's take a single story from the Hippocorpus:

```{r}
# full text as string
story <- word(hippocorpus_df$story[3], end = 140L)

cat(story)
```

To visualize surprise within this text, we can separate it into words and find the embedding of each word. Rather than averaging all of these embeddings together to get the embedding of the full text, we can compute a rolling average, averaging each word's embedding with those of its neighbors.

```{r}
# separate into vector of tokens
story <- word(hippocorpus_df$story[3], end = 140L) |> 
  tokens() |> as.character()
  
# rolling average of embeddings
story_surprise <- as_tibble(predict(word2vec_mod, story, type = "embedding")) |> 
  mutate(
    across(
      V1:V300, 
      ~zoo::rollapply(
        .x, 4, mean, na.rm = TRUE, 
        align = "center",
        fill = c(head(.x, 1), NA, tail(.x, 1))
        )
      )
    )
  
# vector of computed surprise (cosine similarity)
story_surprise <- story_surprise |> 
  rowwise() |> 
  mutate(surprise = cos_sim(c_across(V1:V300), surprise_ddr)) |> 
  pull(surprise)
```

We can now visualize the surprise in each word of the text. Since `ggplot2` makes it difficult to plot dynamically colored text in one continuous chunk, we will use ANSI color codes to print the our text directly to the console.

```{r}
#| code-overflow: wrap
# (see https://www.hackitu.de/termcolor256/ for info on ANSI colors)
# blue-red heat scale
ansi_scale <- c(
  063, 105, 147, 189, 188, 230, 223, 
  224, 217, 210, 203, 196, 160, 124
  )

# turn scale value into ANSI color code
map_to_ansi <- function(x, ansi_scale){
  x_new <- (x - min(x, na.rm = TRUE))*(length(ansi_scale)/diff(range(x, na.rm = TRUE))) + 1
  x_new
  ansi_scale[round(x_new)]
}

story_surprise <- map_to_ansi(story_surprise, ansi_scale)

# print
for (i in 1:length(story_surprise)) {
  if(is.na(story_surprise[i])){
    cat(story[i], " ")
  }else{
    cat(paste0("\033[48;5;", story_surprise[i], "m", story[i], " \033[0m"))
  }
}
```

::: {.callout-tip icon="false"}
## Advantages of DDR

-   **Richer, More Robust Construct Representation Than Word Counting**
-   **Avoids Statistical Problems With Word Count Distributions**
-   **Enables Word-by-Word Analysis**
:::

::: {.callout-important icon="false"}
## Disadvantages of DDR

-   **Can Implicitly Encode Associated Constructs:** For example, if surprised texts tend to have positive valence in the data used to train the word embedding model, the DDR for surprise may embed some positive valence as well. This can be remedied by constructing a DDR for positive valence as well, and using it as a statistical control when testing hypotheses.
-   **May Not Work With Contextualized Embeddings:** Even if we assume that contextualized embeddings conform to the geometrical properties associated with word embeddings, LLMs are not designed to embed single words, which is required for DDR.
-   **Not Appropriate for Linguistic Measures:** Word embeddings encode the general gist of a text, whereas constructs like passive voice or pronoun use refer to specific words.
:::

### Contextualized Construct Representation (CCR)

Dictionaries are not the only validated psychological measures that we can apply using embeddings. With contextualized embeddings, we can extract the gist of any text and compare it to that of any other text. @atari_etal_2023 propose to do this with the most popular form of psychometric scale: the questionnaire. Psychologists have been using questionnaires to measure things for over a century, and tens of thousands of validated questionnaires are now available [online](https://www.apa.org/pubs/databases/psyctests). The LLM embedding of a questionnaire is referred to as a Contextualized Construct Representation (CCR).

We can use CCR to measure surprise in the Hippocorpus texts. For our questionnaire, we will use an adapted version of the surprise scale used in @choi_choi_2010 and @choi_nisbett_2000.

```{r}
surprise_items <- c(
  "I was extremely surprised by the outcome of the event.",
  "The outcome of the event was extremely interesting.",
  "The outcome of the event was extremely new."
  )
```

::: {.callout-important}
## Beware of Reverse Coding!

Many questionnaires include reverse-coded items (e.g. "I often feel happy" on a depression questionnaire). The easiest way to deal with these is to manually add negations to flip their meaning (e.g. "I _do not_ often feel happy"). 
:::

The first step in using CCR is to compute contextualized embeddings for the texts in our dataset. We already did this in @sec-contextualized-embeddings. The next step is to compute contextualized embeddings for the items in the questionnaire, and average them to produce a CCR.

```{r}
#| eval: false
# embed items (using the same model as we used before)
library(text)
  
surprise_sbert <- textEmbed(
  surprise_items,
  model = "sentence-transformers/all-MiniLM-L12-v2", # model name
  layers = -2,  # second to last layer (default)
  tokens_select = "[CLS]", # use only [CLS] token
  dim_name = FALSE,
  keep_token_embeddings = FALSE
  )

# compute CCR by averaging item embeddings
surprise_ccr <- surprise_sbert$texts[[1]] |>
  summarise(across(everything(), mean)) |> 
  unlist()
```

We can now measure surprise in the Hippocorpus texts by computing the cosine similarity between their embeddings and the surprise CCR.^[Cosine similarity is appropriate here because our contextualized embeddings were generated by an SBERT model which was designed to be used with cosine similarity. If we had used another model such as RoBERTa, Euclidean distance might be more appropriate.]

```{r}
#| warning: false
# score documents by surprise
hippocorpus_surprise_ccr <- hippocorpus_sbert |> 
  rowwise() |> 
  mutate(
    surprise = cos_sim(c_across(Dim1:Dim384), surprise_ccr),
    # transform cosine similarity to stay between 0 and 1
    surprise = surprise/2 + 1/2
  ) |> 
  ungroup() |> 
  select(-c(Dim1:Dim384))

# logistic regression
surprise_mod_ccr <- glm(
  surprise ~ memType, 
  family = binomial,
  hippocorpus_surprise_ccr
  )

summary(surprise_mod_ccr)
```

Once again we find no significant difference in surprise between remembered and recalled stories. Don't take this result too seriously though, since CCR has a fundamental problem that needs to be addressed.

Embeddings capture the overall "vibes" of a text, including its tone and dialect. With CCR, we are comparing the "vibes" of a questionnaire written by academics to the "vibes" of narratives written by Hippocorpus participants. By comparing these vectors, we are not just measuring how much surprise is in each text---we are also measuring the extent to which each text is written in the style of a questionnaire written by academics. This introduces a confounding variable into our analysis---questionnaire-ness.

How can we be sure that we are measuring surprise and not questionnaire-ness? We can't, but there are some methods that might help. We will explore these in @sec-ccr-improvement and @sec-dimension-projection. 

::: {.callout-tip icon="false"}
## Advantages of CCR

-   ****
:::

::: {.callout-important icon="false"}
## Disadvantages of CCR

-   ****
:::

### Correlational Methods

i.e. averaging from group in training set

```{r}

```

## Reasoning in Vector Space: Beyond Cosine Similarity

### Parallelograms {#sec-parallelograms}

Introduced with word2vec by @mikolov_etal_2013

Glove [@pennington_etal_2014] is designed with this property in mind. Transformer models are not.

- Transformer models, including BERT, tend to generate embedding spaces that do not center at zero and which tend to form a narrow cone in the vector space [@ethayarajh_2019; @gao_etal_2019]
  - in BERT, token embeddings in the same sentence become more dissimilar to each other in upper layers but are still more similar than randomly sampled words are on average
  - BERT aggregated text embeddings perform worse than word2vec and GloVe when analyzed using cosine similarity [@reimers_gurevych_2019], though averaging the embeddings from the last two layers of BERT can improve this [@li_etal_2020]
  
  static embeddings created by taking the first principal component of a word’s contextualized representations out- perform GloVe and FastText embeddings on many word vector benchmarks [@ethayarajh_2019]

```{r}

```

#### Improving CCR With Geometric Reasoning {#sec-ccr-improvement}

In CCR, for example, there’s a difference between agreeing with statements in a questionnaire and actually writing like a questionnaire. How do I know that I’m measuring how anxious/depressed the comments are and not how questionnaire-y they are?
Potential workaround: reverse all of the statements in the questionnaire (as for reverse-coded items), add them to the original questionnaire, and get the average embedding of the construct-neutral questionnaire. Then subtract this embedding from the original questionnaire average embedding.

```{r}
surprise_items_pos <- c(
  "I was extremely surprised by the outcome of the event.",
  "The outcome of the event was extremely interesting.",
  "The outcome of the event was extremely new."
  )

surprise_items_neg <- c(
  "I was not surprised at all by the outcome of the event.",
  "The outcome of the event was not interesting at all.",
  "The outcome of the event was not new at all."
  )
```

### Advanced Similarity Measures {#sec-advanced-similarity}

#### Dot Product

#### Jaccard similarity

#### Mutual Information

#### Jensen–Shannon divergence

### Semantic Projection {#sec-dimension-projection}

@grand_etal_2022

#### Improving CCR With Semantic Projection

**An example of using dimension projection and CCR in research:** @simchon_etal_2023 collected 10,000 messages from the [r/depression](https://www.reddit.com/r/depression) subreddit, along with a control group of 100 messages each from 100 randomly selected subreddits. They then used a variant of SBERT, `all-MiniLM-L6-v2` (see @sec-contextualized-embeddings), to compute CCR embeddings of a psychological questionnaire measuring "locus of control," the feeling that you have control over your own life. The questionnaire included items measuring an internal locus of control ("I have control"), and items measuring an external locus of control ("External forces have control"). Simchon et al. constructed an anchored vector to capture the direction between internal and external locus of control, and projected embeddings of the Reddit posts onto that vector to measure how much each post reflected an internal vs. an external locus of control. They found that posts in r/depression exhibited a more external locus of control than posts in the control group.

### Machine Learning Methods {#sec-machine-learning-methods}

@kjell_etal_2022

@chersoni_etal_2021 used partial least squares regression to map distributional embeddings to human-rated semantic features derived from research in cognitive psychology.

Some research advises using both the `[CLS]` token and an aggregation of the other token embeddings [@lee_etal_2023]
