# Bag of Words Embeddings: Decontextualized Models {#sec-decontextualized-embeddings}

```{r setup}
#| echo: false
#| include: false

source("_common.R")
```

::: callout-important
## This page is still under construction. Come back soon!
:::

## The Distributional Hypothesis

Hence "Distributional Semantic Models" (DSMs)

Two words are similar in meaning not when they appear together often, but rather when they tend to appear in similar contexts. So synonyms, which tend to very rarely co-occur directly, will have very similar meanings in DSMs [@gunther_etal_2019].

## Document Embeddings

### LSA {#sec-lsa}

DFM -\> Weighting (tf-idf, PPMI) -\> Dimensionality reduction

[AffectiveSpace](https://sentic.net/downloads/)

@deerwester_etal_1990

@garcia_sikstrom_2013

[LSA in Quanteda](https://quanteda.io/articles/pkgdown/examples/lsa.html)

### Topic Modeling {#sec-lda}

#### Supervised LDA {#sec-slda}

[sLDA in R](https://books.psychstat.org/textmining/topic-models.html#supervised-topic-modeling)

#### Semi-Supervised LDA {#sec-seededlda}

[seededLDA in R](https://koheiw.github.io/seededlda/)

## Word Embeddings {#sec-word-embeddings}

### Word2vec {#sec-word2vec}

@chatterjee_etal_2023

### GloVe {#sec-glove}

@pennington_etal_2014

GloVe is built on the same metric that we used in @sec-word-viz and @sec-dla - relative frequency ratios. Rather than comparing two word frequencies in two groups of texts, it instead compares co-occurrence with one word to co-occurrence with another.

#### Training a Custom GloVe Model

[Training a custom GloVe model in Quanteda](https://quanteda.io/articles/pkgdown/replication/text2vec.html)
