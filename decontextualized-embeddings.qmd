# Bag of Words Embeddings: Decontextualized Models {#sec-decontextualized-embeddings}

```{r setup}
#| echo: false
#| include: false

source("_common.R")
```

::: {.callout-important}
## This page is still under construction. Come back soon!
:::

## The Distributional Hypothesis

Hence "Distributional Semantic Models" (DSMs)

- Two words are similar in meaning not because of their mutual co-occurrence score, but rather if they have similar global distributional patterns over all contexts. Due to this, synonyms, which tend to very rarely co-occur directly, will have very similar mean- ings in DSMs [@gunther_etal_2019].

## Document Embeddings

### LSA {#sec-lsa}

DFM -> Weighting (tf-idf, PPMI) -> Dimensionality reduction

[AffectiveSpace](https://sentic.net/downloads/)

@deerwester_etal_1990

@garcia_sikstrom_2013

https://quanteda.io/articles/pkgdown/examples/lsa.html

### Topic Modeling {#sec-lda}

#### Supervised LDA {#sec-slda}

https://books.psychstat.org/textmining/topic-models.html#supervised-topic-modeling

#### Semi-Supervised LDA {#sec-seededlda}

https://koheiw.github.io/seededlda/

## Word Embeddings {#sec-word-embeddings}

### Word2vec {#sec-word2vec}

@chatterjee_etal_2023

### GloVe {#sec-glove}

@pennington_etal_2014

GloVe is built on the same metric that we used in @sec-word-viz and @sec-eda - relative frequency ratios. Rather than comparing two word frequencies in two groups of texts, it instead compares co-occurrence with one word to co-occurrence with another. 

#### Training a Custom GloVe Model

https://quanteda.io/articles/pkgdown/replication/text2vec.html
